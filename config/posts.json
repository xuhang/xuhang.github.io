{
  "posts": [
    {
      "content": "## Spring AOP 注解\n\n在使用spring框架时,通常用它的aop来记录日志,但在spring mvc采用@Controller注解时,对Controller进行Aop拦截不起作用,原因是该注解的Controller已被spring容器内部代理了.\n\n需要对org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter进行Aop才能起作用.经过多次测试可行.\n\n<!-- more -->\n\n```java\npackage com.autoabacus.dal.controller;  \nimport org.aspectj.lang.ProceedingJoinPoint;  \nimport org.aspectj.lang.annotation.Around;  \nimport org.aspectj.lang.annotation.Aspect;  \nimport org.springframework.stereotype.Component;  \n@Component  \n@Aspect  \npublic class Aop {  \n    public Aop() {  \n        System.out.println(\"Aop\");  \n    }  \n    // @Around(\"within(org.springframework.web.bind.annotation.support.HandlerMethodInvoker..*)\")  \n    @Around(\"execution(* org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter.handle(..))\")  \n    public Object aa(ProceedingJoinPoint pjp)  throws Throwable   \n    {  \n        try {  \n            Object retVal = pjp.proceed();  \n            System.out.println(retVal);  \n            return retVal;  \n        } catch (Exception e) {  \n            System.out.println(\"异常\");  \n            return null;  \n        }  \n    }  \n}\n```\n\nController:\n\n```java\npackage com.autoabacus.dal.controller;  \nimport org.springframework.stereotype.Controller;  \nimport org.springframework.web.bind.annotation.RequestMapping;  \n@Controller    \npublic class WelcomeController {  \n    @RequestMapping  \n    public void welcome() {  \n        if (true)  \n            throw new RuntimeException(\"fdsafds\");  \n    }    \n}  \n```\n\n## jQuery 自定义选择器\n\n```jquery\n$(document).ready(function() {\n    $.extend($.expr[':'], {\n        moreThen1000px: function(a) {\n            return $(a).width() > 1000;\n        }\n    });\n    $('.widthDiv:moreThen1000px').click(function() {\n        alert('The element that you have clicked is over 1000 pixels wide');\n    });\n});\n```\n\n## HTML文字自动裁剪\n\n```html\n<a style=\"text-overflow: ellipsis;overflow: hidden;white-space: nowrap;width: 100%;display: inline-block;\">文字自动裁剪</a>\n```\n\n## FIREFOX图片占位符\n\n```css\n@-moz-document url-prefix(http), url-prefix(file) {\n    img:-moz-broken{\n        -moz-force-broken-image-icon:1;\n        width:220px;\n        height:220px;\n    }\n}\n```\n\n## JAVASCRIPT轻量模板\n\n```javascript\nfunction tmpl(template, data) {\n    return template.replace(/\\{([\\*\\w\\.]*)\\}/g, function(str, key) {\n        var keys = key.split(\".\");\n        var rep = '_300*300';\n        //如果有形如“_300*300”的key，则作为图片的大小，并从keys中移除\n        for(var i=0; i<keys.length; i++){\n            if(/_\\d{1,4}\\*\\d{1,4}/.test(keys[i])){\n                rep = keys.splice(i,1);\n                break;\n            }\n        }\n        /*if(keys.length>2) {\n            var ss = keys[1];\n            if(/_\\d{1,4}\\*\\d{1,4}/.test(ss)){\n                rep = keys.splice(1,1);\n            }\n        }*/\n        var v = data[keys.shift()];\n        for (var i = 0, l = keys.length; i < l; i++) v = v[keys[i]];\n        v = (typeof v !== \"undefined\" && v !== null) ? v : \"\";\n        if(strEndsWith(v.toString(), \".jpg\") || strEndsWith(v.toString(), \".JPG\") || strEndsWith(v.toString(), \".webp\")){\n            v = v.replace(/\\.(jpg|JPG|webp)$/, rep+\"$&\");\n        }\n        return v;\n    });\n    function strEndsWith(str, suffix) {\n        return str.match(suffix+\"$\")==suffix;\n    }\n}\n```\n注释：\n```javascript\nfunction tmpl(template, data) {\n    return template.replace(/\\{([\\w\\.]*)\\}/g, function(str, key, idx, ori) {//str-匹配正则，key-匹配括号内，idx-匹配正则的str在原字符串中的位置，ori-原始字符串\n        var keys = key.split(\".\");\n        //shift把数组的第一个元素从其中删除，并返回第一个元素的值\n        var v = data[keys.shift()];\n        for (var i = 0, l = keys.length; i < l; i++) v = v[keys[i]];\n        v = (typeof v !== \"undefined\" && v !== null) ? v : \"\";\n        return v;\n    });\n}\n```\n\n## 在浏览器中调试异步加载的js脚本\n\n如果页面在请求之后返回的是需要插入在原页面中的代码，这些代码通常除了啦HTML代码还会带着一些JS脚本。这些脚本通常不会在Source栏中显示，导致调试这些脚本会比较麻烦，除了在代码中插入`debugger;`还可以在脚本的最后（`</script>`之前）加上标签`//# sourceURL=xxx.js`，这样在浏览器的调试窗口选择打开文件（快捷键`Ctrl + P`）输入xxx代表的名字即可打开调试。\n\n## Spring中注入枚举类型的属性值\n\n```xml\n<bean id=\"databaseId\" class=\"java.lang.Integer\">\n    <constructor-arg type=\"int\" value=\"#{T(com.yhd.kaleido.kedis.domain.KedisPool).${redis.databasePool}.id}\" />\n</bean>\n```\n\n## ConcurrentModificationException\n\n`foreach loop`调用的其实是`Iterator`迭代器，`Iterator`内部维护`expectedModCount`，`ArrayList`维护`modCount`。只有调用`Iterator`的`remove`方法会保证`expectedModCount`和`modCount`会相等，而调用`ArrayList`的`remove`方法只修改`modCount`，`foreach loop`遍历元素时调用`next()`，会触发`checkForComodification()`，此方法中`expectedModCount！=modCount`会抛出`ConcurrentModificationException`。 \n\n## Linux查看连接的创建时间\n\n查看ssh到ip地址221.234.44.52连接的创建时间：\n\n1. `netstat -npt | grep 221.234.44.52  `\n2. 记录结果中的pid和port\n3. 查看这个进程打开的这个连接的文件名：`lsof -p PID | grep PORT `\n4. 结果中有表示进程在这个端口上的连接的文件编号 ：xxxu\n5. 查看文件的创建时间：`ll /proc/PID/fd/xxx`\n6. 结果格式类似`lrwx------ 1 root root 64 Jul 28 14:38 170 -> socket:[788102217]`\n7. 查看socket对应inode信息`netstat -altp | egrep -i 'Inode|788102217'`\n\n## Linux中标准输入输出\n\n标准输入0    从键盘获得输入 /proc/self/fd/0 \n\n标准输出1    输出到屏幕（即控制台） /proc/self/fd/1 \n\n错误输出2    输出到屏幕（即控制台） /proc/self/fd/2 \n\n \n\n/dev/null代表linux的空设备文件，所有往这个文件里面写入的内容都会丢失，俗称“黑洞” \n\n+ `2>/dev/null`意思就是把错误输出到“黑洞” \n+ `>/dev/null 2>&1`默认情况是1，也就是等同于1>/dev/null 2>&1。意思就是把标准输出重定向到“黑洞”，还把错误输出2重定向到标准输出1，也就是标准输出和错误输出都进了“黑洞” \n+ `2>&1 >/dev/null`意思就是把错误输出2重定向到标准出书1，也就是屏幕，标准输出进了“黑洞”，也就是标准输出进了黑洞，错误输出打印到屏幕 \n\n关于这里”&”的作用，我们可以这么理解2>/dev/null重定向到文件，那么2>&1，这里如果去掉了&就是把错误输出给了文件1了，用了&是表明1是标准输出。\n\n## Linux 设置命令执行超时时间\n\n```bash\ncommand & pid=$!;sleep 2;kill -9 $pid\n```\n\n## JS可拖拽元素\n\n``` javascript\nfunction pauseEvent(e) {\n    if (e.stopPropagation) e.stopPropagation();\n    if (e.preventDefault) e.preventDefault();\n    e.cancelBubble = true;\n    e.returnValue = false;\n    return false;\n}\nlet dragableTar;\nlet winHeight = window.screen.height, winWidth = window.screen.width;\nlet startX,\n    startY,\n    currentLeft,\n    currentTop,\n    flag = 0;\ndocument.body.onmousedown = function (e) {\n    let tar = e.target;\n    if (tar.tagName == 'INPUT' || tar.tagName == 'A') {\n        return;\n    }\n\n    pauseEvent(e);\n    while (tar.parentNode) {\n        tar = tar.parentNode;\n        if (tar == document.body) {\n            break;\n        }\n        if (tar.className && tar.className.indexOf('dragable') >= 0) {\n            dragableTar = tar;\n            startX = e.clientX;\n            startY = e.clientY;\n            currentLeft = dragableTar.offsetLeft;　　//通过对象获取对象的坐标\n            currentTop = dragableTar.offsetTop;\n            break;\n        }\n    }\n    let callback = (e) => {\n        if (dragableTar) {\n            let tarWidth = dragableTar.clientWidth||dragableTar.offsetWidth;\n            let tarHeight = dragableTar.clientHeight||dragableTar.offsetHeight;\n\n            let x = e.clientX;　　　　　　　　//e.clientX是一个触摸事件，即是鼠标点击时的X轴上的坐标\n            let y = e.clientY;　　　　　　　　//e.clientY是一个触摸事件，即是鼠标点击时的Y轴上的坐标\n            let distanceX = x - startX;　　　　//X轴上获得移动的实际距离\n            let distanceY = y - startY;　　　　　//Y轴上获得移动的实际距离\n\n            let tarLeft = distanceX + currentLeft;\n            if (tarLeft >= 0 && tarLeft + tarWidth <= winWidth) {\n                dragableTar.style.left = (tarLeft) + \"px\";　　//currentLeft下面的方法会有解释，需要注意最后要添加PX单位，如果给left赋值会破坏文档流，不加单位就会无效\n            }\n            let tarTop = distanceY + currentTop;\n            if (tarTop >= 0 && tarTop + tarHeight <= winHeight) {\n                dragableTar.style.top = (distanceY + currentTop) + \"px\";\n            }\n        }\n    }\n\n    console.log(dragableTar)\n    document.addEventListener('mousemove',callback);\n    document.addEventListener('mouseup', () => {\n        console.log('鼠标UP')\n        document.removeEventListener('mousemove',callback);\n        if (tar.id == 'musicController') {\n            updateThumbnail(tar.offsetLeft, tar.offsetTop);\n        }\n        // pauseEvent(e);\n        dragableTar = null;\n    });\n```\n\n## 在MyBatis中引用常量\n\n```xml\n${@全类名$内部类@常量名}\n${@com.wyouquan.coupon.entity.MarketingCouponDictionary$DeleteStatus@NOT_DELETED}\n```\n\n## SpringMVC复合参数注入和Mybatis复合参数使用\n\n接收参数的Bean定义\n\n```java\npublic class Page {\n    private long total;\n    private Map<String, Object> paramMap;\n    private int limit;\n    private int offset;\n    private String orderBy;\n    ...\n}\n```\n\n要传递limit和offset的同时，还要往paramMap里注入参数\n\nrequest请求头的`contentType`值为`application/x-www-form-urlencoded`\n\n传递的JSON参数值为\n\n```json\n{\n    'limit': 10,\n    'offset': 0,\n    'paramMap[type]': 1,\n    'orderBy': 'name'\n}\n```\n\n这样Controller层接收到的参数Page中的Map里就包含了指定的值。\n\n在Dao层使用复合参数Page根据对象的使用规则，用`.`调用即可\n\n```xml\nSELECT <include refid=\"Base_Column_List\"/>\nFROM marketing_coupons_dictionary\n<trim prefix=\"WHERE\" prefixOverrides=\"AND |OR \">\n    <if test=\"page.paramMap.type != null\">\n        dictionary_type = #{page.paramMap.type} AND\n    </if>\n    is_delete = ${@com.wyouquan.coupon.entity.MarketingCouponDictionary$DeleteStatus@NOT_DELETED}\n</trim>\n<if test=\"page.orderBy != null\">\n    ORDER BY #{page.orderBy}\n</if>\n<if test=\"page.offset != null and page.limit != null\">\n    LIMIT #{page.offset}, #{page.limit}\n</if>\n```\n\n## Shardingsphere 3.1.0分页Bug\n\nShardingSphere在3.1.0的版本中，对没有配置分表的查询，limit查询会默认把前N条记录都查询出来再做处理，比如limit 20, 10，shardingsphere会把所有分表的前30条记录都查询出来，再整合数据选择10条，但是对于没有配置分表的查询，默认返回30条记录。这一bug在4.0.0-RC1修复，但是4.0.0之前的版本的package是`shardingsphere.io`，4.0.0的版本package是`org.apache.shardingsphere`，并且命名空间的schema文件也有相应的变化，如果IDEA报找不到命名空间的文件错误，需要额外导入namespace的包。\n\n```xml\n<!-- ShardingSphere包 -->\n<dependency>\n    <groupId>org.apache.shardingsphere</groupId>\n    <artifactId>sharding-jdbc-core</artifactId>\n    <version>${shardingsphere.version}</version>\n</dependency>\n<!-- 命名空间的包 -->\n<dependency>\n    <groupId>org.apache.shardingsphere</groupId>\n    <artifactId>sharding-jdbc-spring-namespace</artifactId>\n    <version>${shardingsphere.version}</version>\n</dependency>\n```\n\n## MySQL 8.x登陆失败`caching_sha2_password`\n\n8.0.4版本以上，mysql默认授权插件改成了caching_sha2_password模式，所以实际设置的密码是被转换过的。用如下方法解决问题：\n\n命令行登陆MySQL之后，执行命令：`ALTER USER 'username'@'ip_address' IDENTIFIED WITH mysql_native_password BY 'password';`\n\n比如：`alter user 'root'@'%' identified with mysql_native_password by '123456';`\n\n修改之后即可登陆。\n\n## MySQL查询分组的前N条记录\n\n案例：查询学生成绩表`stu`中，各班（`class`）各科目（`subject`）的前10名：\n\n```mysql\nSELECT * FROM `stu` AS a WHERE 10 > (SELECT COUNT(*) FROM `stu` WHERE class = a.class AND `subject` = a.`subject` AND score < a.score) ORDER BY a.class ASC,a.subject ASC,a.score DESC\n```\n\n总结：查询表中，分组1（各班）、分组2下（各科目），按条件（成绩）降序的前N（10）名：\n\n```mysql\nSELECT * FROM 表 AS a WHERE N > (SELECT COUNT(*) FROM 表 WHERE 分组1=a.分组1 AND 分组2=a.分组2 AND 条件 < a.条件) ORDER BY a.分组1, a.分组2, a.条件 DESC\n```\n\n## JAVASCRIPT变量作用域\n\n```javascript\nvar a = 10;\nfunction foo() {\n    console.log(a);\n    var a = 20;\n}\nfoo();\n```\n\n上述代码打印值为`undefined`，原因是`var`关键字声明的变量会被提升，并在内存中分配值`undefined`，具体初始化是发生在赋值的地方。另外`var`声明的变量作用域是函数基本的，`let`和`const`是块级别的。\n\n所以在执行上述foo()函数时，在打印变量`a`时，函数内部用`var`声明变量，会在打印之前局部变量会提升并分配`undefined`，执行到`var a = 20`时才会分配具体值。\n\n`let`和`const`声明的变量不会被提升，只有执行到声明的地方才能访问，提前访问会抛出`ReferenceError`异常。下面的代码会抛出`ReferenceError`异常：\n\n```javascript\nvar a = 10;\nfunction foo() {\n    console.log(a);\n    let a = 20;\n}\nfoo();\n```\n\n## Python图片字符识别tesserocr异常`RuntimeError: Failed to init API, possibly an invalid tessdata path: D:\\Python36\\/tessdata/`\n\n```python\nimport tesserocr\nfrom PIL import Image\nimage = Image.open('C:\\\\Users\\\\Administrator\\\\Desktop\\\\a.jpg')\nprint(tesserocr.image_to_text(image))\n```\n\n将Tesseract-OCR安装路径下的`tessdata`文件夹拷贝到python的安装路径下即可。\n\n## java程序执行会启动的线程\n\n```java\npublic  static Thread[] findAllThread(){\n    ThreadGroup currentGroup =Thread.currentThread().getThreadGroup();\n    while (currentGroup.getParent()!=null){\n        // 返回此线程组的父线程组\n        currentGroup=currentGroup.getParent();\n    }\n    //此线程组中活动线程的估计数\n    int noThreads = currentGroup.activeCount();\n    Thread[] lstThreads = new Thread[noThreads];\n    //把对此线程组中的所有活动子组的引用复制到指定数组中。\n    currentGroup.enumerate(lstThreads);\n    for (Thread thread : lstThreads) {\n        System.out.println(\"线程数量：\"+noThreads+\" \" +\n                \"线程id：\" + thread.getId() + \n                \" 线程名称：\" + thread.getName() + \n                \" 线程状态：\" + thread.getState());\n    }\n    return lstThreads;\n}\n```\n\n输出：\n\n```\n线程数量：6 线程id：2 线程名称：Reference Handler 线程状态：WAITING\n线程数量：6 线程id：3 线程名称：Finalizer 线程状态：WAITING\n线程数量：6 线程id：4 线程名称：Signal Dispatcher 线程状态：RUNNABLE\n线程数量：6 线程id：5 线程名称：Attach Listener 线程状态：RUNNABLE\n线程数量：6 线程id：1 线程名称：main 线程状态：RUNNABLE\n线程数量：6 线程id：6 线程名称：Monitor Ctrl-Break 线程状态：RUNNABLE\n```\n\n| 线程 | **所属**     | **说明**     |\n| ------------------- | ------------ | ------------------------- |\n| ==Attach Listener==| JVM          | Attach Listener线程是负责接收到外部的命令，而对该命令进行执行的并且吧结果返回给发送者。通常我们会用一些命令去要求jvm给我们一些反馈信息，如：java -version、jmap、jstack等等。如果该线程在jvm启动的时候没有初始化，那么，则会在用户第一次执行jvm命令时，得到启动。 |\n| ==Signal Dispatcher== | JVM          | 前面我们提到第一个Attach Listener线程的职责是接收外部jvm命令，当命令接收成功后，会交给signal dispather线程去进行分发到各个不同的模块处理命令，并且返回处理结果。signal dispather线程也是在第一次接收外部jvm命令时，进行初始化工作。 |\n| CompilerThread0  | JVM          | 用来调用JITing，实时编译装卸class。通常，jvm会启动多个线程来处理这部分工作，线程名称后面的数字也会累加，例如：CompilerThread1 |\n| ConcurrentMark-SweepGCThread| JVM          | 并发标记清除垃圾回收器（就是通常所说的CMS GC）线程，该线程主要针对于老年代垃圾回收。ps：启用该垃圾回收器，需要在jvm启动参数中加上：-XX:+UseConcMarkSweepGC |\n| DestroyJavaVM| JVM          | 执行main()的线程在main执行完后调用JNI中的jni_DestroyJavaVM()方法唤起DestroyJavaVM线程。  JVM在Jboss服务器启动之后，就会唤起DestroyJavaVM线程，处于等待状态，等待其它线程（java线程和native线程）退出时通知它卸载JVM。线程退出时，都会判断自己当前是否是整个JVM中最后一个非deamon线程，如果是，则通知DestroyJavaVM线程卸载JVM。ps：扩展一下：1.如果线程退出时判断自己不为最后一个非deamon线程，那么调用thread->exit(false)，并在其中抛出thread_end事件，jvm不退出。2.如果线程退出时判断自己为最后一个非deamon线程，那么调用before_exit()方法，抛出两个事件： 事件1：thread_end线程结束事件、事件2：VM的death事件。然后调用thread->exit(true)方法，接下来把线程从active list卸下，删除线程等等一系列工作执行完成后，则通知正在等待的DestroyJavaVM线程执行卸载JVM操作。 |\n| ContainerBackgroundProcessor| JBOSS        | 它是一个守护线程,在jboss服务器在启动的时候就初始化了,主要工作是定期去检查有没有Session过期.过期则清除.参考：http://liudeh-009.iteye.com/blog/1584876 |\n| ConfigClientNotifier| ConfigServer | ConfigServer服务端当有配置变更时，就会将最新的配置推送到ConfigServer客户端的一个数据列队中，ConfigClientNotifier线程用于定期检查该数据列队中是否有数据，如果有数据，则将数据分发到订阅该数据的组件去做业务逻辑，比如：tair和hsf的数据都订阅了ConfigServer数据源,当ConfigClientNotifier线程发现数据有更新时，就触发做数据分发特定特定信号标识将数据分发到相应的订阅者。 |\n| ConfigClientWorker-Default| ConfigServer | 包括主动向服务器端发送数据（主要是订阅和发布的数据）和接收服务器推送过来的数据（主要是订阅数据的值） 。 |\n| Dispatcher-Thread-3| Log4j        | Log4j具有异步打印日志的功能，需要异步打印日志的Appender都需要注册到AsyncAppender对象里面去，由AsyncAppender进行监听，决定何时触发日志打印操作。AsyncAppender如果监听到它管辖范围内的Appender有打印日志的操作，则给这个Appender生成一个相应的event，并将该event保存在一个buffuer区域内。 Dispatcher-Thread-3线程负责判断这个event缓存区是否已经满了，如果已经满了，则将缓存区内的所有event分发到Appender容器里面去，那些注册上来的Appender收到自己的event后，则开始处理自己的日志打印工作。Dispatcher-Thread-3线程是一个守护线程。 |\n| ==Finalizer线程==| JVM          | 这个线程也是在main线程之后创建的，其优先级为10，主要用于在垃圾收集前，调用对象的finalize()方法；关于Finalizer线程的几点：1)只有当开始一轮垃圾收集时，才会开始调用finalize()方法；因此并不是所有对象的finalize()方法都会被执行；2)该线程也是daemon线程，因此如果虚拟机中没有其他非daemon线程，不管该线程有没有执行完finalize()方法，JVM也会退出；3) JVM在垃圾收集时会将失去引用的对象包装成Finalizer对象（Reference的实现），并放入ReferenceQueue，由Finalizer线程来处理；最后将该Finalizer对象的引用置为null，由垃圾收集器来回收；4) JVM为什么要单独用一个线程来执行finalize()方法呢？如果JVM的垃圾收集线程自己来做，很有可能由于在finalize()方法中误操作导致GC线程停止或不可控，这对GC线程来说是一种灾难； |\n| Gang worker#0| JVM          | JVM用于做新生代垃圾回收（monir gc）的一个线程。#号后面是线程编号，例如：Gang worker#1 |\n| GC Daemon | JVM          | GC Daemon线程是JVM为RMI提供远程分布式GC使用的，GC Daemon线程里面会主动调用System.gc()方法，对服务器进行Full GC。 其初衷是当RMI服务器返回一个对象到其客户机（远程方法的调用方）时，其跟踪远程对象在客户机中的使用。当再没有更多的对客户机上远程对象的引用时，或者如果引用的“租借”过期并且没有更新，服务器将垃圾回收远程对象。不过，我们现在jvm启动参数都加上了-XX:+DisableExplicitGC配置，所以，这个线程只有打酱油的份了。 |\n| IdleRemover | JBOSS        | Jboss连接池有一个最小值，该线程每过一段时间都会被Jboss唤起，用于检查和销毁连接池中空闲和无效的连接，直到剩余的连接数小于等于它的最小值。 |\n| Java2D Disposer| JVM          | 这个线程主要服务于awt的各个组件。说起该线程的主要工作职责前，需要先介绍一下Disposer类是干嘛的。Disposer提供一个addRecord方法。如果你想在一个对象被销毁前再做一些善后工作，那么，你可以调用Disposer#addRecord方法，将这个对象和一个自定义的DisposerRecord接口实现类，一起传入进去，进行注册。Disposer类会唤起“Java2D Disposer”线程，该线程会扫描已注册的这些对象是否要被回收了，如果是，则调用该对象对应的DisposerRecord实现类里面的dispose方法。Disposer实际上不限于在awt应用场景，只是awt里面的很多组件需要访问很多操作系统资源，所以，这些组件在被回收时，需要先释放这些资源。 |\n| FelixDispatchQueue | Sofa         | 该线程会在sofa启动时会唤起该线程,该线程用于分发OSGI事件到Declarative Services 中去发布，查找，绑定目标服务。其实，我们接口配置的service和reference就涉及到服务的发布、查找和绑定工作。  Declarative Services 主要工作职责是方便地对服务之间的依赖关系和状态进行监听和管理。OSGI使用事件策略去调用Declarative Services中的服务。 |\n| InsttoolCacheScheduler_QuartzSchedulerThread| Quartz       | InsttoolCacheScheduler_QuartzSchedulerThread是Quartz的主线程，它主要负责实时的获取下一个时间点要触发的触发器，然后执行触发器相关联的作业。原理大致如下：Spring和Quartz结合使用的场景下，Spring IOC容器初始化时会创建并初始化Quartz线程池（TreadPool），并启动它。刚启动时线程池中每个线程都处于等待状态，等待外界给他分配Runnable（持有作业对象的线程）。继而接着初始化并启动Quartz的主线程（InsttoolCacheScheduler_QuartzSchedulerThread），该线程自启动后就会处于等待状态。等待外界给出工作信号之后，该主线程的run方法才实质上开始工作。run中会获取JobStore中下一次要触发的作业，拿到之后会一直等待到该作业的真正触发时间，然后将该作业包装成一个JobRunShell对象（该对象实现了Runnable接口，其实看是上面TreadPool中等待外界分配给他的Runnable），然后将刚创建的JobRunShell交给线程池，由线程池负责执行作业。线程池收到Runnable后，从线程池一个线程启动Runnable，反射调用JobRunShell中的run方法，run方法执行完成之后，TreadPool将该线程回收至空闲线程中。 |\n| InsttoolCacheScheduler_Worker-2| Quartz       | InsttoolCacheScheduler_Worker-2线程就是ThreadPool线程的一个简单实现，它主要负责分配线程资源去执行InsttoolCacheScheduler_QuartzSchedulerThread线程交给它的调度任务（也就是JobRunShell）。 |\n| java.util.concurrent.ThreadPoolExecutor$Worker| JVM          |                                                              |\n| JBossLifeThread| Jboss        | Jboss主线程启动成功，应用程序部署完毕之后将JBossLifeThread线程实例化并且start，JBossLifeThread线程启动成功之后就处于等待状态，以保持Jboss Java进程处于存活中。 所得比较通俗一点，就是Jboss启动流程执行完毕之后，为什么没有结束？就是因为有这个线程hold主了它。牛b吧～～ |\n| JBoss System Threads(1)-1| Jboss        | 该线程是一个socket服务，默认端口号为：1099。主要用于接收外部naming service（Jboss JNDI）请求。 |\n| JCA PoolFiller| Jboss        | 该线程主要为JBoss内部提供连接池的托管。 简单介绍一下工作原理 ：Jboss内部凡是有远程连接需求的类，都需要实现ManagedConnectionFactory接口，例如需要做JDBC连接的XAManagedConnectionFactory对象，就实现了该接口。然后将XAManagedConnectionFactory对象，还有其它信息一起包装到InternalManagedConnectionPool对象里面，接着将InternalManagedConnectionPool交给PoolFiller对象里面的列队进行管理。  JCA PoolFiller线程会定期判断列队内是否有需要创建和管理的InternalManagedConnectionPool对象，如果有的话，则调用该对象的fillToMin方法，触发它去创建相应的远程连接，并且将这个连接维护到它相应的连接池里面去。 |\n| JDWP Event Helper Thread| JVM          | JDWP是通讯交互协议，它定义了调试器和被调试程序之间传递信息的格式。它详细完整地定义了请求命令、回应数据和错误代码，保证了前端和后端的JVMTI和JDI的通信通畅。 该线程主要负责将JDI事件映射成JVMTI信号，以达到调试过程中操作JVM的目的。 |\n| JDWP TransportListener: dt_socket| JVM          | 该线程是一个Java Debugger的监听器线程，负责受理客户端的debug请求。通常我们习惯将它的监听端口设置为8787。 |\n| Low MemoryDetector| JVM          | 这个线程是负责对可使用内存进行检测，如果发现可用内存低，分配新的内存空间。 |\n| process reaper| JVM          | 该线程负责去执行一个OS命令行的操作。                         |\n| ==Reference Handler==| JVM          | JVM在创建main线程后就创建Reference Handler线程，其优先级最高，为10，它主要用于处理引用对象本身（软引用、弱引用、虚引用）的垃圾回收问题。 |\n| SurrogateLockerThread(CMS)| JVM          | 这个线程主要用于配合CMS垃圾回收器使用，它是一个守护线程，其主要负责处理GC过程中，Java层的Reference（指软引用、弱引用等等）与jvm内部层面的对象状态同步。这里对它们的实现稍微做一下介绍：这里拿WeakHashMap做例子，将一些关键点先列出来（我们后面会将这些关键点全部串起来）：1.   我们知道HashMap用Entry[]数组来存储数据的，WeakHashMap也不例外,内部有一个Entry[]数组。2.    WeakHashMap的Entry比较特殊，它的继承体系结构为Entry->WeakReference->Reference。3.   Reference里面有一个全局锁对象：Lock，它也被称为pending_lock.  注意：它是静态对象。4.   Reference 里面有一个静态变量：pending。5. Reference 里面有一个静态内部类：ReferenceHandler的线程，它在static块里面被初始化并且启动，启动完成后处于wait状态，它在一个Lock同步锁模块中等待。6.   另外，WeakHashMap里面还实例化了一个ReferenceQueue列队，这个列队的作用，后面会提到。7.   上面关键点就介绍完毕了，下面我们把他们串起来。  假设，WeakHashMap对象里面已经保存了很多对象的引用。JVM在进行CMS GC的时候，会创建一个ConcurrentMarkSweepThread（简称CMST）线程去进行GC，ConcurrentMarkSweepThread线程被创建的同时会创建一个SurrogateLockerThread（简称SLT）线程并且启动它，SLT启动之后，处于等待阶段。CMST开始GC时，会发一个消息给SLT让它去获取Java层Reference对象的全局锁：Lock。直到CMS GC完毕之后，JVM会将WeakHashMap中所有被回收的对象所属的WeakReference容器对象放入到Reference的pending属性当中（每次GC完毕之后，pending属性基本上都不会为null了），然后通知SLT释放并且notify全局锁:Lock。此时激活了ReferenceHandler线程的run方法，使其脱离wait状态，开始工作了。ReferenceHandler这个线程会将pending中的所有WeakReference对象都移动到它们各自的列队当中，比如当前这个WeakReference属于某个WeakHashMap对象，那么它就会被放入相应的ReferenceQueue列队里面（该列队是链表结构）。当我们下次从WeakHashMap对象里面get、put数据或者调用size方法的时候，WeakHashMap就会将ReferenceQueue列队中的WeakReference依依poll出来去和Entry[]数据做比较，如果发现相同的，则说明这个Entry所保存的对象已经被GC掉了，那么将Entry[]内的Entry对象剔除掉。 |\n| taskObjectTimerFactory| JVM          | 顾名思义，该线程就是用来执行任务的。当我们把一个任务交给Timer对象，并且告诉它执行时间，周期时间后，Timer就会将该任务放入任务列队，并且通知taskObjectTimerFactory线程去处理任务，taskObjectTimerFactory线程会将状态为取消的任务从任务列队中移除，如果任务是非重复执行类型的，则在执行完该任务后，将它从任务列队中移除，如果该任务是需要重复执行的，则计算出它下一次执行的时间点。 |\n| VM Periodic Task Thread| JVM          | 该线程是JVM周期性任务调度的线程，它由WatcherThread创建，是一个单例对象。该线程在JVM内使用得比较频繁，比如：定期的内存监控、JVM运行状况监控，还有我们经常需要去执行一些jstat这类命令查看gc的情况，如下：jstat -gcutil 23483 250 7 这个命令告诉jvm在控制台打印PID为：23483的gc情况，间隔250毫秒打印一次，一共打印7次。 |\n| VM Thread| JVM          | 这个线程就比较牛b了，是jvm里面的线程母体，根据hotspot源码（vmThread.hpp）里面的注释，它是一个单例的对象（最原始的线程）会产生或触发所有其他的线程，这个单个的VM线程是会被其他线程所使用来做一些VM操作（如，清扫垃圾等）。在 VMThread的结构体里有一个VMOperationQueue列队，所有的VM线程操作(vm_operation)都会被保存到这个列队当中，VMThread本身就是一个线程，它的线程负责执行一个自轮询的loop函数(具体可以参考：VMThread.cpp里面的void VMThread::loop())，该loop函数从VMOperationQueue列队中按照优先级取出当前需要执行的操作对象(VM_Operation)，并且调用VM_Operation->evaluate函数去执行该操作类型本身的业务逻辑。ps：VM操作类型被定义在vm_operations.hpp文件内，列举几个：ThreadStop、ThreadDump、PrintThreads、GenCollectFull、GenCollectFullConcurrent、CMS_Initial_Mark、CMS_Final_Remark…..有兴趣的同学，可以自己去查看源文件。 |\n\n## Markdown画类图\n\n~~~markdown\n``` mermaid\nclassDiagram\n  classA --|> classB : 继承\n  classC --* classD : 组成\n  classE --o classF : 集合\n  classG --> classH : 关联\n  classI -- classJ : 实线连接\n  classK ..> classL : 依赖\n  classM ..|> classN : 实现\n  classO .. classP : 虚线连接\n```\n~~~\n\n``` mermaid\nclassDiagram\n  classA --|> classB : 继承\n  classC --* classD : 组成\n  classE --o classF : 集合\n  classG --> classH : 关联\n  classI -- classJ : 实线连接\n  classK ..> classL : 依赖\n  classM ..|> classN : 实现\n  classO .. classP : 虚线连接\n```\n\n---\n\n\n\n``` mermaid\nclassDiagram\n\tBird --|> Animal: 继承\n\tWing \"2\" --> \"1\" Bird: 组合\n\tAnimal ..> Oxygen: 依赖\n\tAnimal ..> Water: 依赖\n\tclass Animal {\n\t\t<<interface>>\n\t\t+ Life life\n\t\t+ Breathe(Oxygen)\n\t\t+ Reproduction()\n\t}\n\tclass Bird {\n\t\t+ feature\n\t\t+ beak\n\t\t+ layEgg() Egg\n\t}\n```\n\n## C语言printf的占位符\n\n1. 宽度限定\n\n   `printf(\"%5d\\n\", 123)`，默认==右对齐==，长度为5，前面补2位空格\n\n   `printf(\"%-5d\\n\", 123)`，长度前加个减号-，改为左对齐，后面补2位空格\n\n2. 显示正负号\n\n   负数前面总是显示减号-，正数默认不显示加号+，`printf(\"%+d\\n\", 12)`则会显示加号+\n\n3. 小数宽度\n\n   `printf(\"%.2f\\n\", 2.7814)`表示保留2位有效小数\n\n   `printf(\"%10.4f\\n\", 3.14)`表示小数整体长度为10位，小数部分为4为，整数部分为5位（小数点占1位），左对齐\n\n   `printf(\"%*.*f\\n\", 10, 4, 3.14)`中10和4替换占位符中的星号*，效果同上\n\n4. 切割字符\n\n   `printf(\"%.5s\", \"Hello World\")`输出字符串Hello World的前5个字符\n\n## C语言的for循环作用域\n\n`for`的循环条件部分是一个单独的作用域，跟循环体内部不是同一个作用域。\n\n```c\nfor (int i = 0; i < 5; i++) {\n  int i = 999;\n  printf(\"%d\\n\", i);\n}\n\nprintf(\"%d\\n\", i); // 非法\n```\n\n上面示例中，`for`的循环变量是`i`，循环体内部也声明了一个变量`i`，会优先读取。但由于循环条件部分是一个单独的作用域，所以循环体内部的`i`不会修改掉循环变量`i`，因此这段代码的运行结果就是打印5次`999`。\n\n## MacOS审查WebViews\n\n输入命令\n\n```shell\ndefaults write [NSGlobalDomain替换成对应的全局命名空间] WebkitDeveloperExtras -bool true\ndefaults write -g WebkitDeveloperExtras -bool YES\n```\n\n重启电脑之后就可以在MacOS上像调试网页一样审核元素（右键）。\n\n## Java Web应用中查找资源的路径\n\n在Java Web应用中通过`ServletContext.getResource(String path)`或者`ServletContext.getResourceAsStream(String path)`获取资源，需要传入一个以`/`开头的字符串，该路径是相对于上下文的根路径，或者Web应用的`WEB-INF/lib`目录下的jar包中的`META-INF/resources`目录。但是`WEB-INF/lib`下jar包的查找顺序是不确定的。\n\n## 正则匹配Emoji\n\n```javascript\n /(\\ud83c[\\udf00-\\udfff])|(\\ud83d[\\udc00-\\ude4f\\ude80-\\udeff])|[\\u2600-\\u2B55]/\n```\n\n这是一部分Emoji的Unicode范围值，但是Emoji是不断在新增的，所以上面的正则对于一些较新的Emoji是无法匹配的。\n\n类似`\\d`,`\\W`,`\\s`这类转义字符，`\\p`和`\\P`也是一个特殊Unicode转义字符，表示**一组**Unicode字符，使用时在后面加上`/u`启动Unicode感知模式。\n\n`/\\p{Hex_Digit}/u` 表示匹配十六进制的字符。\n\n`/\\p{Extended_Pictographic}/u` 表示Emoji表情\n\n`/\\p{Emoji}/u` 虽然指定的Emoji，但是`123456789*#`这些值也会被认为是Emoji。",
      "data": {
        "title": "开发日志",
        "date": "2024-12-08 11:35:42",
        "tags": [
          "debug",
          "Java"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "## Spring AOP 注解\n\n在使用spring框架时,通常用它的aop来记录日志,但在spring mvc采用@Controller注解时,对Controller进行Aop拦截不起作用,原因是该注解的Controller已被spring容器内部代理了.\n\n需要对org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter进行Aop才能起作用.经过多次测试可行.",
      "fileName": "kai-fa-ri-zhi"
    },
    {
      "content": "## 下载\n\nApache James 3.x配置和2.x有较大区别，网上的文档大多基于2.x，所以我们也使用2.x版本。\n\n```bash\nwget http://mirrors.tuna.tsinghua.edu.cn/apache//james/server/james-binary-2.3.2.1.tar.gz\n```\n\n<!-- more -->\n\n> 3.x启动的时候经常会报`'priority' init parameter is compulsory`异常，这里可能是因为配置文件里本应该是<priority>元素的地方配置成了<value>导致的。修改`conf/mailetcontainer.xml`文件中\n>\n> ```xml\n> <mailet matcher=\"All\" class=\"WithPriority\">\n> \t<value>8</value>\n> </mailet>\n> ```\n>\n> 为\n>\n> ```xml\n> <mailet matcher=\"All\" class=\"WithPriority\">\n> \t<value>8</value>\n> \t<priority>8</priority>\n> </mailet>\n> ```\n\n## 配置\n\n然后解压之后运行`bin/run.sh`(需要给文件添加可执行权限 chmod +x *.sh)\n\n首次运行之后会把`apps/james.sar`文件解压，然后修改解压后的文件里的配置`apps/james/SAR-INF/config.xml`即可。\n\n### 1、修改postmaster\n\n```xml\n<config>\n   <James>\n      <postmaster>admin@steamfavor.com</postmaster>\n       ...\n    </James>\n</config>\n```\n\n当有错误邮件时，会统一有该账户处理，类似其他邮件服务商提供的`catch-all`服务。\n\n### 2、修改servername\n\n```xml\n<config>\n   <James>\n       <servernames autodetect=\"false\" autodetectIP=\"false\">\n           <servername>steamfavor.com</servername>\n       </servernames>\n```\n\nservername填申请的域名，同时需要关闭autodetect和autodetectIP\n\n> 注册完域名之后，需要添加几个解析：\n>\n> A类：mail -> 邮件服务器IP\n>\n> A类：pop3 -> 邮件服务器IP\n>\n> A类：smtp -> 邮件服务器IP\n>\n> MX：@ -> mail.steamfavor.com\n\n### 3、修改邮件存储为Mysql\n\n打开下面的配置\n\n```xml\n<inboxRepository>\n    <repository destinationURL=\"db://maildb/inbox/\" type=\"MAIL\"/>\n</inboxRepository>\n```\n\n注释掉默认的文件保存配置\n\n```xml\n<inboxRepository>\n    <repository destinationURL=\"file://var/mail/inboxes/\" type=\"MAIL\"/>\n</inboxRepository>\n```\n\n打开数据库的配置并注释掉默认以文件保存的配置\n\n```xml\n<database-connections>\n    <data-source name=\"maildb\" class=\"org.apache.james.util.dbcp.JdbcDataSource\">\n        <driver>com.mysql.jdbc.Driver</driver>\n        <dburl>jdbc:mysql://127.0.0.1:3306/caesar?autoReconnect=true&amp;characterEncoding=latin1&amp;useConfigs=maxPerformance</dburl>\n        <user>root</user>\n        <password>123456</password>\n        <max>20</max>\n    </data-source>\n</database-connections>\n```\n\n### 4、修改dnsserver\n\n配置邮件服务器的dnsserver\n\n```xml\n<dnsserver>\n    <servers>\n        <server>114.114.114.114</server>\n        <server>8.8.8.8</server>\n        <server>8.8.4.4</server>\n    </servers>\n    <autodiscover>false</autodiscover>\n    <authoritative>false</authoritative>\n    <maxcachesize>50000</maxcachesize>\n</dnsserver>\n```\n\n### 5、开启远程管理\n\n开启远程管理可以远程通过4555端口对用户数据进行修改，包括添加用户，删除用户，修改密码等。需要配置展示的名称和登陆的用户名和密码。\n\n```xml\n<remotemanager enabled=\"true\">\n    <port>4555</port>\n    <handler>\n        <helloName autodetect=\"false\">steamfavor.com</helloName>\n        <administrator_accounts>\n            <account login=\"root\" password=\"xh123456**\"/>\n        </administrator_accounts>\n        <connectiontimeout> 60000 </connectiontimeout>\n    </handler>\n</remotemanager>\n```\n\n### 6、开启pop3服务\n\n默认监听110端口，SSL端口995，配置服务器名称。\n\n```xml\n<pop3server enabled=\"true\">\n    <port>110</port>\n    <handler>\n        <helloName autodetect=\"false\">steamfavor.com</helloName>\n        <connectiontimeout>120000</connectiontimeout>\n    </handler>\n</pop3server>\n```\n\n### 7、开启smtp服务\n\n默认监听25端口\n\n```xml\n<smtpserver enabled=\"true\">\n    <port>25</port>\n    <handler>\n        <helloName autodetect=\"true\">myMailServer</helloName>\n        <connectiontimeout>360000</connectiontimeout>\n        <maxmessagesize>0</maxmessagesize>\n    </handler>\n</smtpserver>\n```\n\n配置完成后，重新启动james服务器。",
      "data": {
        "title": "James邮件服务器搭建",
        "date": "2024-12-08 11:28:09",
        "tags": [
          "James",
          "邮件服务器"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "## 下载\n\nApache James 3.x配置和2.x有较大区别，网上的文档大多基于2.x，所以我们也使用2.x版本。\n\n```bash\nwget http://mirrors.tuna.tsinghua.edu.cn/apache//james/server/james-binary-2.3.2.1.tar.gz\n```",
      "fileName": "james-you-jian-fu-wu-qi-da-jian"
    },
    {
      "content": "## Manifest\nChrome扩展的Manifest必须包含`name`、`version`和`manifest_version`属性，对于应用来说，还必须包含`app`属性。\n<!-- more -->\n\nManifest.json模板：\n```javascript\n{\n    \"app\": {\n        \"background\": {\n            \"scripts\": [\"background.js\"]\n        }\n    },\n    \"manifest_version\": 2,\n    \"name\": \"My Extension\",\n    \"version\": \"versionString\",\n    \"default_locale\": \"en\",\n    \"description\": \"A plain text description\",\n    \"icons\": {\n        \"16\": \"images/icon16.png\",\n        \"48\": \"images/icon48.png\",\n        \"128\": \"images/icon128.png\"\n    },\n    \"browser_action\": {\n        \"default_icon\": {\n            \"19\": \"images/icon19.png\",\n            \"38\": \"images/icon38.png\"\n        },\n        \"default_title\": \"Extension Title\",\n        \"default_popup\": \"popup.html\"\n    },\n    \"page_action\": {\n        \"default_icon\": {\n            \"19\": \"images/icon19.png\",\n            \"38\": \"images/icon38.png\"\n        },\n        \"default_title\": \"Extension Title\",\n        \"default_popup\": \"popup.html\"\n    },\n    \"background\": {\n        \"scripts\": [\"background.js\"]\n    },\n    \"content_scripts\": [\n        {\n            \"matches\": [\"http://www.google.com/*\"],\n            \"css\": [\"mystyles.css\"],\n            \"js\": [\"jquery.js\", \"myscript.js\"]\n        }\n    ],\n    \"options_page\": \"options.html\",\n    \"permissions\": [\n        \"*://www.google.com/*\"\n    ],\n    \"web_accessible_resources\": [\n        \"images/*.png\"\n    ]\n}\n```\n\n通过Chrome扩展我们可以对用户当前浏览的页面进行操作，实际上就是对用户当前浏览页面的DOM进行操作。通过Manifest中的`content_scripts`属性可以指定将哪些脚本何时注入到哪些页面中，当用户访问这些页面后，相应脚本即可自动运行，从而对页面DOM进行操作。\n\nManifest的content_scripts属性值为数组类型，数组的每个元素可以包含matches、exclude_matches、css、js、run_at、all_frames、include_globs和exclude_globs等属性。其中matches属性定义了哪些页面会被注入脚本，exclude_matches则定义了哪些页面不会被注入脚本，css和js对应要注入的样式表和JavaScript，run_at定义了何时进行注入，all_frames定义脚本是否会注入到嵌入式框架中，include_globs和exclude_globs则是全局URL匹配，最终脚本是否会被注入由matches、exclude_matches、include_globs和exclude_globs的值共同决定。简单的说，如果URL匹配mathces值的同时也匹配include_globs的值，会被注入；如果URL匹配exclude_matches的值或者匹配exclude_globs的值，则不会被注入。\n\n## 跨域请求：\n| URL                                      | 说明      | 是否允许请求 |\n| ---------------------------------------- | ------- | ------ |\n| http://a.example.com/<br/>http://a.example.com/a.txt | 同域下     | 允许     |\n| http://a.example.com/<br/>http://a.example.com/b/a.txt | 同域下不同目录 | 允许     |\n| http://a.example.com/<br/>http://a.example.com:8080/a.txt | 同域下不同端口 | 不允许    |\n| http://a.example.com/<br/>https://a.example.com/a.txt | 同域下不同协议 | 不允许    |\n| http://a.example.com/<br/>http://b.example.com/a.txt | 不同域下    | 不允许    |\n| http://a.example.com/<br/>http://a.foo.com/a.txt | 不同域下    | 不允许    |\n\nGoogle允许Chrome扩展应用不必受限于跨域限制。但出于安全考虑，需要在Manifest的`permissions`属性中声明需要跨域的权限。\n```javascript\n{\n    ...\n    \"permissions\": [\n        \"*://*.wikipedia.org/*\"\n    ]\n}\n```\n\n## 常驻后台\n在Manifest中指定background域可以使扩展常驻后台。background可以包含三种属性，分别是scripts、page和persistent。如果指定了scripts属性，则Chrome会在扩展启动时自动创建一个包含所有指定脚本的页面；如果指定了page属性，则Chrome会将指定的HTML文件作为后台页面运行。通常我们只需要使用scripts属性即可，除非在后台页面中需要构建特殊的HTML——但一般情况下后台页面的HTML我们是看不到的。persistent属性定义了常驻后台的方式——当其值为true时，表示扩展将一直在后台运行，无论其是否正在工作；当其值为false时，表示扩展在后台按需运行，这就是Chrome后来提出的Event Page。Event Page可以有效减小扩展对内存的消耗，如非必要，请将persistent设置为false。persistent的默认值为true。\n\n如果想在用户打开浏览器之前就让扩展运行，可以在Manifest的permissions属性中加入\"background\"，但除非必要，否则尽量不要这么做，因为大部分用户不喜欢这样。\n\n## 带选项页面的扩展\n有一些扩展允许用户进行个性化设置，这样就需要向用户提供一个选项页面。Chrome通过Manifest文件的options_page属性为开发者提供了这样的接口，可以为扩展指定一个选项页面。当用户在扩展图标上点击右键，选择菜单中的“选项”后，就会打开这个页面。\n\n## 扩展页面间的通信\n有时需要让扩展中的多个页面之间，或者不同扩展的多个页面之间相互传输数据，以获得彼此的状态。比如音乐播放器扩展，当用户鼠标点击popup页面中的音乐列表时，popup页面应该将用户这个指令告知后台页面，之后后台页面开始播放相应的音乐。\n\nChrome提供了4个有关扩展页面间相互通信的接口，分别是`runtime.sendMessage`、`runtime.onMessage`、`runtime.connect`和`runtime.onConnect`。\n\n> Chrome提供的大部分API是不支持在content_scripts中运行的，但runtime.sendMessage和runtime.onMessage可以在content_scripts中运行，所以扩展的其他页面也可以同content_scripts相互通信。\n\n`chrome.runtime.sendMessage(extensionId, message, options, callback)` : extensionId为所发送消息的目标扩展，如果不指定这个值，则默认为发起此消息的扩展本身，message为要发送的内容，options为对象类型，callback是回调函数。\n`chrome.runtime.onMessage.addListener(callback)`：callback接收到的参数有三个，分别是message、sender和sendResponse，即消息内容、消息发送者相关信息和相应函数。\n\n## 储存数据\n### localStorage\n`localStorage`是HTML5新增的方法，它允许JavaScript在用户计算机硬盘上永久储存数据（除非用户主动删除）。但localStorage也有一些限制，首先是localStorage和Cookies类似，都有域的限制，运行在不同域的JavaScript无法调用其他域localStorage的数据；其次是单个域在localStorage中存储数据的大小通常有限制（虽然W3C没有给出限制），对于Chrome这个限制是5MB(通过声明unlimitedStorage权限，Chrome扩展和应用可以突破这一限制)；最后localStorage只能储存字符串型的数据，无法保存数组和对象，但可以通过join、toString和JSON.stringify等方法先转换成字符串再储存。\n### Chrome存储API\nChrome为扩展应用提供了存储API，以便将扩展中需要保存的数据写入本地磁盘。Chrome提供的存储API可以说是对localStorage的改进，它与localStorage相比有以下区别：\n\n+ 如果储存区域指定为sync，数据可以自动同步；\n+ content_scripts可以直接读取数据，而不必通过background页面；\n+ 在隐身模式下仍然可以读出之前存储的数据；\n+ 读写速度更快；\n+ 用户数据可以以对象的类型保存。\n\n> 首先localStorage是基于域名的，这在前面的小节中已经提到过了。而content_scripts是注入到用户当前浏览页面中的，如果content_scripts直接读取localStorage，所读取到的数据是用户当前浏览页面所在域中的。所以通常的解决办法是content_scripts通过runtime.sendMessage和background通信，由background读写扩展所在域（通常是chrome-extension://extension-id/）的localStorage，然后再传递给content_scripts。\n\n使用Chrome存储API必须要在Manifest的permissions中声明\"storage\"，之后才有权限调用。Chrome存储API提供了2种储存区域，分别是sync和local。两种储存区域的区别在于，sync储存的区域会根据用户当前在Chrome上登陆的Google账户自动同步数据，当无可用网络连接可用时，sync区域对数据的读写和local区域对数据的读写行为一致。\n\n> StorageArea = sync / local\n\n1. `chrome.storage.StorageArea.get(keys, callback)`：keys可以是字符串、包含多个字符串的数组或对象。如果keys是字符串，则和localStorage的用法类似；如果是数组，则相当于一次读取了多个数据；如果keys是对象，则会先读取以这个对象属性名为键值的数据，如果这个数据不存在则返回keys对象的属性值（比如keys为{'name':'Billy'}，如果name这个值存在，就返回name原有的值，如果不存在就返回Billy）。如果keys为一个空数组（[]）或空对象（{}），则返回一个空列表，如果keys为null，则返回所有存储的数据。\n2. `chrome.storage.StorageArea.getBytesInUse(keys, callback)`：此处的keys只能为null、字符串或包含多个字符串的数组。\n3. `chrome.storage.StorageArea.set(items, callback)`：items为对象类型，形式为键/值对。items的属性值如果是字符型、数字型和数组型，则储存的格式不会改变，但如果是对象型和函数型的，会被储存为“{}”，如果是日期型和正则型的，会被储存为它们的字符串形式。\n4. `chrome.storage.StorageArea.remove(keys, callback)`：其中keys可以是字符串，也可以是包含多个字符串的数组。\n5. `chrome.storage.StorageArea.clear(callback)`：删除所有数据。\n\nChrome同时还为存储API提供了一个onChanged事件，当存储区的数据发生改变时，这个事件会被激发。callback会接收到两个参数，第一个为changes，第二个是StorageArea。changes是词典对象，键为更改的属性名称，值包含两个属性，分别为oldValue和newValue；StorageArea为local或sync。\n\n> position属性还有另外的三个值，分别是absolute、relative和fixed。如果元素的位置属性为absolute，则它的位置是相对于除static定位以外的父系元素的，如果没有这样的父系元素，则相对于body；如果元素的位置属性为relative，则它的位置是相对于它默认在HTML流中位置的；如果元素的位置属性为fixed，则它的位置是相对于浏览器窗口的。\n\n## Browser Actions\n### 图标\nBrowser Actions可以在Manifest中设定一个默认的图标\n```javascript\n\"browser_action\": {\n    \"default_icon\": {\n        \"19\": \"images/icon19.png\",\n        \"38\": \"images/icon38.png\"\n    }\n}\n```\n一般情况下，Chrome会选择使用19像素的图片显示在工具栏中，但如果用户正在使用视网膜屏幕的计算机，则会选择38像素的图片显示。两种尺寸的图片并不是必须都指定的，如果只指定一种尺寸的图片，在另外一种环境下，Chrome会试图拉伸图片去适应，这样可能会导致图标看上去很难看。另外，default_icon也不是必须指定的，如果没有指定，Chrome将使用一个默认图标。\n\n通过setIcon方法可以动态更改扩展的图标:`chrome.browserAction.setIcon(details, callback)`,其中details的类型为对象，可以包含三个属性，分别是imageData、path和tabId。\n\nimageData是图片的像素数据，可以通过HTML的canvas标签获取到。\npath的值可以是字符串，也可以是对象。如果是对象，结构为{size: imagePath}。imagePath为图片在扩展根目录下的相对位置。\ntabId的值限定了浏览哪个标签页时，图标将被更改。\n### Popup页面\npopup在关闭后，就相当于用户关闭了相应的标签页，这个页面不会继续运行。当用户再次打开这个页面时，所有的DOM和js空间变量都将被重新创建。\n#### 使用带有滚动条的DIV容器。\n\n#### [设计一个更好的滚动条样式。][1]\n\n#### 考虑屏蔽右键菜单。\n\n#### 使用外部引用的脚本。\n#### 不要在popup页面的js空间变量中保存数据。\n\n### 标题和badge\n将鼠标移至扩展图标上，片刻后所显示的文字就是扩展的标题，在Manifest中，browser_action的default_title属性可以设置扩展的默认标题。还可以用JavaScript来动态更改扩展的标题：`chrome.browserAction.setTitle({title: 'This is a new title'});`。\n\nBadge是扩展为用户提供有限信息的另外一种方法，这种方法较标题优越的地方是它可以一直显示，其缺点是只能显示大约4字节长度的信息。Badge目前只能够通过JavaScript设定显示的内容，同时Chrome还提供了更改badge背景的方法。如果不定义badge的背景颜色，默认将使用红色：\n```javascript\nchrome.browserAction.setBadgeBackgroundColor({color: '#0000FF'});\nchrome.browserAction.setBadgeText({text: 'Dog'});\n```\n\n## Content Scripts\n\n```json\n\"content_scripts\": [\n  {\n    //\"matches\": [\"http://*/*\", \"https://*/*\"],\n    // \"<all_urls>\" 表示匹配所有地址\n    \"matches\": [\"<all_urls>\"],\n    // 多个JS按顺序注入\n    \"js\": [\"js/jquery-1.8.3.js\", \"js/content-script.js\"],\n    // JS的注入可以随便一点，但是CSS的注意就要千万小心了，因为一不小心就可能影响全局样式\n    \"css\": [\"css/custom.css\"],\n    // 代码注入的时间，可选值： \"document_start\", \"document_end\", or \"document_idle\"，最后一个表示页面空闲时，默认document_idle\n    \"run_at\": \"document_start\"\n  }\n]\n```\n\n如果没有主动指定`run_at`为`document_start`（默认为`document_idle`），下面这种代码不会生效：\n\n``` javascript\ndocument.addEventListener('DOMContentLoaded', function(){\n\tconsole.log('我被执行了！');\n});\n```\n\ncontent_scripts只能访问下面4种chrome api\n- chrome.extension(getURL , inIncognitoContext , lastError , onRequest , sendRequest)\n- chrome.i18n\n- chrome.runtime(connect , getManifest , getURL , id , onConnect , onMessage , sendMessage)\n- chrome.storage\n\n## 通信\n\n参考 [chrome插件开发攻略](https://www.cnblogs.com/liuxianan/p/chrome-plugin-develop.html#blog-comments-placeholder)\n\n### popup和background\n\n>  popup可以直接调用background中的JS方法，也可以直接访问background的DOM\n\n``` javascript\nfunction test(){\n    alert('我是background！');\n}\n\n// popup.js\nvar bg = chrome.extension.getBackgroundPage();\nbg.test();\n```\n\n> background访问popup（popup是显示状态）\n\n``` javascript\nvar views = chrome.extension.getViews({type:'popup'});\nif(views.length > 0) {\n    console.log(views[0].location.href);\n}\n```\n\n### popup和content\n\n> popup和background其实几乎可以视为一种东西，因为它们可访问的API都一样、通信机制一样、都可以跨域。\n\n### background和content\n\n``` javascript\n// background或popup\nfunction sendMessageToContentScript(message, callback){\n    chrome.tabs.query({active: true, currentWindow: true}, function(tabs)\n    {\n        chrome.tabs.sendMessage(tabs[0].id, message, function(response)\n        {\n            if(callback) callback(response);\n        });\n    });\n}\nsendMessageToContentScript({cmd:'test', value:'你好，我是popup！'}, function(response){\n    console.log('来自content的回复：'+response);\n});\n```\n\n``` javascript\n// content-script.js 接收\nchrome.runtime.onMessage.addListener(function(request, sender, sendResponse){\n    // console.log(sender.tab ?\"from a content script:\" + sender.tab.url :\"from the extension\");\n    if(request.cmd == 'test') alert(request.value);\n    sendResponse('我收到了你的消息！');\n});\n```\n\n### content和background\n\n``` javascript\n// content-script.js 发送\nchrome.runtime.sendMessage({greeting: '你好，我是content-script呀，我主动发消息给后台！'}, function(response) {\n    console.log('收到来自后台的回复：' + response);\n});\n```\n\n``` javascript\n// background.js 或者 popup.js监听来自content-script的消息\nchrome.runtime.onMessage.addListener(function(request, sender, sendResponse)\n{\n    console.log('收到来自content-script的消息：');\n    console.log(request, sender, sendResponse);\n    sendResponse('我是后台，我已收到你的消息：' + JSON.stringify(request));\n});\n```\n\n> content_scripts向`popup`主动发消息的前提是popup必须打开！否则需要利用background作中转；\n>\n> 如果background和popup同时监听，那么它们都可以同时收到消息，但是只有一个可以sendResponse，一个先发送了，那么另外一个再发送就无效；\n\n### content和popup\n\n> 同content和background\n\n### injected script和content-script\n\n> `content-script`和页面内的脚本（`injected-script`自然也属于页面内的脚本）之间唯一共享的东西就是页面的DOM元素，有2种方法可以实现二者通讯：\n>\n> 1. 可以通过`window.postMessage`和`window.addEventListener`来实现二者消息通讯；\n> 2. 通过自定义DOM事件来实现；\n\n+ 方法一：（推荐）\n\n``` javascript\n//injected-script\nwindow.postMessage({\"test\": '你好！'}, '*');\n\n//content script\nwindow.addEventListener(\"message\", function(e){\n    console.log(e.data);\n}, false);\n```\n\n+ 方法二：\n\n``` javascript\n//injected-script\nvar customEvent = document.createEvent('Event');\ncustomEvent.initEvent('myCustomEvent', true, true);\nfunction fireCustomEvent(data) {\n    hiddenDiv = document.getElementById('myCustomEventDiv');\n    hiddenDiv.innerText = data\n    hiddenDiv.dispatchEvent(customEvent);\n}\nfireCustomEvent('你好，我是普通JS！');\n\n//content script\nvar hiddenDiv = document.getElementById('myCustomEventDiv');\nif(!hiddenDiv) {\n    hiddenDiv = document.createElement('div');\n    hiddenDiv.style.display = 'none';\n    document.body.appendChild(hiddenDiv);\n}\nhiddenDiv.addEventListener('myCustomEvent', function() {\n    var eventData = document.getElementById('myCustomEventDiv').innerText;\n    console.log('收到自定义事件消息：' + eventData);\n});\n```\n\n## 长连接\n\n短连接：`chrome.tabs.sendMessage`和`chrome.runtime.sendMessage`\n\n长连接：`chrome.tabs.connect`和`chrome.runtime.connect`\n\n``` javascript\n// popup.js\ngetCurrentTabId((tabId) => {\n    var port = chrome.tabs.connect(tabId, {name: 'test-connect'});\n    port.postMessage({question: '你是谁啊？'});\n    port.onMessage.addListener(function(msg) {\n        alert('收到消息：'+msg.answer);\n        if(msg.answer && msg.answer.startsWith('我是'))\n        {\n            port.postMessage({question: '哦，原来是你啊！'});\n        }\n    });\n});\n\n// content-script.js\n// 监听长连接\nchrome.runtime.onConnect.addListener(function(port) {\n    console.log(port);\n    if(port.name == 'test-connect') {\n        port.onMessage.addListener(function(msg) {\n            console.log('收到长连接消息：', msg);\n            if(msg.question == '你是谁啊？') port.postMessage({answer: '我是你爸！'});\n        });\n    }\n});\n```\n\n## 动态注入\n\n### script\n\n虽然在`background`和`popup`中无法直接访问页面DOM，但是可以通过`chrome.tabs.executeScript`来执行脚本，从而实现访问web页面的DOM（注意，这种方式也不能直接访问页面JS）\n\n``` json\n{\n    \"name\": \"动态JS注入演示\",\n    ...\n    \"permissions\": [\n        \"tabs\", \"http://*/*\", \"https://*/*\"\n    ],\n    ...\n}\n```\n\n``` javascript\n// 动态执行JS代码\nchrome.tabs.executeScript(tabId, {code: 'document.body.style.backgroundColor=\"red\"'});\n// 动态执行JS文件\nchrome.tabs.executeScript(tabId, {file: 'some-script.js'});\n```\n\n### css\n\n``` json\n{\n    \"name\": \"动态CSS注入演示\",\n    ...\n    \"permissions\": [\n        \"tabs\", \"http://*/*\", \"https://*/*\"\n    ],\n    ...\n}\n```\n\n``` javascript\n// 动态执行CSS代码，TODO，这里有待验证\nchrome.tabs.insertCSS(tabId, {code: 'xxx'});\n// 动态执行CSS文件\nchrome.tabs.insertCSS(tabId, {file: 'some-style.css'});\n```\n\n## 获取标签和窗口\n\n``` javascript\n// 获取当前窗口ID\nchrome.windows.getCurrent(function(currentWindow){\n    console.log('当前窗口ID：' + currentWindow.id);\n});\n\n//获取当前标签页ID\n// 1\nfunction getCurrentTabId(callback){\n    chrome.tabs.query({active: true, currentWindow: true}, function(tabs){\n        if(callback) callback(tabs.length ? tabs[0].id: null);\n    });\n}\n\n//2\nfunction getCurrentTabId2(){\n    chrome.windows.getCurrent(function(currentWindow){\n        chrome.tabs.query({active: true, windowId: currentWindow.id}, function(tabs){\n            if(callback) callback(tabs.length ? tabs[0].id: null);\n        });\n    });\n}\n```\n\n\n\n##右键菜单\n\n要将扩展加入到右键菜单中，首先要在Manifest的permissions域中声明contextMenus权限。\n```javascript\n\"permissions\": [\n    \"contextMenus\"\n]\n```\n同时还要在icons域声明16像素尺寸的图标，这样在右键菜单中才会显示出扩展的图标。\n```javascript\n\"icons\": {\n    \"16\": \"icon16.png\"\n}\n```\nChrome提供了三种方法操作右键菜单，分别是create、update和remove，对应于创建、更新和移除操作。\n\n通常create方法由后台页面来调用，即通过后台页面创建自定义菜单。如果后台页面是Event Page，通常在onInstalled事件中调用create方法。\n\n右键菜单提供了4种类型，分别是普通菜单、复选菜单、单选菜单和分割线，其中普通菜单还可以有下级菜单。连续相邻的单选菜单会被自动认为是对同一设置的选项，同时单选菜单会自动在两端生成分割线。下面的代码生成了一系列的菜单：\n\n我们还可以定义自定义的右键菜单在何时显示，比如当用户选择文本时，或者在超级链接上单击右键时。下面的代码定义当用户在超级链接上点击右键时，在菜单中显示“My Menu”菜单：\n```javascript\nchrome.contextMenus.create({\n    type: 'normal',\n    title: 'My Menu',\n    contexts: ['link']\n});\n```\ncontexts域的值是数组型的，也就是说我们可以定义多种情况下显示自定义菜单，完整的选项包括all、page、frame、selection、link、editable、image、video、audio和launcher，默认情况下为page，即在所有的页面唤出右键菜单时都显示自定义菜单。其中launcher只对Chrome应用有效，如果包含launcher选项，则当用户在chrome://apps/或者其他地方的应用图标点击右键，将显示相应的自定义菜单。需要注意的是，all选项不包括launcher。\n\nupdate方法可以动态更改菜单属性，指定需要更改菜单的id和所需要更改的属性即可。remove方法可以删除指定的菜单，removeAll方法可以删除所有的菜单。\n\n## 桌面提醒\n要使用桌面提醒功能，需要在Manifest中声明notifications权限。\n```javascript\n\"permissions\": [\n    \"notifications\"\n]\n```\n创建桌面提醒非常容易，只需指定标题、内容和图片即可。桌面系统窗口创建之后是不会立刻显示出来的，为了让其显示，还要调用show方法：`notification.show();`\n\n> 需要注意的是，对于要在桌面窗口中显示的图片，必须在Manifest的web_accessible_resources域中进行声明，否则会出现图片无法打开的情况:\n```javascript\n\"web_accessible_resources\": [\n    \"icon48.png\"\n]\n```\n\n\n>如果希望images文件夹下的所有png图片都可被显示，可以通过如下声明实现：\n```javascript\n\"web_accessible_resources\": [\n    \"images/*.png\"\n]\n```\n桌面提醒窗口提供了四种事件：ondisplay、onerror、onclose和onclick。\n\n[桌面通知](http://www.cnblogs.com/champagne/p/4831874.html)\n\n## Omnibox\n要使用omnibox需要在Manifest的omnibox域指定keyword：`\"omnibox\": { \"keyword\" : \"hamster\" }`。同时最好指定一个16像素的图标，当用户键入关键字后，这个图标会显示在地址栏的前端。`\"icons\": {\"16\": \"icon16.png\"}`。\n\nOmnibox有四种事件：onInputStarted、onInputChanged、onInputEntered和onInputCancelled，分别用于监听用户开始输入、输入变化、执行指令和取消输入行为。其中执行指令是指用户敲击回车键或用鼠标点击建议结果。\n\n## 书签\n要在扩展中操作书签，需要在Manifest中声明bookmarks权限：\n```javascript\n\"permissions\": [\n    \"bookmarks\"\n]\n```\n书签对象有8个属性，分别是id、parentId、index、url、title、dateAdded、dateGroupModified和children。这8个属性并不是每个书签对象都具有的，比如书签分类，即一个文件夹，它就不具有url属性。index属性是这个书签在其父节点中的位置，它的值是从0开始的。children属性值是一个包含若干书签对象的数组。dateAdded和dateGroupModified的值是自1970年1月1日至修改时间所经过的毫秒数。只有id和title是书签对象必有的属性，其他的属性都是可选的。id不需要人为干预，它是由Chrome管理的。根的id为'0'。\n\n'0'为根节点id，根节点下是不允许创建书签和书签分组的，它的下面默认只有三个书签分组：书签栏、其他书签和移动设备书签，如果创建时不指定parentId，则所创建的书签会默认加入到其他书签中。\n\n如果创建的书签不包含url属性，则Chrome自动将其视作为书签分类。\n\n通过move方法可以调整书签的位置，这种调整可以是跨越父节点的。\n\n通过update方法可以更改书签属性，包括标题和URL，更新时未指定的属性值将不会更改。\n\n通过remove和removeTree可以删除书签，remove方法可以删除书签和空的书签分组，removeTree可以删除包含书签的书签分组。\n\n通过getTree方法可以获得用户完整的书签树。\n\nsearch方法可以返回匹配指定条件的书签对象，匹配的条件只能字符串。\n\n书签的事件：onCreated、onRemoved、onChanged、onChildrenReordered、onImportBegan、onImportEnded。\n```javascript\nchrome.bookmarks.onCreated.addListener(function(bookmark){\n    console.log(bookmark);\n});\n```\n\n## Cookies\n要管理Cookies，需要在Manifest中声明cookies权限，同时也要声明所需管理Cookies所在的域：\n```javascript\n\"permissions\": [\n    \"cookies\",\n    \"*://*.google.com\"\n]\n```\n如果想要管理所有的Cookies可以声明如下权限：\n```javascript\n\"permissions\": [\n    \"cookies\",\n    \"<all_urls>\"\n]\n```\n\nChrome定义的Cookie对象包含如下属性：name（名称）、value（值）、domain（域）、hostOnly（是否只允许完全匹配domain的请求访问）、path（路径）、secure（是否只允许安全连接调用）、httpOnly（是否禁止客户端调用）、session（是否是session cookie）、expirationDate（过期时间）和storeId（包含此cookie的cookie store的id）。\n\nChrome提供了get和getAll两个方法读取Cookies，get方法可以读取指定name、url和storeId的Cookie，其中storeId可以不指定，但是name和url必须指定。如果在同一URL中包含多个name相同的Cookies，则会返回path最长的那个，如果有多个Cookies的path长度相同，则返回创建最早的那个。\n\n> 如果cookie的secure属性值为true，那么通过get获取时url应该是https协议。\n\nset方法可以设置Cookie：\n```javascript\nchrome.cookies.set({\n    'url':'http://github.com/test_cookie',\n    'name':'TEST',\n    'value':'foo',\n    'secure':false,\n    'httpOnly':false\n}, function(cookie){\n    console.log(cookie);\n});\n```\n\n## 历史\n要使用history接口，需要在Manifest中声明history权限：\n```javascript\n\"permissions\": [\n    \"history\"\n]\n```\n管理历史的方法包括search、getVisits、addUrl、deleteUrl、deleteRange和deleteAll。其中search和getVisits用于读取历史，addUrl用于添加历史，deleteUrl、deleteRange和deleteAll用于删除历史。\n\n## 管理扩展与应用\n除了通过chrome://extensions/管理Chrome扩展和应用外，也可以通过Chrome的management接口管理。management接口可以获取用户已安装的扩展和应用信息，同时还可以卸载和禁用它们。通过management接口可以编写出智能管理扩展和应用的程序。\n\n要使用management接口，需要在Manifest中声明management权限：\n```javascript\n\"permissions\": [\n    \"management\"\n]\n```\n读取用户已安装扩展和应用的信息。Management提供了两个方法获取用户已安装扩展应用的信息，分别是getAll和get。\nexInfo是扩展信息对象，其结构如下：\n```javascript\n{\n    id: 扩展id,\n    name: 扩展名称,\n    shortName: 扩展短名称,\n    description: 扩展描述,\n    version: 扩展版本,\n    mayDisable: 是否可被用户卸载或禁用,\n    enabled: 是否已启用,\n    disabledReason: 扩展被禁用原因,\n    type: 类型,\n    appLaunchUrl: 启动url,\n    homepageUrl: 主页url,\n    updateUrl: 更新url,\n    offlineEnabled: 离线是否可用,\n    optionsUrl: 选项页面url,\n    icons: [{\n        size: 图片尺寸,\n        url: 图片URL\n    }],\n    permissions: 扩展权限,\n    hostPermissions: 扩展有权限访问的host,\n    installType: 扩展被安装的方式\n}\n```\n\n## 标签\n标签对象的结构：\n```javascript\n{\n    id: 标签id,\n    index: 标签在窗口中的位置，以0开始,\n    windowId: 标签所在窗口的id,\n    openerTabId: 打开此标签的标签id,\n    highlighted: 是否被高亮显示,\n    active: 是否是活动的,\n    pinned: 是否被固定,\n    url: 标签的URL,\n    title: 标签的标题,\n    favIconUrl: 标签favicon的URL,\n    status :标签状态，loading或complete,\n    incognito: 是否在隐身窗口中,\n    width: 宽度,\n    height: 高度,\n    sessionId: 用于sessions API的唯一id\n}\n```\nChrome通过tabs方法提供了管理标签的方法与监听标签行为的事件，大多数方法与事件是无需声明特殊权限的，但有关标签的url、title和favIconUrl的操作（包括读取），都需要声明tabs权限。\n```javascript\n\"permissions\": [\n    \"tabs\"\n]\n```\n获取标签信息。Chrome提供了三种获取标签信息的方法，分别是get、getCurrent和query。get方法可以获取到指定id的标签，getCurrent则获取运行的脚本本身所在的标签，query可以获取所有符合指定条件的标签。\n```javascript\nchrome.tabs.get(tabId, function(tab){\n    console.log(tab);\n});\n\nchrome.tabs.getCurrent(function(tab){\n    console.log(tab);\n});\n```\nquery方法可以指定的匹配条件如下：\n```javascript\n{\n    active: 是否是活动的,\n    pinned: 是否被固定,\n    highlighted: 是否正被高亮显示,\n    currentWindow: 是否在当前窗口,\n    lastFocusedWindow: 是否是上一次选中的窗口,\n    status: 状态，loading或complete,\n    title: 标题,\n    url: 所打开的url,\n    windowId: 所在窗口的id,\n    windowType: 窗口类型，normal、popup、panel或app,\n    index: 窗口中的位置\n}\n```\n创建标签。创建标签与在浏览器中打开新的标签行为类似，但可以指定更加丰富的信息，如URL、窗口中的位置和活动状态等。\n```javascript\nchrome.tabs.create({\n    windowId: wId,\n    index: 0,\n    url: 'http://www.google.com',\n    active: true,\n    pinned: false,\n    openerTabId: tId\n}, function(tab){\n    console.log(tab);\n});\n```\n获取指定窗口活动标签可见部分的截图。\n```javascript\nchrome.tabs.captureVisibleTab(windowId, {\n    format: 'jpeg',\n    quality: 50\n}, function(dataUrl){\n    window.open(dataUrl, 'tabCapture');\n});\n```\n其中format还支持png，如果指定为png，则quality属性会被忽略。如果指定jpeg格式，quality的取值范围为0-100，数值越高，图片质量越好，体积也越大。扩展只有声明activeTab或<all_url>权限能获取到活动标签的截图：\n```javascript\n\"permissions\": [\n    \"activeTab\"\n]\n```\ncontent_scripts可以向匹配条件的页面注入JS和CSS，但是却无法向用户指定的标签注入。通过executeScript和insertCSS可以做到向指定的标签注入脚本。\n```javascript\nchrome.tabs.executeScript(tabId, {\n    file: 'js/ex.js',\n    allFrames: true,\n    runAt: 'document_start'\n}, function(resultArray){\n    console.log(resultArray);\n});\n```\n也可以直接注入代码：\n```javascript\nchrome.tabs.executeScript(tabId, {\n    code: 'document.body.style.backgroundColor=\"red\"',\n    allFrames: true,\n    runAt: 'document_start'\n}, function(resultArray){\n    console.log(resultArray);\n});\n```\n向指定的标签注入CSS：\n```javascript\nchrome.tabs.insertCSS(tabId, {\n    file: 'css/insert.css',\n    allFrames: false,\n    runAt: 'document_start'\n}, function(){\n    console.log('The css has been inserted.');\n});\n```\nexecuteScript和insertCSS方法中runAt的值可以是'document_start'、'document_end'或'document_idle'。\n\n与指定标签中的内容脚本（content script）通信。前面章节介绍过扩展页面间的通信，我们也可以与指定的标签通信，方法如下：\n```javascript\nchrome.tabs.sendMessage(tabId, message, function(response){\n    console.log(response);\n});\n```\n\n> 后台页面主动与content_scripts通信需要使用chrome.tabs.sendMessage方法\n\n监控标签行为的事件包含onCreated、onUpdated、onMoved、onActivated、onHighlighted、onDetached、onAttached、onRemoved和onReplaced。\n\n## Override Pages\n目前支持替换的页面包含Chrome的书签页面、历史记录和新标签页面。\n只需在Manifest中进行声明即可（一个扩展只能替换一个页面）：\n```javascript\n\"chrome_url_overrides\" : {\n    \"bookmarks\": \"bookmarks.html\"\n}\n\n\"chrome_url_overrides\" : {\n    \"history\": \"history.html\"\n}\n\n\"chrome_url_overrides\" : {\n    \"newtab\": \"newtab.html\"\n}\n```\n\n## 下载\nChrome提供了downloads API，扩展可以通过此API管理浏览器的下载功能，包括暂停、搜索和取消等。扩展使用downloads接口需要在Manifest文件中声明downloads权限：\n```javascript\n\"permissions\": [\n    \"downloads\"\n]\n```\n`chrome.downloads.download(options, callback);`其中options的完整结构如下：\n```javascript\n{\n    url: 下载文件的url,\n    filename: 保存的文件名,\n    conflictAction: 重名文件的处理方式,\n    saveAs: 是否弹出另存为窗口,\n    method: 请求方式（POST或GET），\n    headers: 自定义header数组,\n    body: POST的数据\n}\n```\n## 网络请求\n![网络请求的整个生命周期所触发事件的时间顺序][2]\n要对网络请求进行操作，需要在Manifest中声明webRequest权限以及相关被操作的URL。如需要阻止网络请求，需要声明webRequestBlocking权限。\n```javascript\n\"permissions\": [\n    \"webRequest\",\n    \"webRequestBlocking\",\n    \"*://*.google.com/\"\n]\n```\n```javascript\nchrome.webRequest.onBeforeRequest.addListener(\n    function(details){\n        return {cancel: true};\n    },\n    {\n        urls: [\n            \"*://bad.example.com/*\"\n        ]\n    },\n    [\n        \"blocking\"\n    ]\n);\n```\n> header中的如下属性是不支持更改的：Authorization、Cache-Control、Connection、Content-Length、Host、If-Modified-Since、If-None-Match、If-Range、Partial-Data、Pragma、Proxy-Authorization、Proxy-Connection和Transfer-Encoding。\n\n所有事件中，回调函数所接收到的信息对象均包括如下属性：requestId、url、method、frameId、parentFrameId、tabId、type和timeStamp。其中type可能的值包括\"main_frame\"、\"sub_frame\"、\"stylesheet\"、\"script\"、\"image\"、\"object\"、\"xmlhttprequest\"和\"other\"。\n\n## 代理\nChrome浏览器提供了代理设置管理接口，这样可以让扩展来做到更加智能的代理设置。要让扩展使用代理接口，需要声明proxy权限：\n```javascript\n\"permissions\": [\n    \"proxy\"\n]\n```\n通过chrome.proxy.settings.set方法可以设置代理服务器，该方法需要两个参数，一个是代理设置对象，另一个是回调函数。\n\n代理设置对象包括mode属性、rules属性和pacScript属性。其中mode属性为代理模式，可选的值有'direct'（直接连接，即不通过代理）、'auto_detect'（通过WPAD协议自动获取pac脚本）、'pac_script'（使用指定的pac脚本）、'fixed_servers'（固定的代理服务器）和'system'（使用系统的设置）。rules属性和pacScript属性都是可选的，rules指定了不同的协议通过不同的代理。\n\n## API \n\n`chrome.alarms.*`\n\n通过此制定周期性任务，需在manifest.json文件中声明权限\n\n``` json\n\"permissions\": [\n\n\t\"alarms\"\n\n],\n```\n\n`chrome.alarms.create(string name, object alarmInfo)`\n\n| 属性              | 类型     | 注释             |\n| --------------- | ------ | -------------- |\n| when            | double | 触发alarm时间，ms   |\n| delayInMinutes  | double | 延迟时间，minute    |\n| periodInMinutes | double | 周期性时间间隔，minute |\n\n- 获取指定名字的alarm\n  `chrome.alarms.get(string name, function(Alarm alarm) {...})`\n\n- 获取所有alarm\n\n  `chrome.alarms.getAll(function(array of Alarm alarms) {...})`\n\n- 通过名字删除alarm\n\n  `chrome.alarms.clear(string name, function(boolean wasCleared) {...})`\n\n- 清除所有alarm\n\n  `chrome.alarms.clearAll(function(boolean wasCleared) {...})`\n\n- 监听alarm发生的事件，用于event page\n\n  `chrome.alarms.onAlarm.addListener(function(Alarm alarm) {...})`\n\n[1]: http://css-tricks.com/custom-scrollbars-in-webkit/\n[2]: http://storage1.imgchr.com/CflJs.jpg",
      "data": {
        "title": "Chrome 插件开发",
        "date": "2024-12-08 11:16:11",
        "tags": [
          "chrome 插件",
          "JavaScript"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "## Manifest\nChrome扩展的Manifest必须包含`name`、`version`和`manifest_version`属性，对于应用来说，还必须包含`app`属性。",
      "fileName": "chrome-cha-jian-kai-fa"
    },
    {
      "content": "# 〇、从入门到入狱\n\n [中国爬虫违法违规案例汇总](https://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China)\n\n![](https://xuhang.github.io/post-images/1733627425302.png)\n\n# 一、什么是爬虫\n![什么是爬虫](https://xuhang.github.io/post-images/1733627403396.png)\n\n# 二、爬虫的分类\n\n<!-- more -->\n\n搜索引擎：百度、谷歌\n\n数据采集：天眼查、企查查\n\n薅羊毛：抢票机器人、秒杀软件，比价软件，微博僵尸粉\n\n……\n\n# 三、爬虫与反爬虫\n\n1. 君子协议：robots.txt\n\n   [www.baidu.com/robots.txt](http://www.baidu.com/robots.txt)\n\n![robots协议](https://xuhang.github.io/post-images/1733627377901.png)\n   ![image-20220303093748092]\n\n1. 最简单的爬虫\n\nPython版\n\n```python\nimport requests\nrsp = requests.get('http://www.httpbin.org/user-agent')\n```\n\nJava版\n\n```java\n@Test\npublic void testHttpclient() throws IOException {\n    CloseableHttpClient client = HttpClientBuilder.create().build();\n    HttpGet get = new HttpGet(\"http://www.httpbin.org/user-agent\");\n    CloseableHttpResponse response = client.execute(get);\n    HttpEntity entity = response.getEntity();\n    String string = EntityUtils.toString(entity);\n    System.out.println(string);\n}\n    \n@Test\npublic void testHtmlUnit() throws IOException {\n    WebClient edge = new WebClient(BrowserVersion.FIREFOX);\n    edge.getOptions().setCssEnabled(false);\n    edge.getOptions().setJavaScriptEnabled(true);\n    edge.getOptions().setThrowExceptionOnFailingStatusCode(false);\n    edge.getOptions().setThrowExceptionOnScriptError(false);\n    edge.waitForBackgroundJavaScript(600*1000);\n    UnexpectedPage page = edge.getPage(\"http://www.httpbin.org/user-agent\");\n    System.out.println(page.getWebResponse().getContentAsString());\n}\n```\n\n加入依赖\n\n```xml\n<dependency>\n    <groupId>org.apache.httpcomponents</groupId>\n    <artifactId>httpclient</artifactId>\n    <version>4.5.3</version>\n</dependency>\n<dependency>\n    <groupId>net.sourceforge.htmlunit</groupId>\n    <artifactId>htmlunit</artifactId>\n    <version>2.45.0</version>\n</dependency>\n```\n\n但是这样的爬虫很容易通过检测UA头被发现，服务器就可以对这样的爬虫做出反爬的措施。\n\n2. 修改爬虫的UA\n\nPython版\n\n```python\nheader = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36'\n}\nrsp = requests.get('http://www.httpbin.org/user-agent', headers = header)\n```\n\nJava版\n\n```java\n@Test\npublic void testHttpclient() throws IOException {\n    CloseableHttpClient client = HttpClientBuilder.create().build();\n    HttpGet get = new HttpGet(\"http://www.httpbin.org/user-agent\");\n    get.setHeader(\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36\");\n    CloseableHttpResponse response = client.execute(get);\n    HttpEntity entity = response.getEntity();\n    String string = EntityUtils.toString(entity);\n    System.out.println(string);\n}\n```\n\n3. 控制爬虫的频率\n\n修改UA只是第一步，服务器还会针对每个ip地址的请求频率来识别爬虫，比如一分钟内请求几百几千次，一天24小时不间断的请求，这些特征都可以被识别为爬虫程序。所以在大规模抓取数据时，需要对降低抓取频率，比如每次请求后sleep 3~5 秒；但是这样会大大降低抓取的效率，所以这里就需要用到代理IP池——当然也可以通过部署集群的方式来提高爬的速度\n\n代理服务器会转发爬虫请求，这样服务器针对IP的限制就会被绕过。\n\n```python\nproxy = {\n    'http': '',\n    'https': ''\n}\nheader = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36'\n}\nrsp = requests.get('http://www.httpbin.org/user-agent', headers = header, proxies = proxy)\n```\n\n>代理IP分透明代理，匿名代理和高匿代理，透明代理其实并不会隐藏爬虫的真实IP，匿名代理会在请求头里带上爬虫的IP，有一定几率会被识别，高匿代理则会完全隐藏爬虫的IP，更推荐使用。\n\n据说现在互联网上50%以上的流量都是由爬虫产生的，针对一些热门资源这一比例可以高达98%以上。针对这种情况，服务器会需要用户登录才能访问，而简单的登录验证就是将浏览器上的cookie和服务器session绑定起来。\n\n4. 验证码（**C**ompletely **A**utomated **P**ublic **T**uring test to tell **C**omputers and **H**umans **A**part，简称**CAPTCHA**）\n![图片验证码开始显“威力”](https://xuhang.github.io/post-images/1733627453270.jpeg)\n\n![图片验证码开始显“威力”](https://xuhang.github.io/post-images/1733627479206.jpeg)\n\n对于简单的字母和数字组成的验证码，可以通过自己训练模型来识别，或者使用第三方的打码平台来验证。\n![captcha](https://xuhang.github.io/post-images/1733627496743.jpeg)\n\n\n```python\ndef bypassWithDama():\n    rec_url = \"http://pred.fateadm.com\"\n    tm = str(int(time.time()))\n    sign = CalcSign(pd_id, pd_key, tm)\n    asign = CalcSign(app_id, app_key, tm)\n    param = {\n        \"user_id\": pd_id,\n        \"timestamp\": tm,\n        \"appid\": app_id,\n        \"sign\":sign,\n        \"asign\": asign,\n        \"predict_type\": \"30600\",\n        \"up_type\": \"mt\"\n    }\n    url = rec_url + \"/api/capreg\"\n    img_data = open('/Users/xuhang/Desktop/captcha.jpeg', 'rb')\n    files = {\n        \"img_data\": ('img_data', img_data)\n    }\n    header = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'\n    }\n    rsp_data = requests.post(url, param, files=files, headers=header)\n    print('【斐斐打码】' + rsp_data.text)\n    return rsp_data.json()\n\n// 【斐斐打码】{\"RetCode\":\"0\",\"ErrMsg\":\"\",\"RequestId\":\"20220303110050322d81ed0007a58386\",\"RspData\":\"{\\\"result\\\": \\\"yfx5\\\"}\"}\n```\n\n对于像Google的CAPTCHA或者 Intuition的hCaptcha或者arkoselabs的FunCaptcha，这类复杂的验证码，需要识别图片中的物品并点击符合要求的图片，或者将图片旋转到正确的角度，可以使用打码平台的人工打码，由人工完成后将结果返回。\n\n1. 带上登录后的cookie\n\n对于一般安全性不强的网站，并没有针对登录验证做太多的设计，所以爬虫很容易就能实现带cookie访问。\n\n```python\nheader = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36'\n}\nurl = 'https://store.steampowered.com/account/'\nrsp = requests.get(url, headers=header, verify=False)\n```\n\n将响应的文本复制到文件中保存为html格式，然后用浏览器打开，虽然是乱码，但是可以看到Login的按钮，说明是未登录的状态。登录steam之后将请求中的cookie复制出来，修改header后再次请求\n\n```python\nheader = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36',\n    'Cookie': '...'\n}\n```\n\n可以从页面中找到该账号绑定的邮箱和余额等信息，说明登录成功。虽然可以爬到有限制的信息，但是爬虫和账号绑定了，这里代理就不起作用了，频繁爬取的话可能会导致被封号，这里就需要准备多个账号了，好在爬虫可以代替人工申请账号，只是成本会高一些。\n\n虽然上面的操作或多或少可以骗过服务器的检测，但是相比与真实的浏览器，爬虫程序还是会在很多方面存在差异。比如真是浏览器通常会有document，setInterval等对象，爬虫程序缺少这些对象可能会导致某些关键性步骤无法执行，从而被服务器识别出来。另外有些网站需要post提交请求数据，但是某些必需的参数又是经过各种复杂加密混淆之后的结果，有时候可能追踪一个参数忙碌了好几天，成功的爬取了一天，第二天网站改参数了。针对这种情况，我们可以使用真实的浏览器的爬取。\n\n> 如果是个人网站的站长，通常没有过多的精力来对抗爬虫，简单点的方法就是上面的随机生成验证码，然后加点干扰线。或者接入第三方的验证码平台，比如极验。但是验证码通常只用在有敏感操作的地方，不可能每个请求都要验证码，这时就可以使用网页防火墙之类的服务，通过检查客户端的引擎和一些特征来识别是不是爬虫，这样就可以拦截大部分的爬虫了。\n> ![](https://xuhang.github.io/post-images/1733627531440.png)\n>\n> 该防火墙最开始是通过浏览器的引擎进行一些计算任务，只有计算正确才能成功跳转，但是已经有人成功用python模拟了计算过程，目前使用的是hCaptcha的验证码。\n> ![](https://xuhang.github.io/post-images/1733627544953.png)\n>\n> \n\n1. headless浏览器\n\n   ![](https://xuhang.github.io/post-images/1733627713500.png)\n\n[PhantomJS](https://phantomjs.org/)是一个可以执行javascript脚本的无头网页浏览器，由于Chrome浏览器在17年开始支持无头模式，PhantomJS的作者已经停止维护了，推荐大家去使用Chrome。\n\n下面是PhantomJS的一个demo\n\n```javascript\nvar page = require('webpage').create();\npage.open('http://www.google.com', function() {\n    setTimeout(function() {\n        page.render('google.png');\n        phantom.exit();\n    }, 200);\n});\n```\n\n这里推荐使用Selenium，这个工具可以驱动Chrome、Firefox、IE、Safari、Opera和PhantomJS，并且提供多种语言的版本，只需要安装相应的浏览器并指定浏览器的驱动路径即可。\n\n```python\n# -*- coding: utf-8 -*-\nfrom selenium import webdriver\nimport time\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver import ActionChains\n\n\"\"\"\n如果控制台出现乱码，尝试修改编码格式：chcp <编码>\n65001\tUTF-8\n950\t\t繁体中文\n936\t\tGBK\n437\t\tMS-DOS\n\n\"\"\"\n\noptions = webdriver.ChromeOptions()\n# 以Headless模式启动\n# options.add_argument('headless')\n# 窗口大小（通过截图可以反映窗口大小）\noptions.add_argument('window-size=1200x600')\n# 设置User-Agent\noptions.add_argument('user-agent=Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36')\n\n# 创建实例\nbrowser = webdriver.Chrome(chrome_options = options, executable_path='/Users/xuhang/Downloads/chromedriver')\n\n# 窗口最大化\nbrowser.maximize_window()\n\n# Part Ⅰ\n\n# 请求网址\nbrowser.get(url = \"https://www.smzdm.com\")\ntime.sleep(2)\n\n# 截图\nbrowser.save_screenshot('smzdm_1.png')\n\n# 下拉滑动页面 - 通过JS脚本\nbrowser.execute_script(\"window.scrollTo(10000, document.body.scrollHeight)\")\n\n# 下拉滑动页面 - 通过模拟鼠标移动\n# from selenium.webdriver.support.wait import WebDriverWait\n# WebDriverWait(browser, 20, 0.5).until(lambda x: x.find_element_by_id('feed-main-list'))\n# actionChain = browser.find_element_by_css_selector(\"#feed-main-list li:last-child\")\n# ActionChains(browser).move_to_element(actionChain).perform()\nbrowser.save_screenshot('smzdm_2.png')\n\n# 新开窗口打开百度首页\nbrowser.execute_script(\"window.open('https://www.baidu.com')\")\nhandlers = browser.window_handles\n# print(handlers)\n\n# 切换到第2个窗口\nbrowser.switch_to_window(handlers[1])\n\n# 在第2个窗口操作\nbrowser.find_element_by_id('kw').send_keys('selenium')\nbrowser.find_element_by_id('su').click()\n\n# 获取cookies\ncookies = browser.get_cookies()\ncookie = browser.get_cookie('BAIDUID')\ntime.sleep(2)\nbrowser.save_screenshot('baidu_1.png')\nbrowser.switch_to_window(handlers[0])\ntime.sleep(2)\nprint(cookie)\n\nbrowser.quit()\n```\n\n到这里，配合上代理IP和账号登陆已经可以绕开大部分的网站验证了，但是即使用上了真实的浏览器，Selenium操控的浏览器还是会暴露出一部分特征。比如`window.navigator.webdriver`属性，这个属性是正常浏览器没有的，但是Chrome Headless里有，虽然可以通过参数关闭该属性，但是仍有其他属性会暴露出来。\n\n除了以上用到的Selenium，还有Pupperteer和Python版本的Pypperteer，器中Pupperteer是Google官方推出的基于Chrome DevTool protocol 协议的Nodejs包，通常在Selenium失败之后尝试使用Pupperteer，还是不行可以考虑开发Chrome的插件来爬，因为Chrome插件是运行在真正的浏览器上面，和平时使用的一样，它还能使用浏览器以往的缓存，不容易被识别出来。\n\n![selenium和puppeteer](%E5%9B%BE%E7%89%87%E7%B4%A0%E6%9D%90%E6%96%87%E4%BB%B6%E5%A4%B9/selenium%E5%92%8Cpuppeteer.png)\n\n# 四、扩展\n\n 反爬虫除了在服务端对可疑请求进行拦截，还可以在客户端增加爬虫的开发难度，其中就包括代码混淆、干扰调试、数据投毒、图片替换数据、字体乱序。\n\n1. 代码混淆\n\n   ![base62加密](https://xuhang.github.io/post-images/1733627635187.png)\n\nbase62加密最明显的特征是以`eval(function(p,a,c,k,e,r))`开头，这样加密后的代码没有可读性，对于不熟悉此加密的人有一定难度，但是由于此方法最终要执行eval方法，所以只需要通过console.log将内容打印出来就是加密前的代码。\n\n1. 干扰调试\n\n![image-20220308103521169](https://xuhang.github.io/post-images/1733627650683.png)\n\n由于爬虫必须通过找出数据接口才能进行数据抓取，对于浏览器最基础的操作就是打开DevTools来分析请求和数据，所以一旦发现用户打开DevTools就可以做一些干扰来增加难度。比如`debugger;`本来是开发人员用来在调试代码时使用的命令，该命令会强制在此处打断点，所以可以对爬虫的开发人员进行一定的干扰。但是此方法也可以通过浏览器的停用断点使其失效。类似的方法还有检测到DevTools后立即删除所有关键的信息，这样也就不会暴露数据接口了。（微信读书用到了此方法）\n\n1. 数据投毒\n\n此方法会有一定几率误伤到正常用户，所以使用得很少，而且使用也必须很谨慎。如果后台在检测到是爬虫之后，将原本正确的数据替换掉，让爬虫拿到的是毫无意义的数据，这样爬虫方就会因为这些异常数据做出错误的策略。\n\n4. 图片替换\n\n因为爬虫主要抓取的是文本类型的数据，比如价格、邮箱等，而爬虫处理文本数据的成本是很低的。如果将关键的数据用图片进行替换，图片上展示的是正常的数据，这样不会对正常访问的用户造成影响，只会增加爬虫获取数据的难度。\n\n5. 字体乱序\n\n这种方法和数据投毒的效果类似，但是不会误伤正常访问的用户。具体操作是在页面上加载乱序过的字体文件，但是乱序的规则后台是知道的，后台在返回数据的时候只要根据乱序的规则做一次反向的替换，就能让html的源数据和页面上展示的不一致，而爬虫是根据html源里的数据来进行处理的。\n\n[在线字体编辑器-JSON在线编辑器 (qqe2.com)](https://font.qqe2.com/)\n\n![](https://xuhang.github.io/post-images/1733627666181.png)\n\n这个字体文件里，数字7和1交换了顺序（为演示方便，只乱序了2个数字，通常是越乱越好），在页面上定义这个字体并指定文件路径，然后使用定义的字体。\n\n![](https://xuhang.github.io/post-images/1733627679511.png)\n\n源文件里的数字是`1234567890`，页面上展示的是`7234561890`。\n\n![](https://xuhang.github.io/post-images/1733627688770.png)\n\n![](https://xuhang.github.io/post-images/1733627698376.png)\n\n对于中文也是类似的，只不过中文字符过多，乱序之后映射关系复杂，会让维护变得困难。\n\n---",
      "data": {
        "title": "爬虫",
        "date": "2024-12-08 11:06:46",
        "tags": [
          "爬虫",
          "chrome 插件",
          "python"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "# 〇、从入门到入狱\n\n [中国爬虫违法违规案例汇总](https://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China)\n\n![](https://xuhang.github.io/post-images/1733627425302.png)\n\n# 一、什么是爬虫\n![什么是爬虫](https://xuhang.github.io/post-images/1733627403396.png)\n\n# 二、爬虫的分类",
      "fileName": "pa-chong"
    },
    {
      "content": "## 软件开发的7条原则\n\n###### 开闭原则\n\n开闭原则（Open Closed Principle，OCP）由勃兰特·梅耶（Bertrand Meyer）提出，他在 1988 年的著作《面向对象软件构造》（Object Oriented Software Construction）中提出：软件实体应当==对扩展开放，对修改关闭==（Software entities should be open for extension，but closed for modification），这就是开闭原则的经典定义。\n\n<!-- more -->\n\n> 当应用的需求改变时，在不修改软件实体的源代码或者二进制代码的前提下，可以扩展模块的功能，使其满足新的需求。\n\n软件实体包括以下几个部分：==项目中划分出的模块==、==类与接口==、==方法==。\n\n闭原则是面向对象程序设计的==终极目标==，它使软件实体拥有一定的==适应性==和==灵活性==的同时具备==稳定性==和==延续性==。具体来说，其作用如下：\n\n1. 对软件==测试==的影响\n\n   软件遵守开闭原则的话，软件测试时只需要==对扩展的代码进行测试==就可以了，因为原有的测试代码仍然能够正常运行。\n\n2. 可以提高代码的==可复用性==\n\n   粒度越小，被复用的可能性就越大；在面向对象的程序设计中，根据原子和抽象编程可以提高代码的可复用性。\n\n3. 可以提高软件的==可维护性==\n\n   遵守开闭原则的软件，其稳定性高和延续性强，从而易于扩展和维护。\n\n###### 里氏替换原则\n\n里氏替换原则（Liskov Substitution Principle，LSP）由麻省理工学院计算机科学实验室的里斯科夫（Liskov）女士在 1987 年的“面向对象技术的高峰会议”（OOPSLA）上发表的一篇文章《数据抽象和层次》（Data Abstraction and Hierarchy）里提出来的，她提出：==继承必须确保超类所拥有的性质在子类中仍然成立==（Inheritance should ensure that any property proved about supertype objects also holds for subtype objects）。\n\n里氏替换原是==继承==复用的基础，它反映了基类与子类之间的关系，是对开闭原则的补充，是对实现抽象化的具体步骤的规范。\n\n> 子类可以扩展父类的功能，但不能改变父类原有的功能。\n\n具体体现在以下几点：\n\n- 子类可以实现父类的抽象方法，但不能覆盖父类的非抽象方法\n- 子类中可以增加自己特有的方法\n- 当子类的方法重载父类的方法时，方法的前置条件（即方法的输入参数）要比父类的方法更宽松\n- 当子类的方法实现父类的方法时（重写/重载或实现抽象方法），方法的后置条件（即方法的的输出/返回值）要比父类的方法更严格或相等\n\n里氏替换原则的主要作用如下：\n\n1. 里氏替换原则是实现开闭原则的重要方式之一。\n2. 它克服了继承中重写父类造成的可复用性变差的缺点。\n3. 它是动作正确性的保证。即类的扩展不会给已有的系统引入新的错误，降低了代码出错的可能性。\n4. 加强程序的健壮性，同时变更时可以做到非常好的兼容性，提高程序的维护性、可扩展性，降低需求变更时引入的风险。\n\n###### 依赖倒置原则\n\n依赖倒置原则（Dependence Inversion Principle，DIP）是 Object Mentor 公司总裁罗伯特·马丁（Robert C.Martin）于 1996 年在 [C++](http://c.biancheng.net/cplus/) Report 上发表的文章。依赖倒置原则的原始定义为：高层模块不应该依赖低层模块，两者都应该依赖其抽象；抽象不应该依赖细节，细节应该依赖抽象（High level modules shouldnot depend upon low level modules.Both should depend upon abstractions.Abstractions should not depend upon details. Details should depend upon abstractions）。其核心思想是：要==面向接口编程==，不要面向实现编程。\n\n依赖倒置原则是实现开闭原则的重要途径之一，它降低了客户与实现模块之间的耦合。\n\n开发中需要遵循以下几点：\n\n1. 每个类尽量提供接口或抽象类，或者两者都具备。\n2. 变量的声明类型尽量是接口或者是抽象类。\n3. 任何类都不应该从具体类派生。\n4. 使用继承时尽量遵循里氏替换原则。\n\n依赖倒置原则的主要作用如下：\n\n+ 依赖倒置原则可以降低类间的耦合性。\n\n- 依赖倒置原则可以提高系统的稳定性。\n- 依赖倒置原则可以减少并行开发引起的风险。\n- 依赖倒置原则可以提高代码的可读性和可维护性。\n\n###### 单一指责原则\n\n单一职责原则（Single Responsibility Principle，SRP）又称单一功能原则，由罗伯特·C.马丁（Robert C. Martin）于《敏捷软件开发：原则、模式和实践》一书中提出的。这里的职责是指类变化的原因，单一职责原则==规定一个类应该有且仅有一个引起它变化的原因==，否则类应该被拆分（There should never be more than one reason for a class to change）。\n\n单一职责原则将有以下优点。\n\n- 降低类的复杂度。一个类只负责一项职责，其逻辑肯定要比负责多项职责简单得多。\n- 提高类的可读性。复杂性降低，自然其可读性会提高。\n- 提高系统的可维护性。可读性提高，那自然更容易维护了。\n- 变更引起的风险降低。变更是必然的，如果单一职责原则遵守得好，当修改一个功能时，可以显著降低对其他功能的影响。\n\n###### 接口隔离原则\n\n接口隔离原则（Interface Segregation Principle，ISP）要求程序员尽量将臃肿庞大的接口拆分成更小的和更具体的接口，让接口中只包含客户感兴趣的方法。\n\n接口隔离原则应做到以下几点：\n\n- 接口尽量小，但是要有限度。一个接口只服务于一个子模块或业务逻辑。\n- 为依赖接口的类定制服务。只提供调用者需要的方法，屏蔽不需要的方法。\n- 了解环境，拒绝盲从。每个项目或产品都有选定的环境因素，环境不同，接口拆分的标准就不同深入了解业务逻辑。\n- 提高内聚，减少对外交互。使接口用最少的方法去完成最多的事情。\n\n遵循接口隔离原则有以下 5 个优点。\n\n1. 将臃肿庞大的接口分解为多个粒度小的接口，可以预防外来变更的扩散，提高系统的灵活性和可维护性。\n2. 接口隔离提高了系统的内聚性，减少了对外交互，降低了系统的耦合性。\n3. 如果接口的粒度大小定义合理，能够保证系统的稳定性；但是，如果定义过小，则会造成接口数量过多，使设计复杂化；如果定义太大，灵活性降低，无法提供定制服务，给整体项目带来无法预料的风险。\n4. 使用多个专门的接口还能够体现对象的层次，因为可以通过接口的继承，实现对总接口的定义。\n5. 能减少项目工程中的代码冗余。过大的大接口里面通常放置许多不用的方法，当实现这个接口的时候，被迫设计冗余的代码。\n\n###### 迪米特法则\n\n迪米特法则（Law of Demeter，LoD）又叫作最少知识原则（Least Knowledge Principle，LKP)，产生于 1987 年美国东北大学（Northeastern University）的一个名为迪米特（Demeter）的研究项目，由伊恩·荷兰（Ian Holland）提出，被 UML 创始者之一的布奇（Booch）普及，后来又因为在经典著作《程序员修炼之道》（The Pragmatic Programmer）提及而广为人知。迪米特法则的定义是：只与你的直接朋友交谈，不跟“陌生人”说话（Talk only to your immediate friends and not to strangers）。其含义是：如果两个软件实体无须直接通信，那么就不应当发生直接的相互调用，可以通过第三方转发该调用。其目的是==降低类之间的耦合度，提高模块的相对独立性==。\n\n迪米特法则时要注意以下 6 点。\n\n1. 在类的划分上，应该创建弱耦合的类。类与类之间的耦合越弱，就越有利于实现可复用的目标。\n2. 在类的结构设计上，尽量降低类成员的访问权限。\n3. 在类的设计上，优先考虑将一个类设置成不变类。\n4. 在对其他类的引用上，将引用其他对象的次数降到最低。\n5. 不暴露类的属性成员，而应该提供相应的访问器（set 和 get 方法）。\n6. 谨慎使用序列化（Serializable）功能。\n\n迪米特法则有以下两个优点。\n\n1. 降低了类之间的耦合度，提高了模块的相对独立性。\n2. 由于亲合度降低，从而提高了类的可复用率和系统的扩展性。\n\n###### 合成复用原则\n\n合成复用原则（Composite Reuse Principle，CRP）又叫组合/聚合复用原则（Composition/Aggregate Reuse Principle，CARP）。它要求在软件复用时，要尽量先使用==组合==或者==聚合==等关联关系来实现，其次才考虑使用==继承==关系来实现。如果要使用继承关系，则必须严格遵循里氏替换原则。\n\n通常类的复用分为继承复用和合成复用两种，继承复用虽然有简单和易实现的优点，但它也存在以下缺点。\n\n1. 继承复用破坏了类的封装性。因为继承会将父类的实现细节暴露给子类，父类对子类是透明的，所以这种复用又称为“白箱”复用。\n2. 子类与父类的耦合度高。父类的实现的任何改变都会导致子类的实现发生变化，这不利于类的扩展与维护。\n3. 它限制了复用的灵活性。从父类继承而来的实现是静态的，在编译时已经定义，所以在运行时不可能发生变化。\n\n采用组合或聚合复用时，可以将已有对象纳入新对象中，使之成为新对象的一部分，新对象可以调用已有对象的功能，它有以下优点。\n\n1. 它维持了类的封装性。因为成分对象的内部细节是新对象看不见的，所以这种复用又称为“黑箱”复用。\n2. 新旧类之间的耦合度低。这种复用所需的依赖较少，新对象存取成分对象的唯一方法是通过成分对象的接口。\n3. 复用的灵活性高。这种复用可以在运行时动态进行，新对象可以动态地引用与成分对象类型相同的对象。\n\n###### 七大原则总结\n\n| 设计原则     | 一句话归纳                                                   | 目的                                       |\n| ------------ | ------------------------------------------------------------ | ------------------------------------------ |\n| 开闭原则     | 对扩展开放，对修改关闭                                       | 降低维护带来的新风险                       |\n| 依赖倒置原则 | 高层不应该依赖低层，要面向接口编程                           | 更利于代码结构的升级扩展                   |\n| 单一职责原则 | 一个类只干一件事，实现类要单一                               | 便于理解，提高代码的可读性                 |\n| 接口隔离原则 | 一个接口只干一件事，接口要精简单一                           | 功能解耦，高聚合、低耦合                   |\n| 迪米特法则   | 不该知道的不要知道，一个类应该保持对其它对象最少的了解，降低耦合度 | 只和朋友交流，不和陌生人说话，减少代码臃肿 |\n| 里氏替换原则 | 不要破坏继承体系，子类重写方法功能发生改变，不应该影响父类方法的含义 | 防止继承泛滥                               |\n| 合成复用原则 | 尽量使用组合或者聚合关系实现代码复用，少使用继承             | 降低代码耦合                               |\n\n## 设计模式分类概述\n\n设计模式的本质是面向对象设计原则的实际运用，是对类的封装性、继承性和多态性以及类的关联关系和组合关系的充分理解。\n\n- 可以提高程序员的思维能力、编程能力和设计能力。\n- 使程序设计更加标准化、代码编制更加工程化，使软件开发效率大大提高，从而缩短软件的开发周期。\n- 使设计的代码可重用性高、可读性强、可靠性高、灵活性好、可维护性强。\n\n\n\n**创建型模式**：用于描述“怎样创建对象”，它的主要特点是“将对象的创建与使用分离”。\n\n**结构型模式**：用于描述如何将类或对象按某种布局组成更大的结构。\n\n**行为型模式**：用于描述类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，以及怎样分配职责。\n\n###### 设计模式总结\n\n| 模式分类 | 模式名称 | 一句话总结 |\n| -------- | -------- | -------- |\n|   创建型模式   |    工厂方法      | 简单工厂（静态工厂）模式有==一个==具体的工厂类，可以生成==多个==不同的产品。工厂方法模式有==多个==工厂==实现类==，每个实现类生成具体的产品。 |\n| : |   抽象工厂      |为访问类提供一个创建==一组相==关或相互依赖对象的接口，且访问类无须指定所要产品的具体类就能得到同族的不同等级的产品的模式结构。|\n| : |     建造者     | 将一个复杂对象的==构造==与它的==表示==分离，使同样的构建过程可以创建不同的表示。 |\n| : | 原型 | 用一个已经创建的实例作为原型，通过复制该原型对象来创建一个和原型相同或相似的新对象。（==clone==） |\n| : | 单例 | 指一个类==只有一个实例==，且该类能自行创建这个实例的一种模式。 |\n| 结构型模式 | 适配器 | 将一个类的接口==转换==成客户希望的另外一个接口，使得原本由于接口==不兼容==而不能一起工作的那些类能一起工作。 |\n| : | 桥接 | 将==抽象==与==实现==分离，使它们可以==独立变化==。它是用==组合关系==代替继承关系来实现的，从而降低了抽象和实现这两个可变维度的耦合度。 |\n| : | 组合 | 将对象组合成==树状==层次结构，使用户对单个对象和组合对象具有==一致的访问性==。 |\n| : | 装饰 | ==动态==地给对象增加一些==职责==，即增加其额外的功能。 |\n| : | 外观 | 为==多个==复杂的子系统提供一个==一致的接口==，使这些子系统更加容易被访问。 |\n| : | 享元 | 运用==共享技术==来有效地支持大量==细粒度==对象的复用。 |\n| : | 代理 | 为某对象提供一种==代理==以控制对该对象的访问。即客户端通过代理==间接==地访问该对象，从而限制、增强或修改该对象的一些特性。 |\n| 行为模式 | 责任链 | 把请求从链中的一个对象传到下一个对象，直到请求被响应为止。通过这种方式去除对象之间的耦合。 |\n| : | 命令 | 将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。 |\n| : | 迭代器 | 提供一种方法来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 |\n| : | 中介者 | 定义一个中介对象来简化原有对象之间的交互关系，降低系统中对象间的耦合度，使原有对象之间不必相互了解。 |\n| : | 备忘录 | 在不破坏封装性的前提下，获取并保存一个对象的内部状态，以便以后恢复它。 |\n| : | 观察者 | 多个对象间存在一对多关系，当一个对象发生改变时，把这种改变通知给其他多个对象，从而影响其他对象的行为。 |\n| : | 状态 | 允许一个对象在其内部状态发生改变时改变其行为能力。 |\n| : | 策略 | 定义了==一系列==算法，并将每个算法封装起来，使它们可以相互替换，且算法的改变不会影响使用算法的客户。 |\n| : | 模版方法 | 定义一个操作中的算法==骨架==，将算法的一些步骤==延迟到子类==中，使得子类在可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 |\n| : | 访问者 | 在不改变集合元素的前提下，为一个集合中的每个元素提供多种访问方式，即每个元素有多个访问者对象访问。 |\n| : | 解释器 | 提供如何定义语言的文法，以及对语言句子的解释方法，即解释器。 |",
      "data": {
        "title": "设计模式和开发原则",
        "date": "2024-12-08 11:05:28",
        "tags": [
          "设计模式",
          "note"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "## 软件开发的7条原则\n\n###### 开闭原则\n\n开闭原则（Open Closed Principle，OCP）由勃兰特·梅耶（Bertrand Meyer）提出，他在 1988 年的著作《面向对象软件构造》（Object Oriented Software Construction）中提出：软件实体应当==对扩展开放，对修改关闭==（Software entities should be open for extension，but closed for modification），这就是开闭原则的经典定义。",
      "fileName": "she-ji-mo-shi-he-kai-fa-yuan-ze"
    },
    {
      "content": "## 术语\n\n发布订阅的对象是==主题（Topic）==，向主题发布消息的客户端应用程序称为==生产者（Producer）==，订阅主题消息的客户端应用程序称为==消费者（Consumer）==，生产者和消费者统称为==客户端（Clients）==。\n\nKafka 的服务器端由被称为 ==Broker== 的==服务进程==构成，即一个 Kafka 集群由多个 Broker 组成，Broker 负责接收和处理客户端发送过来的请求，以及对消息进行持久化。虽然多个Broker可以同时运行在一台机器上，但是通常将不同的Broker分散在不同机器上，以保证高可用。\n\n实现高可用的另一个手段就是==备份机制（Replication）==。备份的思想很简单，就是把相同的数据拷贝到多台机器上，而这些相同的数据拷贝在 Kafka 中被称为==副本（Replica）==。副本的数量是可以配置的，这些副本保存着相同的数据，但却有不同的角色和作用。Kafka 定义了两类副本：==领导者副本（Leader Replica）==和==追随者副本（Follower Replica）==。前者对外提供服务，这里的对外指的是与客户端程序进行交互；而后者只是被动地追随领导者副本而已，不能与外界进行交互。\n\n<!-- more -->\n\n\n副本机制可以保证数据的持久化或消息不丢失，但没有解决伸缩性的问题（Scalability），如果领导者副本积累了太多的数据以至于单台 Broker 机器都无法容纳了，此时就需要把数据分割成多份保存在不同的 Broker 上。这种机制就是所谓的==分区（Partitioning）==。\n\nKafka 中的分区机制指的是将每个主题划分成多个==分区（Partition）==，每个分区是一组==有序==的消息日志，生产者生产的每条消息只会被发送到一个分区中。\n\n**副本是在分区这个层级定义的。**每个分区下可以配置若干个副本，其中只能有 1 个领导者副本和 N-1 个追随者副本。生产者向分区写入消息，每条消息在分区中的位置信息由一个叫==位移（Offset）==的数据来表征。分区位移总是从 0 开始，单调递增且不会改变。\n\nKafka 使用==消息日志（Log）==来保存数据，一个日志就是磁盘上一个==**只能追加写**（Append-only）==消息的物理文件。因为只能追加写入，故避免了缓慢的随机 I/O 操作，改为性能较好的顺序I/O 写操作，这也是实现 Kafka 高吞吐量特性的一个重要手段。不过如果你不停地向一个日志写入消息，最终也会耗尽所有的磁盘空间，因此 Kafka 必然要定期地删除消息以回收磁盘。怎么删除呢？简单来说就是通过==日志段（Log Segment）==机制。在 Kafka 底层，一个日志又近一步细分成多个日志段，消息被追加写到当前最新的日志段中，当写满了一个日志段后，Kafka 会自动切分出一个新的日志段，并将老的日志段封存起来。Kafka 在后台还有定时任务会定期地检查老的日志段是否能够被删除，从而实现回收磁盘空间的目的。\n\n==消费者组（Consumer Group）==指的是多个消费者实例共同组成一个组来消费一个主题，主题中的每个分区只会被组内的一个消费者实例消费。这样可以有效提升消费端的吞吐量。此外，Kafka如果检测到消费者组内某个实例挂了，会把这个实例之前负责的分区转移给其他消费者，这个过程就是==重平衡（Rebalance）==。\n\n![](https://xuhang.github.io/post-images/1733626924845.png)\n\n当数据在网络和磁盘进行传输时，避免昂贵的内核态数据拷贝，Kafka使用了Linux平台实现的==零拷贝（Zero Copy）==技术。\n\nKafka版本演进\n\n+ 0.8 引入副本机制，至此成为一个真正意义上完备的分布式高可靠消息队列解决方案，此版本API需要指定ZooKeeper地址；\n+ 0.8.2.0 社区版引入新版本Producer API，此版本需要指定Broker地址；\n+ 0.9.0.0 增加基础的安全认证/权限，使用Java重写消费者API，引入Kafka Connect组件，此版本Producer API比较稳定；\n+ 0.10.0.0 里程碑，引入Kafka Stream；\n+ 0.10.2.2 新版本Consumer API，修复了可能导致Producer API性能降低的bug；\n+ 0.11.0.0 提供幂等性Producer API和事务 API，对消息格式做了重构。\n\n## 集群参数配置\n\n**Broker端参数**\n\n```properties\nlog.dirs: Broker需要使用的文件目录路径，逗号分割，最好是不同物理磁盘上的目录，能提高吞吐量\nlog.dir: Broker需要使用的单个文件路径\nzookeeper.connect: zk地址\n# chroot zk别名，加在zookeeper.connect参数最后，不用每个zk地址都加\n# zookeeper.connect=zk1:2181,zk2:2181,zk3:2181/kafka1\nlisteners: 协议名称://主机:端口, 协议名称://主机:端口... 告诉外部连接着通过什么协议访问主机名和端口开放的kafka\nadvertised.listeners: 这组监听器是Broker用于对外发布的（通常是双网卡的外网IP）\n# 协议可以自定义，如果自定义协议名称，必须用listener.security.protocol.map参数指定具体底层安全协议\nlistener.security.protocol.map=CONTROLLER:PLAINTEXT :表示CONTROLLER自定义协议底层使用明文不加密传输\nauto.create.topics.enable: 是否允许自动创建Topic\nunclean.leader.election.enable: 是否允许Unclean Leader（落后很多的分区副本）选举\nauto.leader.rebalance.enable: 是否允许定期进行Leader选举\nlog.retention.{hour|minutes|ms}: 控制一条消息数据被保存的时间\nlog.retention.bytes: Broker为消息保存的总磁盘容量大小\nmessage.max.bytes: 控制Broker能够接收的最大消息大小\n```\n\n**Topic级别参数**\n\n> Topic 级别参数会覆盖全局Broker 参数的值\n\n```properties\nretention.ms: 该Topic消息被保存的时长，默认7days\nretention.bytes: 为该Topic预留的磁盘空间，默认-1，无限制\nmax.message.bytes: Broker能够正常接收该Topic的最大消息大小\n```\n\n**JVM参数**\n\n```properties\nKAFKA_HEAP_OPTS: 堆大小\nKAFKA_JVM_PERFORMANCE_OPTS: GC参数\n```\n\n**操作系统参数**\n\n```shell\nulimit -n 1000000: Linux操作系统能打开的最大文件描述符数量，尽可能大\n```\n\n操作系统其他参数包括swap调优和Flush落盘时间，swap如果设置为0，当物理内存耗尽，操作系统会随机选择一个进程 kill 掉，无法观测到 Broker 性能的下降和预警。Kafka只要将数据写入到操作系统的页缓存就可以任务消息写入成功，之后由操作系统定期将脏数据落盘到磁盘上。鉴于Kafka提供的多副本冗余机制，可以稍微将定期刷盘的间隔增大。\n\n## 分区机制\n\n**分区策略**（由生产者决定消息发送到主题的哪个分区）\n\n+ 自定义分区策略：通过生产者端`partitioner.class`参数指定分区策略，需要指定一个实现`org.apache.kafka.clients.producer.Partitioner`接口的类并实现其中的`partition()`方法。\n+ 轮询策略（Round-robin）：顺序分配，默认分区策略；\n+ 随机策略（Randomness）：随机地将消息放到某个分区中；\n+ 按消息键保序策略（Key-ordering）：同一个key的所有消息进入相同的分区\n\n## 压缩\n\n压缩可能发生在两个地方：生产者和Broker端\n\n生产者程序中配置`compression.type`启用指定的压缩算法类型，比如\n\n```java\nProperties props = new Properties();\nprops.put(\"bootstrap.servers\", \"localhost:9092\");\nprops.put(\"acks\", \"all\");\nprops.put(\"key.serializer\", \"org.apache.kafka.common.seriailization.StringSerializer\");\nprops.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\nprosp.put(\"comression.type\", \"gzip\"); // 启用GZIP压缩\nProducer<String, String> producer = new KafkaProducer<>(props);\n```\n\n大部分情况下，Broker端接收到Producer的消息后原封不动地保存，但是有两种例外情况可能让Broker重新压缩消息：\n\n1. Broker端指定了和Producer端不同的压缩算法。Broker端接收到消息后先用Producer端的压缩算法解压，然后用Broker端的压缩算法重新压缩。\n2. Broker端发生了消息格式转换。主要是为了兼容老版本的消费者程序。\n\nKafka会将消息使用的压缩算法封装进消息集合中，当Consumer读取消息集合时，就知道使用哪种压缩算法解压了。Broker端也会进行解压缩，目的时为了对消息执行验证，但是会对Broker端性能造成一定影响。\n\nFacebook Zstandard提供的各压缩算法benckmark结果\n\n| 算法            | 压缩比 | 压缩吞吐量 MB/s | 解压吞吐量 MB/s |\n| --------------- | ------ | --------------- | --------------- |\n| zstd 1.3.4-1    | 2.877  | 470             | 1380            |\n| zlib 1.2.11-1   | 2.743  | 110             | 400             |\n| brotli 1.0.2-0  | 2.701  | 410             | 430             |\n| quicklz 1.5.0-1 | 2.238  | 550             | 710             |\n| lzo1x2.09-1     | 2.108  | 650             | 830             |\n| lz4 1.8.1       | 2.101  | 750             | 3700            |\n| snappy 1.1.4    | 2.091  | 530             | 1800            |\n| lzf 3.6-1       | 2.077  | 400             | 860             |\n\n## 无消息丢失配置\n\n当Kafka的若干个（可以选择一个或者所有）Broker成功接收到一条消息并成功写入到日志文件后，会告诉生产者这条消息已成功提交。对于“已提交”的消息，Kafka会保证消息有限度的持久化。\n\n如果生产者使用的是`producer.send(msg)`发送消息，这个API是异步的，调用后立即返回，无法知道消息是否发送成功。应该使用`producer.send(msg, callback)`，这个API能准确的告诉客户端消息是否提交成功。\n\n`acks=all`表示Producer对“已提交”消息的定义，all表明需要所有副本Broker都接收到消息才算“已提交”。\n\n`retries=3`表示Producer的自动重试消息发送次数。\n\n## 拦截器\n\n生产者拦截器，在消息发送前和消息成功提交后插入逻辑；消费者拦截器，在消息消费前和提交位移（offset）后插入逻辑。\n\n自定义生产者拦截器需要实现`org.apache.kafka.clients.producer.ProducerInterceptor`接口，核心方法：\n\n1. `onSend`：在消息发送之前被调用\n2. `onAcknowledgement`：在消息成功提交或发送失败后被调用，`onAcknowledgement`的调用要早于生产者发送消息的回调callback的调用\n\n自定义消费者拦截器需要实现`org.apache.kafka.clients.consumer.ConsumerInterceptor`接口，核心方法：\n\n1. `onConsume`：在消息被Consumer处理之前调用\n2. `onCommit`：Consumer在提交offset之后调用\n\n## 连接\n\n在==**创建**== KafkaProducer 实例时，生产者应用会在后台创建并启动一个名为 Sender 的线程，该 Sender 线程开始运行时首先会创建与 Broker 的连接。也就是说，在调用send方法前，Producer就已经连接到Broker了，连接的是`bootstrap.servers`指定的Broker地址。实际使用中，并不需要把所有Broker地址都配置在`bootstrap.servers`参数中，因为为 Producer 一旦连接到集群中的任一台Broker，会向某一台Broker发送METADATA请求，就能拿到整个集群的 Broker 信息。\n\n当 Producer 更新了集群的元数据信息之后，如果发现与某些 Broker 当前没有连接，那么它就会创建一个 TCP 连接。同样地，当要发送消息时，Producer 发现尚不存在与目标 Broker 的连接，也会创建一个。\n\nProducer端参数`connections.max.idle.ms`配置了Kafka关闭TCP连接的空闲时间，默认9分钟。如果设置为 `-1`则表示不关闭空闲的TCP连接。\n\n和生产者不同的是，消费者端构建`KafkaConsumer`实例时是不会创建任何TCP连接的，而是在调用`KafkaConsumer.poll`方法时创建。再具体一点，poll方法内部有3个创建TCP连接的时机：\n\n1. 发起`FindCoordinator`请求时。消费者向集群中负载最小（待发送请求最少）的Broker发送请求，查询Coordinator对于的Broker，这一步会创建Socket连接。这个TCP之后会被关闭。\n2. 完成上一步的`FindCoordinator`请求后，连接正真的Coordinator，此时会创建Socket连接。\n3. 消费数据时。消费者会为每个要消费的分区创建与该分区Leader副本所在的Broker连接的TCP。\n\n消费者关闭Socket也分为主动关闭和Kafka自动关闭。主动关闭是调用API关闭，如`KafkaCosnumer.close()`，自动关闭由消费者端参数`connection.max.idle.ms`控制，默认9分钟。\n\n## 消息可靠性保障\n\nKafka消息交付可靠性保障：\n\n1. ==最多一次==：消息可能会丢失，但不会重复发送（禁止Producer重试）\n2. ==至少一次==：消息不会丢失，但可能重复发送（Kafka默认）\n3. ==精确一次==：消息不会丢失，也不会重复发送（幂等和事务）\n\n**幂等性（Idempotence）**\n\n指的是某些操作或函数能够被执行多次，但每次得到的结果都是不变的。\n\n0.11.0.0之后引入了幂等性Producer，开启方法：\n\n```java\nprops.put(\"enable.idempotence\", true);\n// or\nprops.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n```\n\n为实现幂等性，Kafka底层设计架构中引入了ProducerID和SequenceNumber\n\n- ProducerID：在每个新的Producer初始化时，会被分配一个唯一的ProducerID，这个ProducerID对客户端使用者是不可见的。\n- SequenceNumber：对于每个ProducerID，Producer发送数据的每个Topic和Partition都对应一个从0开始单调递增的SequenceNumber值。\n\nProducer在每条消息中附带了PID（ProducerID）和SequenceNumber。Broker 为每个 Topic 的每个 Partition 都维护了一个当前写成功的消息的最大 PID-SequenceNumber 元组，当 Broker 收到一个比当前最大 PID-Sequence Number 元组小（或等于）的 SequenceNumber 消息时，就会丢弃该消息，以避免造成数据重复存储。\n\n**幂等性Producer** \n\n能够保证某个主题的**一个分区上**不出现重复消息，它无法实现多个分区的幂等性。或者当Producer进程重启后，这种幂等性也会丢失。\n\n要实现多分区及多会话上的消息不重复，需要事务（Transaction）或依赖事务型Producer。\n\n**事务（Transaction）**\n\n主要是在 read committed 隔离级别上做事情。它能保证多条消息原子性地写入到目标分区，同时也能保证 Consumer 只能看到事务成功提交的消息。\n\n**事务型 Producer**\n\n事务型 Producer 能够保证将消息==原子性==地写入到多个分区中。这批消息要么全部写入成功，要么全部失败。另外，事务型 Producer 也不惧进程的重启。Producer 重启回来后，Kafka 依然保证它们发送消息的精确一次处理。\n\n```properties\n// 开启事务型Producer\nenable.idempotence=true\n```\n\n在Consumer端读取事务型Producer发送的消息，设置 `isolation.level`参数：\n\n1. `read_uncommitted`：默认值，Consumer能读取Kafka写入的任何消息\n2. `read_committed`：Consumer只会读取事务型Producer成功提交的事务写入的消息，也能看到非事务型Producer写入的所有消息。\n\n## 消费者组\n\nConsumer Group 是 Kafka 提供的可扩展且具有容错性的消费者机制，组内的多个消费者或消费者实例共享一个公共的ID（Group ID），组内所有消费者协调在一起消费订阅主题的所有分区。\n\n如果所有实例都属于同一个 Group，那么它实现的就是消息队列模型；如果所有实例分别属于不同的 Group，那么它实现的就是发布 / 订阅模型。·\n\n理想情况下，Consumer 实例的数量应该等于该 Group 订阅主题的分区总数。\n\n在新版本的 Consumer Group 中，Kafka 社区重新设计了 Consumer Group 的位移管理方式，采用了将位移保存在 Kafka 内部主题（ __consumer_offsets）的方法。\n\nRebalance 本质上是一种协议，规定了一个 Consumer Group 下的所有 Consumer 如何达成一致，来分配订阅Topic 的每个分区。\n\n触发Rebalance的3个条件：\n\n1. 组成员数发生变更。有新的Consumer加入或有Consumer崩溃下线\n2. 订阅主题数发生变更。Consumer Group 可以使用正则表达式的方式订阅主题，比如`consumer.subscribe(Pattern.compile(“t.*c”)) `，当有新的主题匹配正则表达式则会触发\n3. 订阅主题的分区数发生变更。\n\n每个Consumer会定期地向Coordinator发送心跳，如果Consumer不能及时地发送心跳请求，Coordinator就会将其从Group中移除，开启新一轮的Rebalance。这个时间由参数`session.timeout.ms`控制，默认10s。参数`heartbeat.interval.ms`控制心跳请求的频率，Coordinator通过心跳请求的响应通知Consumer开启Rebalance。参数`max.poll.interval.ms`参数限定Consumer端两次poll调用的最大间隔，默认5min，如果超过这个时间，表明Consumer无法顺利地消费完poll的消息，Consumer会主动离开Group，Coordinator会开启新一轮的Rebalance。\n\n但是，Rebalance发生时，所有Consumer实例都会停止消费，直到Rebalance完成。Rebalance过程中，由协调者Coordinator组件帮助完成订阅主题分区的分配，Coordinator专门为Consumer Group 服务，负责为Group执行Rebalance以及提供位移管理和组成员管理等。所有Broker在启动时，都会创建和开启相应的Coordinator组件。\n\n**协调者Coordinator**\n\n协调者专门为Consumer Group 服务，负责为Group执行Rebalance以及提供位移管理和组成员管理。Consumer在提交位移时，其实是向Coordinator所在的Broker提交位移；当Consumer启动时，也是向Coordinator所在的Broker发送各种请求，然后由Coordinator执行消费者组的注册、成员管理等元数据管理。\n\n所有的Broker都有各自的Coordinator组件。\n\n1. 通过计算`partitionId = Math.abs(groupId.hashCode() % offsetTopicPartitionCount)`确定由位移主题的哪个分区来保存Group的数据。\n2. 找到该分区的Leader副本，该Broker就是对于的Coordinator。\n\n**位移主题 Offset Topic**\n\n`__consumer_offsets`使用Kafka自定义的消息格式，主题中的key包含3部分内容：GroupID，主题名，分区号。\n\n主题消息体的格式：\n\n1. 包含位移数据、时间戳和自定义的数据\n2. 用于保存Consumer Group信息的消息\n3. 用于删除Group过期位移甚至是删除Group的消息（消息体为null，当某个Consumer Group下所有Consumer都停止且位移数据都已被删除，Kafka会向位移主题的对应分区写入）\n\n当Kafka集群中第一个Consumer启动时，会自动创建位移主题。`__consumer_offsets`主题的分区数通过Broker端参数`offset.topic.num.partitions`设置，默认50，副本数默认3。\n\n如果Consumer采用的是自动提交位移的方式，就会无限期地向`__consumer_offsets`中写入主题消息。此时需要Kafka定期删除过期消息。Kafka使用的是`Compact`策略，即对于主题中同一个Key的多条消息，只保留最新的消息。Kafka提供了专门的后台线程（`Log Cleaner`）定期巡检待Compact的主题，看看是否存在满足条件的可删除数据。\n\n## 多线程消费者实例\n\n`KafkaConsumer`是单线程设计的（心跳线程是单独的线程），能简化Consumer端的设计，客户端可以在Consumer获取到消息后，多线程处理消息。\n\n`KafkaConsumer`类不是线程安全的，如果在多线程中共享一个`KafkaConsumer`实例，程序会抛出异常。可以多线程中每个线程维护一个专属的`KafkaConsumer`实例，负责完整的消息获取、处理流程。\n\n## 消费进度\n\nKafka监控的是分区的Lag，如果要计算主题的Lag，需要手动汇总素有分区的Lag。如果消费者速度赶不上生产者的速度，会导致数据不在操作系统的页缓存中，而是磁盘中，这样这些数据就不能通过零拷贝传输，进一步拉大消费者和生产者的差距，Lag越来越大。\n\n有3种方法监控消费进度：\n\n1. Kafka自带的命令行工具`kafka-consumer-groups`脚本\n2. Kafka Java Consumer API编程\n3. Kafka自带的JMX监控指标\n\n## 副本\n\n副本机制，也称备份机制，通常指分布式系统在多台机器上有相同的数据拷贝。\n\n1. 提供数据冗余。即使部分组件失效，系统依然能继续运转，提高整体可用性和数据持久性。\n2. 提高高伸缩性。支持横向扩展，能通过增加机器的方式提高读写性能，进而提高吞吐量。\n3. 改善数据局部性。运行数据放入与用户地理位置相近的地方。\n\nKafka的副本机制与其他分布式系统不同，Kafka的追随者副本不对外提高服务，所有的请求必须由Leader副本处理，追随者副本的唯一任务就是从Leader副本==异步拉取==消息，写入到自己的提交日志种，从而实现与Leader副本的同步。当Leader副本挂了，Kafka依托于Zookeeper提供的监控功能可以实时感知，并立即开启新一轮的Leader选举。\n\n这种机制有两个好处：\n\n1. 实现“Read-your-writes”，生产者写入消息后，消费者马上可以读取到，而不用等待异步同步完成。\n2. 实现单调读（Monotonic Reads），由于副本的同步进度不一样，避免从不同的副本上读取到不一致的数据。\n\n**ISR （In-Sync Replicas）**\n\nISR副本集合是一个动态调整的集合，集合中的副本都被认为是和Leader副本同步的，Leader副本是天然就在ISR中的（如果Leader副本挂了，集合就是空的）。Broker端参数`replica.lag.time.max.ms`含义是追随者副本能够落后Leader副本的最大时间间隔（默认10s），如果追随者副本落后超过这个时间，就会被踢出ISR集合，等重新追上了会被重新加回ISR。\n\n所有不在ISR集合中的存活副本称为==非同步副本==，如果允许选举这种副本的过程称为==Unclean 领导者选举==，Broker端参数`unclean.leader.election.enable`可以控制是否允许非同步副本参与Leader选举。如果开启，可能回造成数据丢失，反之，则能保证数据的一致性。通过这个参数，Kafka赋予你选择CP还是AP的权利。",
      "data": {
        "title": "Kafka 核心技术与实战笔记",
        "date": "2024-12-08 11:01:07",
        "tags": [
          "kafka",
          "geektime",
          "note"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "## 术语\n\n发布订阅的对象是==主题（Topic）==，向主题发布消息的客户端应用程序称为==生产者（Producer）==，订阅主题消息的客户端应用程序称为==消费者（Consumer）==，生产者和消费者统称为==客户端（Clients）==。\n\nKafka 的服务器端由被称为 ==Broker== 的==服务进程==构成，即一个 Kafka 集群由多个 Broker 组成，Broker 负责接收和处理客户端发送过来的请求，以及对消息进行持久化。虽然多个Broker可以同时运行在一台机器上，但是通常将不同的Broker分散在不同机器上，以保证高可用。\n\n实现高可用的另一个手段就是==备份机制（Replication）==。备份的思想很简单，就是把相同的数据拷贝到多台机器上，而这些相同的数据拷贝在 Kafka 中被称为==副本（Replica）==。副本的数量是可以配置的，这些副本保存着相同的数据，但却有不同的角色和作用。Kafka 定义了两类副本：==领导者副本（Leader Replica）==和==追随者副本（Follower Replica）==。前者对外提供服务，这里的对外指的是与客户端程序进行交互；而后者只是被动地追随领导者副本而已，不能与外界进行交互。",
      "fileName": "kafka-he-xin-ji-zhu-yu-shi-zhan-bi-ji"
    },
    {
      "content": "## 一、了解Electron\n\nElectron 是由 Github 开发，用 HTML，CSS 和 JavaScript 来构建跨平台桌面应用程序的一个开源库。最初的时候是属于 Github 开源代码编辑器 Atom 的一部分，在 2014 春季这两个项目相继开源。\n\n<!-- more -->\n\n**里程碑事件**\n\n| 时间         | 里程碑                                         |\n| ------------ | ---------------------------------------------- |\n| 2013 年 4 月 | Atom Shell 项目启动 。                         |\n| 2014 年 5 月 | Atom Shell 被开源 。                           |\n| 2015 年 4 月 | Atom Shell 被重命名为 Electron 。              |\n| 2016 年 5 月 | Electron 发布了 v1.0.0 版本 。                 |\n| 2016 年 5 月 | Electron 构建的应用程序可上架 Mac App Store 。 |\n| 2016 年 8 月 | Windows Store 支持 Electron 构建的应用程序 。  |\n\n**Electron优点**：\n\n+ 跨平台，降低开发成本。\n+ 强大的 npm 生态，提升开发效率\n+ 简单易学的 JavaScript 语言，基于 Web 的桌面开发，不用再去学习 C# 或者 Swift，降低学习成本。\n+ 苹果商店和微软商店都支持提交 Electron 应用程序。\n+ 开发速度快，适合需要段时间内看到效果的应用开发\n\n**缺点**：\n\n+ 打包体积大\n+ Chromium比较吃内存\n+ 跨平台并不代表一次编写，处处运行，需要在各个系统上测试、打包\n+ 版本更新频繁，多个版本间有兼容性的问题\n\n**跨平台开发框架比较**\n| 框架                  | 支持语言             | 优缺点                                                       |\n| --------------------- | -------------------- | ------------------------------------------------------------ |\n| Qt                    | C++/Python           | 优点：流行、稳定，内置自绘引擎，保证不同系统上界面一致，PyQt可以使用Python开发<br/>缺点：高分屏缩放显示问题，商业需授权，免费版本版权问题 |\n| GTK                   | C/C++/JS/Rust等      | 优点：自绘引擎，提供大量系统相关API，商业授权友好<br/>缺点：只在Linux环境下流行，其他环境下开发困难 |\n| wxWidgets             | C++                  | 优点：稳定、成熟<br/>缺点：没有自绘引擎，界面风格不统一      |\n| FLTK                  | C++                  | 优点：轻量，OpenGL引擎，有Rust绑定fltk-rs<br/>缺点：绘图API少 |\n| Duilib                | C++                  | 优点：开源、有不少大厂使用<br/>缺点：只支持Windows、不支持高分屏、几乎没有系统级API，更新不及时，开发环境搭建麻烦 |\n| Sciter                | C++/Rust/Go/Python等 | 优点：精简，个人使用免费，内部有一个浏览器核心，自研脚本类似JS<br/>缺点：闭源，商业收费，不支持Flex布局，非官方绑定质量差，社区不成熟 |\n| CEF                   | C/C++/Go/Python/Java | 优点：基于Chromium，装机量大，支持HTML/CSS/JS特性<br/>缺点：体积大、几乎不提供系统及API，版本发布频繁 |\n| MAUI                  | C#                   | 优点：微软开发，有自绘引擎，支持桌面端和移动端<br/>缺点：暂无稳定版，界面有XAML描述 |\n| Compose Multiplatform | Kotlin               | 优点：新，自绘引擎Skia（Goole）<br/>缺点：稳定欠缺，打包需要封装JRE |\n| flutter-desktop       | Dart                 | 优点：新，自绘引擎Skia，支持FFI<br/>缺点：不稳定，Dart大括号嵌套不友好，打包体积大 |\n| webview2              | C#/C++               | 优点：可以复用系统当中已存在的webview2二进制资源，打包体积小<br/>缺点：只支持Windows，闭源 |\n| TAURI                 | Rust                 | 优点：安装包小，新，开源，最近比较火<br/>缺点：新，不稳定，webview有的问题TARI都有 |\n| NW.js（node-webkit）  | JS                   | 优点：开源，开发方便<br/>缺点：封装的系统级API不如Electron，生态不完善，API不稳定 |\n| Electron              | JS                   | 优点：作者是NW.js原成员，Github，微软等大厂在用（github客户端，VSCode），生态完善，系统级API多<br/>缺点：打包体积大 |\n| ImGui                 | C++                  | 优点：对游戏开发者友好，支持多种引擎比如OpenGL，Directx，Vulkan等，打包体积小<br/>缺点：循环绘制整个界面，消耗CPU，有一些小问题 |\n\n如果是会C/C++，可选的框架就很多，可以根据需求选择最合适的框架，但是如果会一点JS，这里的Electron可能是比较合适的。首先是开源，开发语言JS相对大众，遇到问题也比较容易找到解决方案；其次是Electron的生态完善，有比较多的大厂在使用，比如微软的VS Code，Github官方的客户端GUI，Slack，Postman等，国内也有很多使用案例，比如迅雷X，utools等，更多的Electron 应用可以在 [🔗这里](https://www.electronjs.org/apps) 发现。\n\n![Electron - www.electronjs.org](https://xuhang.github.io/post-images/1733619671523.png)\n\n## 二、使用Electron开发应用\n### 问题\n在项目开发中，大部分时间是使用QQ作为沟通工具，那项目相关的需求文档、接口文档也基本上是通过QQ分享，时间久了在QQ的本地目录里就会有很多不同项目的相关文档，要找到就会很困难，而且很多文档名仅差几个字，很难区分每个文档属于哪个项目下的。\n\n### 方案1\n每次涉及新的需求，在用户目录下新建一个该项目的文件夹，每次有收到项目相关的文件就将文档存放到该路径下，再次使用时就在该目录下查找。\n\n![](https://xuhang.github.io/post-images/1733619772394.png)\n\n### 方案2\n方案1虽然能将文件分项目存储，但是每个文件的内容很难通过文件获取；每次进入项目目录需要打开资源管理器，进入用户目录，选择相关项目，步骤繁琐；……这里我们开发一个专门解决这个痛点的软件，用来管理项目相关的文件，该软件有以下功能：\n\n+ [x] 分项目管理所属文件\n+ [x] 以易读的方式显示文件的加入时间\n+ [x] 支持直接从聊天软件对话框中拖拽文件到项目中\n+ [ ] 如果其他人需要，可以将项目所有文件打包分享\n+ [x] 可以对每个文件注释，标注该文件的作用、内容等\n+ [x] 快速定位到文件和打开文件\n+ [ ] 针对每个项目内嵌一个Kanban来记录具体任务的完成情况\n+ [ ]  根据不同的命名规则自动关联到文件的最新版本\n+ [x] 加入沟通功能，可以快速与小组成员分享文件\n\n\n| Windows UI                                                   | MacOS UI                                                     |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| ![](https://xuhang.github.io/post-images/1733619845296.png)  |\n\n### 开发\n\n由于Electron版本更新较快，接口和功能可能会有较大的变动，建议参考官方文档[🔗官方文档](https://www.electronjs.org/zh/docs/latest/)（有中文）。\n\n#### **0. Electron中常用的模块**\n\n可以从命名规则中得知，如果引入的是类，用户是可以创建多个实例对象的，如果引入的是对象，则是Electron已经创建好的，且一个应用里只有一个实例。\n\n```javascript\nconst { \n    app, // 对该应用程序的引用，一个应用可以包含0-多个窗口\n    BrowserWindow, // 窗口类\n    Menu, // 菜单类\n    Tray, // 任务栏图标\n    Notification, // 通知\n    shell, // Shell对象，执行系统命令\n    ipcMain, // IpcMain对象，用来触发和监听消息事件\n    globalShortcut, // GlobalShortcut对象，用来处理全局快捷键的绑定和解绑\n    screen, // Screen对象，用来获取显示器相关数据\n    nativeImage // NativeImage对象，用来生成和获取图片\n} = require('electron');\n```\n\n#### **1. 单实例**\n\n单实例是为了保证只有一个在工作，防止误操作多次双击后启动多个实例。这里多个实例没有意义，所以限制多实例的生成，如果重复创建app对象则直接退出。\n\n```javascript\nconst gotTheLock = app.requestSingleInstanceLock()\nif (!gotTheLock) {\n    app.quit()\n} else {\n    app.on('second-instance', (event, commandLine, workingDirectory) => {\n            // 当运行第二个实例时,将会聚焦到myWindow这个窗口\n            if (win) {\n                if (win.isMinimized()) win.restore()\n                win.focus()\n            }\n        })\n        // app 启动创建窗口\n    app.on('ready', createWindow);\n}\n```\n#### **2. 自定义协议**\n\n在浏览器中常用的协议有`http`、`https`、`file`等，自定义协议可以让我们通过在浏览器访问自定义schema打开我们的应用，并执行指定操作。这里我们以软件名称`fily`作为协议名称，后面的uri路径为要访问的项目名称(目前支持英文字母和数字的名称，中文需要额外的编解码)。\n\n```http\nfily://project-name\n```\n实现方式MacOS和Windows有区别，MacOS通过`app.setAsDefaultProtocolClient`设置默认协议，Windows通过第二实例打开。\n```javascript\n// MacOS\nif (process.defaultApp) {\n    if (process.argv.length >= 2) {\n        app.setAsDefaultProtocolClient('fily', process.execPath, [path.resolve(process.argv[1])])\n    }\n} else {\n    app.setAsDefaultProtocolClient('fily')\n}\n\napp.on('open-url', (event, url) => {\n    openUrl(url);\n})\n\n//Windows\nconst gotTheLock = app.requestSingleInstanceLock()\nif (!gotTheLock) {\n    app.quit()\n} else {\n    app.on('second-instance', (event, commandLine, workingDirectory) => {\n            // 当运行第二个实例时,将会聚焦到myWindow这个窗口\n            if (win) {\n                if (win.isMinimized()) win.restore()\n                win.focus()\n                let lastCommand = commandLine[commandLine.length - 1];  // ①\n                if (lastCommand.indexOf('fily://') === 0) {             // ②\n                    openUrl(lastCommand)                                // ③\n                }\n            }\n        })\n        // app 启动创建窗口\n    app.on('ready', createWindow);\n}\n```\n\n#### **3. 进程间通信**\n\nElectron分主进程和渲染进程，主程序 (main process)，它运行在 Node.js环境里，负责控制应用的生命周期、显示原生界面、执行特殊操作并管理渲染器进程 (renderer processes)。应用中的每个页面都在一个单独的进程中运行，我们称这些进程为 渲染器(renderer) 。 渲染器也能访问前端开发常会用到的 API和工具，例如用于打包并压缩代码的webpack，还有用于构建用户界面的 React 。\n\n| 通信方向                 | 代码                                              |\n| ------------------------ | ------------------------------------------------- |\n| 主进程监听事件           | `ipcMain.on(EVENT-NAME, (event, data) => {})`     |\n| 主进程向渲染进程发送通知 | `win.webContents.send(EVENT-NAME, {})`            |\n| 渲染进程监听事件         | `ipcRenderer.on(EVENT-NAME, (event, data) => {})` |\n| 渲染进程向主进程发送通知 | `ipcRenderer.send(EVENT-NAME, {})`                |\n\n主进程和渲染进程主要负责的功能范围不同，进程间的通信通过事件监听和消息通知来交互完成。\n\n> Electron 的主进程是一个拥有着完全操作系统访问权限的 Node.js 环境。 除了 Electron 模组之外，你也可以使用 Node.js 内置模块 和所有通过 npm 安装的软件包。另一方面，出于安全原因，渲染进程默认跑在网页页面上，而并非 Node.js里。为了将 Electron 的不同类型的进程桥接在一起，我们需要使用被称为 预加载 的特殊脚本。\n\n#### **4. 创建窗口**\n\n|                                                              |              |\n| ------------------------------------------------------------ | ------------ |\n|![](https://xuhang.github.io/post-images/1733619943936.png)  | Windows   UI |\n| ![](https://xuhang.github.io/post-images/1733619968189.png) | MacOS UI     |\n\n软件打开之后需要展示给用户使用的界面，ELectron封装了不同操作系统的UI界面，所以在不同系统上使用会有不同的风格。为了统一风格，我们去掉窗口的边框，使用HTML+CSS绘制一套统一的界面。可以从上面的对比图看出，Windows和MacOS上的顶部titleBar的样式是有区别的，Windows的titleBar会显示菜单，MacOS是空白的，因为MacOS的菜单是当程序窗口是活动窗口时，在显示器左上角会显示当前程序的菜单，所以两个平台的菜单绑定是有区别的。\n\n![](https://xuhang.github.io/post-images/1733620004039.png)\n\n```javascript\n// 创建窗口要在app启动完成之后开始\napp.on('ready', createWindow);\n\nfunction createWindow() {\n    // 读取package.json文件中的属性\n    var package = require('./package.json')\n    console.log('starting ' + package.name + ' ' + package.version)\n\n    // 隐藏菜单 Menu.setApplicationMenu(null)\n    // 这里菜单仅对MacOS有效，因为Windows隐藏边框之后没有菜单栏\n    const appMenu = Menu.buildFromTemplate(appMenuTemplate);\n    Menu.setApplicationMenu(appMenu)\n\n    // 创建浏览器窗口并设置宽高\n    win = new BrowserWindow({\n        width: 1200,\n        height: 800,\n        titleBarStyle: 'customButtonsOnHover',\n        icon: __dirname + './img/fily.png',\n        maximizable: false,\n        minimizable: false,\n        fullscreenable: false,\n        frame: false, // 是否有边框\n        // resizable: true,        // 是否可调整大小\n        transparent: true, //是否透明\n        webPreferences: { // 新版Electron添加了一些安全限制，这里需要设置\n            nodeIntegration: true,\n            enableRemoteModule: true,\n            contextIsolation: false\n        }\n    })\n    remoteMain.initialize()\n    remoteMain.enable(win.webContents);\n\n    // MacOS底部的Dock栏图标\n    if (systemType === \"Darwin\") {\n        app.dock.setIcon(path.join(__dirname, './img/icon_512x512.png'));\n        // 设置Dock菜单\n        const dockMenu = Menu.buildFromTemplate(dockMenuTempalte);\n        app.dock.setMenu(dockMenu);\n    }\n\n    // 加载页面 -- 向用户展示的界面\n    win.loadFile('./page/projectRelatedFiles.html')\n\n    // Windows 任务栏图标菜单（右下角） / MacOS任务栏图片（右上角）\n    // tray = new Tray(path.join(__dirname, \"./img/fily.png\"));\n    const trayImage = nativeImage.createFromPath(path.join(__dirname, \"./img/icons.iconset/file20Template.png\"));\n    trayImage.resize({ width: 16, height: 16 })\n    trayImage.setTemplateImage(true);\n    tray = new Tray(trayImage);\n    const contextMenu = Menu.buildFromTemplate([\n        { label: '退出', click: function() { app.quit() } }\n    ])\n    tray.setToolTip('Fily - File manager for you!')\n    tray.setContextMenu(contextMenu)\n\n    // 添加window关闭触发事件\n    win.on('close', () => {\n        win = null;\n        app.quit();\n    });\n\n    win.setProgressBar(0.7);\n}\n\n```\n#### **5. 添加快捷键注册**\n\n可以通过注册全局快捷键快速执行某个操作或者在菜单项里对指定的菜单绑定快捷键。\n\n```javascript\n// 注册全局快捷键-退出应用\nglobalShortcut.register('Ctrl+W', () => {\n    win = null;\n    app.quit();\n});\n// 全局快捷键-打开开发中工具\nglobalShortcut.register('Ctrl+Shift+I', () => {\n    win.webContents.toggleDevTools()\n})\n\n// 通过构建菜单绑定快捷键执行菜单操作\nconst appMenuTemplate = [{\n    label: 'Projects',\n    submenu: [{\n        label: 'New Project',\n        accelerator: 'Cmd+N',\n        click: () => {\n            win.webContents.send('new-project-dialog', {})\n        }\n    }, {\n        label: 'Rename Project'\n    }, {\n        label: 'Delete Project'\n    }, {\n        type: 'separator'\n    }, {\n        label: 'Archive'\n    }]\n}, {\n    label: 'Files',\n    submenu: [{\n        label: 'Import File'\n    }, {\n        label: 'Import Folder'\n    }]\n}]\n```\n#### **6. 添加进度条**\n\nWindows和MacOS都可以在任务（Dock）栏的图标那里显示一个进度条，通常可以用来显示下载进度或者任务完成进度。\n\n| Windows UI                                                   | MacOS UI                                                     |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n|![](https://xuhang.github.io/post-images/1733620043980.png)  |\n\n```javascript\n// 这里的进度条在70%的进度\nwin.setProgressBar(0.7);\n```\n\n#### **7. 任务栏预览快捷操作（仅Windows）**\n\n在任务栏的预览视图那里可以设置快捷操作键，这里我们通过添加两个按钮用于访问“前一个”和“后一个”项目。\n\n![](https://xuhang.github.io/post-images/1733620060775.png)\n\n```javascript\n// 主线程\nwin.setThumbarButtons([{\n    tooltip: '上一张',\n    icon: path.join(__dirname, './resource/icos/pre.ico'),\n    click() { switchThumbImage(-1) }\n}, {\n    tooltip: '下一张',\n    icon: path.join(__dirname, './resource/icos/next.ico'),\n    click() { switchThumbImage(1) }\n}])\n\nfunction switchThumbImage(delta) {\n    if (delta > 0) {\n        win.webContents.send('show-next-project', {})\n    } else {\n        win.webContents.send('show-prev-project', {})\n    }\n}\n\n// 渲染进程\nipcRenderer.on('show-next-project', (event, message) => {\n    // document.getElementById('gallery_image').setAttribute('src', message['src']);\n    let curProj = $('.projlist ul li.active')\n    let nextProj;\n    if (!!curProj[0]) {\n        nextProj = curProj.next();\n        if (!nextProj[0]) {\n            nextProj = $('.projlist ul li').first()\n        }\n    } else {\n        nextProj = $('.projlist ul li').first();\n    }\n    nextProj.click();\n})\nipcRenderer.on('show-prev-project', (event, message) => {\n    let curProj = $('.projlist ul li.active')\n    let prevProj;\n    if (!!curProj[0]) {\n        prevProj = curProj.prev();\n        if (!prevProj[0]) {\n            prevProj = $('.projlist ul li').last()\n        }\n    } else {\n        prevProj = $('.projlist ul li').last();\n    }\n    prevProj.click();\n})\n```\n\n#### **8. 通知**\n\n这里监听右上角的关闭按钮，点击关闭之后会弹出通知，并延迟1s后退出应用。\n\n| Windows UI                                                   | MacOS UI                                                     |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n|![](https://xuhang.github.io/post-images/1733620099033.png) |\n\n```javascript\nipcMain.on('close-window', (event, data) => {\n    if (!Notification.isSupported()) {\n        console.log('当前系统不支持通知');\n    }\n    app.setAppUserModelId(\"我的Electorn示例\")\n    const notification = {\n        appId: \"com.xuhang.electron.demo\",\n        icon: path.join(__dirname, './img/icon_512x512.png'),\n        title: '正在关闭……',\n        body: 'Bye ヾ(•ω•`)o'\n    }\n    new Notification(notification).show()\n\n    setTimeout(() => {\n        console.log('fily is exiting ...')\n        app.quit();\n    }, 1000);\n});\n```\n\n#### **9. 数据存储**\n\n项目列表和项目相关的文件关联关系需要持久化才能保证数据不丢失，通常可以选择将数据保存到远程数据库，但是这样必须要有一个实时在线的服务。也可以选择数据保存在本地，这样既能保证数据实时可用，也能保证数据的安全性，一来也是因为数据没有在线访问的必要。\n\n离线存储有几种可选方案：\n\n1. 利用浏览器的localStorage存储，优点是浏览器原生支持，Electron本身就包含了Chromium，缺点是如果清空浏览器缓存数据也丢失了。\n2. 利用Electron的文件读写API，自定义存储格式和存储路径，将数据保存到本地文件，优点是没有第三方依赖，缺点是需要自己实现数据的序列化与反序列化。\n3. 利用第三方库，如lowdb将数据以JSON格式存储在本地文件，优点是提供了增删改查的API，缺点是有学习成本。\n4. 集成SQLite，优点是数据有各种工具可以读取，也可以通过网盘将数据进行同步，SQLite也比较稳定成熟，缺点是Electron对SQLite的集成比较复杂，打包后体积也会变大。\n\n综合各种方案的优劣，我们前期需要快速出一个可用的版本，所以选择了lowdb。\n\n#### **10. 引入jQuery**\n\n前端开发的发展速度很快，可能两三年技术路线就会更新迭代，jQuery虽然过时了，但是对于非前端开发来说还是相对熟悉的框架，所以为了尽可能简化开发这里还是使用jQuery。\n\nElectron默认启用了Node.js的require模块，而jQuery等新版本框架为了支持commondJS标准，当Window中存在require时，会启用模块引入的方式。这里主要针对jQuery1.11.1及以后的版本。\n\n```javascript\n<script src=\"../script/jquery.min.js\"></script>\n<script>\n    if (typeof module === 'object') {\n        window.jQuery = window.$ = module.exports;\n    }\n</script>\n```\n\n由于Electron和ES6对于模块的导入导出采用了不同的方式，所以在引用JS文件时很容出现报错。关于各种Javascript运行时环境下的模块导入导出可以参见 [🔗这篇文章](https://mp.weixin.qq.com/s?__biz=MzIzMDE5NTQyMQ==&mid=2247484053&idx=1&sn=dec6d77da1de255d1b9a92289c3a3df3)\n\n#### **11. 开发者工具**\n\n为了方便对页面进行调试，需要为每个窗口添加打开/关闭开发者工具的快捷键监听事件。在页面的js文件中为window对象添加事件监听器并在按键为`Ctrl+Shift+I`是打开/关闭开发者工具，针对当前活动窗口的快捷键才会生效 。\n\n```javascript\nwindow.addEventListener('keydown', (event) => {\n    if (event.ctrlKey && event.shiftKey && event.key === 'I') {\n        remote.getCurrentWindow().toggleDevTools();\n    }\n})\n```\n\n#### **12. 聊天功能**\n\n在界面左上角重载按钮的旁边有个聊天图标，点击会在屏幕右侧出现一个透明窗口，该窗口实现了鼠标点击穿透，但同时也保持窗口置顶，最下方的文字输入区域设置透明度0.6，但是为了能输入文字，该区域为点击不穿透。该聊天功能使用腾讯的IM云服务，每个打开该聊天界面的终端会自动申请加入一个开放群，目前仅实现了文字聊天功能。\n\n| Windows UI                                                   | MacOS UI                                                     |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| ![](https://xuhang.github.io/post-images/1733620151597.png) |\n\n\n\n\n\n## 三、打包发布\n\n开发完成后将代码打包并生成系统安装包。之前主要使用`electron-package`和`electron-builder`来打包，目前Electron官方推荐使用`electron-forge`，后者针对发布和更新会更方便一点。\n\nelectron-builder在package.json中添加如下参数：\n\n```json\n\"build\": {\n    \"productName\": \"fily\",\n    \"appId\": \"com.xuhang.electron.demo\",\n    \"copyright\": \"Copyright © 2021-2022 xu\",\n    \"directories\": {\n        \"output\": \"build\"\n    },\n    \"nsis\": {\n        \"oneClick\": false,\n        \"allowElevation\": true,\n        \"allowToChangeInstallationDirectory\": true,\n        \"installerIcon\": \"./img/icon.ico\",\n        \"uninstallerIcon\": \"./img/icon.ico\",\n        \"installerHeaderIcon\": \"./img/icon.ico\",\n        \"createDesktopShortcut\": true,\n        \"createStartMenuShortcut\": true,\n        \"shortcutName\": \"Fily\"\n    },\n    \"win\": {\n        \"icon\": \"./img/icon.ico\",\n        \"target\": [\n            {\n                \"target\": \"nsis\",\n                \"arch\": [\n                    \"ia32\"\n                ]\n            }\n        ]\n    },\n    \"dmg\": {\n        \"background\": \"media/images/dmg-bg.png\",\n        \"icon\": \"media/images/icon.icns\",\n        \"iconSize\": 100,\n        \"sign\": false,\n        \"contents\": [\n            {\n                \"x\": 112,\n                \"y\": 165\n            },\n            {\n                \"type\": \"link\",\n                \"path\": \"/Applications\",\n                \"x\": 396,\n                \"y\": 165\n            }\n        ]\n    },\n    \"mac\": {\n        \"target\": [\n            \"dmg\"\n        ],\n        \"icon\": \"./img/Icon.icns\",\n        \"hardenedRuntime\": true,\n        \"identity\": null,\n        \"entitlements\": \"electron-builder/entitlements.plist\",\n        \"entitlementsInherit\": \"electron-builder/entitlements.plist\",\n        \"provisioningProfile\": \"electron-builder/comalibabaslobs.provisionprofile\"\n    },\n    \"pkg\": {\n        \"isRelocatable\": false,\n        \"overwriteAction\": \"upgrade\"\n    },\n    \"mas\": {\n        \"icon\": \"./img/Icon.icns\",\n        \"hardenedRuntime\": true,\n        \"entitlements\": \"electron-builder/entitlements.mas.plist\",\n        \"entitlementsInherit\": \"electron-builder/entitlements.mas.plist\"\n    }\n}\n```\n\nelectron-forge添加如下参数：\n\n```json\n\"config\": {\n    \"forge\": {\n        \"packagerConfig\": {},\n        \"makers\": [\n            {\n                \"name\": \"@electron-forge/maker-squirrel\",\n                \"config\": {\n                    \"name\": \"fily\"\n                }\n            },\n            {\n                \"name\": \"@electron-forge/maker-zip\",\n                \"platforms\": [\n                    \"darwin\"\n                ]\n            },\n            {\n                \"name\": \"@electron-forge/maker-deb\",\n                \"config\": {}\n            },\n            {\n                \"name\": \"@electron-forge/maker-rpm\",\n                \"config\": {}\n            }\n        ]\n    }\n}\n```\n\n然后配置script脚步指定指定的任务：\n\n```json\n\"scripts\": {\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\",\n    \"start\": \"electron-forge start\",\n    \"package\": \"electron-forge package\",\n    \"dist\": \"npm run package && electron-builder build\",\n    \"make\": \"electron-forge make\"\n}\n```\n\n## 四、源码\n\n该实验项目开源，代码存放在 [🔗Github](https://github.com/xuhang/fily.git)\n\n\n[1]: https://www.electronjs.org/apps\n[2]: https://github.com/xuhang/fily.git",
      "data": {
        "title": "Electron 应用开发",
        "date": "2024-12-08 09:00:31",
        "tags": [
          "electron",
          "note",
          "app",
          "fily"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "## 一、了解Electron\n\nElectron 是由 Github 开发，用 HTML，CSS 和 JavaScript 来构建跨平台桌面应用程序的一个开源库。最初的时候是属于 Github 开源代码编辑器 Atom 的一部分，在 2014 春季这两个项目相继开源。",
      "fileName": "electron-ying-yong-kai-fa"
    },
    {
      "content": "## 基础架构\n![MySQL 基础架构](https://xuhang.github.io/post-images/1733619417022.png)\n\n### 连接器\n\n连接器负责跟客户端建立连接、获取权限、维持和管理连接。\n- 如果用户名或密码不对，你就会收到一个\"Access denied for user\"的错误，然后客户端程序结束执行。\n-   如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。\n\n<!-- more -->\n\n客户端如果太长时间没动静（show processlist结果的Command列显示为sleep），连接器就会自动将它断开。这个时间是由参数wait_timeout控制的，默认值是8小时。\nMySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，导致MySQL异常重启。\n解决办法：\n1.  定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。\n2.  如果你用的是MySQL 5.7或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。\n\n### 查询缓存\n执行过的语句及其结果可能会以key-value对的形式，被直接缓存在内存中。key是查询的语句，value是查询的结果。如果你的查询能够直接在这个缓存中找到key，那么这个value就会被直接返回给客户端。\n\n查询缓存的失效非常频繁，**只要有对一个表的更新，这个表上所有的查询缓存都会被清空**。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。\n\n参数query_cache_type设置成DEMAND，这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用SQL_CACHE显式指定，像下面这个语句一样：\n\n```sql\nmysql> select SQL_CACHE * from T where ID=10；\n```\n\n> MySQL 8.0版本直接将查询缓存的整块功能删掉了，也就是说8.0开始彻底没有这个功能了。\n\n### 分析器\n\n分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条SQL语句，MySQL需要识别出里面的字符串分别是什么，代表什么。\n\n做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个SQL语句是否满足MySQL语法。\n\n### 优化器\n\n优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。\n\n### 执行器\n\n开始执行的时候，要先判断一下你对这个表T有没有执行查询的权限，如果没有，就会返回没有权限的错误。\n\n如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。\n\n\n## 日志系统\n\n### 日志模块：redo log\n\n如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高。MySQL使用WAL技术来提升效率，WAL的全称是Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。\n\n有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为**crash-safe**。\n\n### 日志模块：binlog\n\n上面聊到的redo log是InnoDB引擎特有的日志，而Server层也有自己的日志，称为binlog（归档日志）。\n\n因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统——也就是redo log来实现crash-safe能力。\n\n两种日志有以下三点不同。\n\n1. redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。\n2. redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。\n3. redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。\n\n```sql\nmysql> update T set c=c+1 where ID=2;\n```\n\n执行上面这条update语句时执行器和InnoDB引擎内部的流程：\n\n1. 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。\n\n2. 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。\n\n3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。\n\n4. 执行器生成这个操作的binlog，并把binlog写入磁盘。\n\n5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。\n\n![执行器和InnoDB引擎内部的流程](https://xuhang.github.io/post-images/1733619501073.png)\n\ninnodb_flush_log_at_trx_commit这个参数设置成1的时候，表示每次事务的redo log都直接持久化到磁盘。sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。\n\n> - binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED，区别如下：\n>   - STATEMENT：每一条修改数据的 SQL 都会被记录到 binlog 中（相当于记录了逻辑操作，所以针对这种格式， binlog 可以称为逻辑日志），主从复制中 slave 端再根据 SQL 语句重现。但 STATEMENT 有动态函数的问题，比如你用了 uuid 或者 now 这些函数，你在主库上执行的结果并不是你在从库执行的结果，这种随时在变的函数会导致复制的数据不一致；\n>   - ROW：记录行数据最终被修改成什么样了（这种格式的日志，就不能称为逻辑日志了），不会出现 STATEMENT 下动态函数的问题。但 ROW 的缺点是每行数据的变化结果都会被记录，比如执行批量 update 语句，更新多少行数据就会产生多少条记录，使 binlog 文件过大，而在 STATEMENT 格式下只会记录一个 update 语句而已；\n>   - MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式；\n> - redo log 是物理日志，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新；\n\n## 事务隔离\n\n### 隔离性与隔离级别\n\n事务拥有四个重要的特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）\n\n- **原子性** （Atomicity）\n\n事务开始后所有操作，要么全部做完，要么全部不做，不可能停滞在中间环节。事务执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生一样。\n\n- **一致性** （Consistency）\n\n指事务将数据库从一种状态转变为另一种一致的的状态。事务开始前和结束后，数据库的完整性约束没有被破坏。\n\n- **隔离性** （Isolation）\n\n要求每个读写事务的对象对其他事务的操作对象能互相分离，即该事务提交前对其他事务不可见。 也可以理解为多个事务并发访问时，事务之间是隔离的，一个事务不应该影响其它事务运行效果。这指的是在并发环境中，当不同的事务同时操纵相同的数据时，每个事务都有各自的完整数据空间。由并发事务所做的修改必须与任何其他并发事务所做的修改隔离。\n\n> MySQL 通过锁机制来保证事务的隔离性。\n\n- **持久性** （Durability）\n\n事务一旦提交，则其结果就是永久性的。即使发生宕机的故障，数据库也能将数据恢复，也就是说事务完成后，事务对数据库的所有更新将被保存到数据库，不能回滚。\n\n| **隔离级别**     | 脏读 | **不可重复读** | **幻读** |\n| ---------------- | ---- | -------------- | -------- |\n| SERIALIZABLE     | 避免 | 避免           | 避免     |\n| REPEATABLE READ  | 避免 | 避免           | 允许     |\n| READ COMMITTED   | 避免 | 允许           | 允许     |\n| READ UNCOMMITTED | 允许 | 允许           | 允许     |\n\n>### 幻读和不可重复读的区别\n>\n>- 不可重复读的重点是修改：在同一事务中，相同的条件，第一次和第二次读到的数据不一致（中间有其它事务提交了修改）。\n>- 幻读的重点是新增或者删除：在同一事务中，相同的条件，第一次和第二次读到的记录数不一样（中间有其它事务提交了新增或者删除）。\n\n![隔离级别](https://xuhang.github.io/post-images/1733619524395.png)\n\n在不同的隔离级别下，事务A的返回结果：\n\n- 若隔离级别是“读未提交”， 则V1的值就是2。这时候事务B虽然还没有提交，但是结果已经被A看到了。因此，V2、V3也都是2。\n- 若隔离级别是“读提交”，则V1是1，V2的值是2。事务B的更新在提交后才能被A看到。所以， V3的值也是2。\n- 若隔离级别是“可重复读”，则V1、V2是1，V3是2。之所以V2还是1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。\n- 若隔离级别是“串行化”，则在事务B执行“将1改成2”的时候，会被锁住。直到事务A提交后，事务B才可以继续执行。所以从A的角度看， V1、V2值是1，V3的值是2。\n\n在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“**可重复读**”隔离级别下，这个视图是在**事务启动时**创建的，整个事务存在期间都用这个视图。在“**读提交**”隔离级别下，这个视图是在每个**SQL语句开始执行**的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。\n\n### 事务隔离的实现\n\n在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。\n\n>为什么尽量不要使用长事务？\n>\n>长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。\n\n\n\n*多版本并发控制*（Multiversion Concurrency Control），每一个写操作都会创建一个新版本的数据，读操作会从有限多个版本的数据中挑选一个最合适的结果直接返回；在这时，读写操作之间的冲突就不再需要被关注，而管理和快速挑选数据的版本就成了 MVCC 需要解决的主要问题。\n\n#### MySQL与MVCC\n\nMySQL 中实现的多版本两阶段锁协议（Multiversion 2PL）将 MVCC 和 2PL 的优点结合了起来，每一个版本的数据行都具有一个唯一的时间戳，当有读事务请求时，数据库程序会直接从多个版本的数据项中具有最大时间戳的返回。\n\n更新操作就稍微有些复杂了，事务会先读取最新版本的数据计算出数据更新后的结果，然后创建一个新版本的数据，新数据的时间戳是目前数据行的最大版本 `＋1`：\n\n数据版本的删除也是根据时间戳来选择的，MySQL 会将版本最低的数据定时从数据库中清除以保证不会出现大量的遗留内容。\n\n#### 事务的起点\n\n`begin`/`start transaction` 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用`start transaction with consistent snapshot `这个命令。\n\n在MySQL里，有两个“视图”的概念：\n\n- 一个是view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是create view … ，而它的查询方法与表一样。\n- 另一个是InnoDB在实现MVCC时用到的一致性读视图，即consistent read view，用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。\n\n### 事务的启动方式\n\nMySQL的事务启动方式有以下几种：\n\n1. 显式启动事务语句， begin 或 start transaction。配套的提交语句是commit，回滚语句是rollback。\n2. set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个select语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行commit 或 rollback 语句，或者断开连接。\n\n## 索引\n\n### InnoDB 的索引模型\n\n在InnoDB中，表都是根据**主键顺序**以索引的形式存放的，这种存储方式的表称为索引组织表。\n\n每一个索引在InnoDB里面对应一棵B+树。\n\n根据叶子节点的内容，索引类型分为主键索引和非主键索引。\n\n- 主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引（clustered index）。\n- 非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引（secondary index）。\n\n\n> **基于主键索引和普通索引查询的区别**\n>\n> - 如果语句是select * from T where ID=500，即主键查询方式，则只需要搜索ID这棵B+树；\n> - 如果语句是select * from T where k=5，即普通索引查询方式，则需要先搜索k索引树，得到ID的值为500，再到ID索引树搜索一次。这个过程称为**回表**。\n>\n> *基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。*\n\n### 覆盖索引\n\n如果一个索引包含所有需要查询的字段的值，称为**覆盖索引**，即只需扫描索引而无须回表。由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。\n\n### 最左前缀原则\n\n对于联合索引的索引项是按照索引定义里面出现的字段顺序排序的，比如对列A，B，C建立联合索引，实际上是建立了(A)，(A, B)，(A, B, C)三个索引。\n\n1. MySQL会一直向右匹配直到遇到范围查询(`>`、`<`、`between`、`like`)就停止匹配，比如A = 1 and B = 2 and C > 3 and D = 4 如果建立(A，B，C，D)顺序的索引，d是用不到索引的，如果建立(A，B，D，C)的索引则都可以用到，A，B，D的顺序可以任意调整。\n\n2. `=`和`in`可以乱序，比如A = 1 and B = 2 and C = 3 建立(A，B，C)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。\n\n**为什么要用联合索引**\n\n- 减少开销。建一个（A，B，C）的联合索引，实际上是建立了(A)，(A, B)，(A, B, C)三个索引。\n- 覆盖索引。联合索引（A，B，C）的叶节点包含ABC三列的数据，如果查询的列在（A，B，C）之中可以直接通过索引获取到数据，无需回表，减少树的随机IO。\n- 效率高。通过联合索引筛选出的数据少，列越多数据越少\n\n### 索引下推\n\n对于**联合索引**中不符合最左前缀的部分，MySQL的处理方式：\n\n- 在MySQL 5.6之前，只能从第一个满足最左前缀索引的数据开始一个个回表。到主键索引上找出数据行，再对比字段值。\n- 而MySQL 5.6 引入的**索引下推优化**（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。\n\n也就是说在5.6之前的版本，不符合最左前缀的部分对InnoDB引擎是没用的，5.6及以后的版本进行了优化，直接在索引上进行筛选，减少回表次数。\n\n### 普通索引\n\n当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。\n\n> 唯一索引的更新就不能使用change buffer\n\n对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。\n\n## 锁\n\n### 全局锁\n\n全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。\n\n**全局锁的典型使用场景是，做全库逻辑备份。**也就是把整库每个表都select出来存成文本。\n\n对于支持事务的引擎，比如InnoDB，可以通过启动一个事务，确保拿到一致性视图，而且由于MVCC的支持，这个过程不需要锁库。对于不支持事务的引擎，比如MyISAM，就需要全局锁FTWRL来避免数据的更新。\n\n> **set global readonly=true**的方式也能使整个库只读，但是这种方式有两个弊端：\n>\n> 1. 在有些系统中，readonly的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。\n> 2. 在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时\n> 3. 间处于不可写状态，风险较高。\n\n### 表级锁\n\nMySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。\n\n**表锁的语法是 lock tables … read/write。**与FTWRL类似，可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。\n\n**另一类表级的锁是MDL（metadata lock)。**MDL不需要显式使用，在访问一个表的时候会被自动加上。MDL的作用是，保证读写的正确性。\n\nMySQL 5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。\n\n- 读锁之间不互斥，因此你可以有多个线程同时对一张表**增删改查**。\n- **读写锁**之间、**写锁**之间是互斥的，用来保证变更表结构操作的安全性。\n\n| 时间线 | session A       | session B       | session C                        | session D               |\n| ------ | --------------- | --------------- | -------------------------------- | ----------------------- |\n| t1     | begin -> select |                 |                                  |                         |\n| t2     |                 | begin -> select |                                  |                         |\n| t3     |                 |                 | begin -> **alter table**【阻塞】 |                         |\n| t4     |                 |                 |                                  | begin -> select【阻塞】 |\n\n> **如何安全地给小表加字段？**\n>\n> 比较理想的机制是，在alter table语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。\n\n```sql\nALTER TABLE tbl_name NOWAIT add column ...\nALTER TABLE tbl_name WAIT N add column ... \n```\n\n### 行锁\n\nMySQL的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如MyISAM引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB是支持行锁的，这也是MyISAM被InnoDB替代的重要原因之一。\n\n#### 两阶段锁\n\n**在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。**\n\n> 如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。这样事务之间锁等待的时间最少，提高并发度。\n\n### 死锁和死锁检测\n\n当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。\n\n解决死锁的两种策略：\n\n1. 直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout来设置，但是会误伤等待非死锁的查询。\n2. 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。但是死锁检测在并发情况下会消耗大量CPU资源。\n\n结合实际情况中的场景，一般是启用死锁检测，控制客户端的并发数，这样死锁检测不至于太占用CPU资源。\n\n## 如何保证数据不丢失\n\n### binlog写入机制\n\n事务执行过程中将日志写到 binlog cache，事务提交的时候，将 binlog cache 写入 binlog 文件中。一个事务的 binlog 是不能被拆分的，如果这个事务多的，都要确保一次性写入。系统给每个线程分配一个 binlog cache 内存区域（但是 binlog 文件只有一份），参数`binlog_cache_size`用于控制单个线程内 binlog cache 所占内存大小，如果超过这个大小，就要暂存到磁盘上。事务提交的时候，把 binlog cache 中的完整事务写入 binlog 中，并清空 binlog cache。\n\n参数`sync_binlog`的含义：\n\n+ `0` ：每次事务提交只 write（将 binlog cache 中内容写入文件系统的 page cache），不 fsync（将 page cache 中数据持久化到硬盘）\n+ `1`：每次事务提交都会执行 fsync\n+ `N`：每次提交都 write，但是累积 N 个事务后才 fsync\n\n### redo log 写入机制\n\n事务执行过程中将日志写到 redo log buffer，然后由`innodb_flush_log_at_trx_commit`参数控制 redo log 持久化的逻辑。\n\n参数`innodb_flush_log_at_trx_commit`的含义：\n\n+ `0`：每次事务提交只是把 redo log 留在 redo log buffer 中；\n+ `1`：每次事务提交都将 redo log 持久化到硬盘；\n+ `2`：每次事务提交只是把 redo log 写到 page cache。\n\n同时，InnoDB 后台有个线程，每隔 1s，将 redo log buffer 中的日志调用 write 写到文件系统的 page cache 中，然后调用 fsync 持久化到硬盘。所以事务未提交时，redo log buffer 中的内容也有可能被持久化到硬盘上。\n\n此外，还有 2 种情况会让没有提交的事务的 redo log 写入磁盘：\n\n1. redo log buffer 占用空间即将达到`innodb_log_buffer_size`的一半时，后台线程会主动写盘(write，没有 fsync)。\n2. 并行事务提交时，顺带将这个事务的 redo log buffer 持久化到磁盘。\n\n### 组提交（Group commit）机制\n\n日志逻辑序列号（log sequence number, LSN）:单调递增，对应 redo log 上的一个个写入点，每次写入长度为 length 的 redo log，LSN 的值就会加上 length。\n\nLSN 也会写到 InnoDB 的数据页中，用来确保数据页不会被多次执行重复的 redo log。\n\n多个事务同时执行，最先到达的会被选为这组的 leader；leader 写盘的时候会将整组的 redo log 一起持久化，组员越多，节约 IOPS 的效果越好。\n\n`binlog_group_commit_sync_delay`：binlog 调用 write 多少微妙后才调用 fsync\n\n`binlog_group_commit_sync_no_delay_count`：累积多少次后调用 fsync\n\n## 如何保证主备一致\n\n通常将备库设置成只读(readonly)，可以方便地判断节点状态，但是 readonly 并不影响备库和主库保持同步。同步更新的线程，拥有超级权限，而 readonly 对超级权限用户是无效的。\n\n事务日志同步的完整过程：\n\n1. 备库上通过`change master`命令设置主库 IP、端口、用户名、密码以及 binlog 文件名和日志偏移量（从哪个位置开始请求 binlog）\n2. 在备库上执行`start slave`命令，启动`io_thread`和`sql_thread`两个线程，分别用于负责与主库建立连接和解析日志执行\n3. 主库校验完用户名密码后，按照备库制定的文件和位置，读取 binlog 发送给备库\n4. 备库拿到 binlog 后写入本地文件（中转日志 relay log）\n5. sql_thread 从中转日志读取并解析，然后执行\n\n### binlog 的三种格式\n\n+ `STATEMENT`：binlog 记录的时原始 SQL 语句（可能导致主备不一致）\n+ `ROW`：记录的是受影响的记录（占用空间），但是方便恢复数据\n+ `MIXED`：由 MySQL 判断是否会影响主备不一致选择 ROW 或 STATEMENT\n\n>通过 SQL查看 binlog 中的内容：\n>\n>```show binlog events in 'xxxx.0000001';```\n>\n>借助 mysqlbinlog 工具查看 binlog 详细信息：\n>\n>```mysqlbinlog -vv data/master.000001[binlog 文件] --start-position=9xxx[从指定行开始解析] --stop-position=9xxx[解析截止到制定行]```\n\n",
      "data": {
        "title": "MySQL45 讲笔记",
        "date": "2024-12-08 08:56:12",
        "tags": [
          "mysql",
          "geektime",
          "note"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "## 基础架构\n![MySQL 基础架构](https://xuhang.github.io/post-images/1733619417022.png)\n\n### 连接器\n\n连接器负责跟客户端建立连接、获取权限、维持和管理连接。\n- 如果用户名或密码不对，你就会收到一个\"Access denied for user\"的错误，然后客户端程序结束执行。\n-   如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。",
      "fileName": "mysql45-jiang-bi-ji"
    },
    {
      "content": "## 数据类型和数据结构\n\n| 数据类型   | 数据结构                             |\n| ---------- | ------------------------------------ |\n| String     | SDS（简单动态字符串）                |\n| List       | 双向链表、压缩列表(ZipList)          |\n| Hash       | 压缩列表(ZipList)、哈希表(HashTable) |\n| Set        | 整数数组(IntSet)、哈希表(HashTable)  |\n| Sorted Set | 压缩列表(ZipList)、跳表(SkipList)    |\n\n### 压缩列表\n\n压缩列表类似数组，不同的是压缩列表表头有3个字段`zlbytes`、`zltail`、`zllen`，分别表示列表长度（字节数）、列表尾元素偏移量、列表中元素个数。此外列表尾还有一个`zlend`表示列表结束，固定值`0xFF`。\n<!-- more -->\n\n![](https://xuhang.github.io/post-images/1733574319043.png)\n\n压缩列表只有查找第一个元素、最后一个元素获取列表长度的时间复杂度为`O(1)`，查找其他元素为`O(N)`。\n\n```\narea        |<------------------- entry -------------------->|\n\n            +------------------+----------+--------+---------+\ncomponent   | pre_entry_length | encoding | length | content |\n            +------------------+----------+--------+---------+\n```\n\npre_entry_length:\n\n- `1` 字节：如果前一节点的长度小于 `254` 字节，便使用一个字节保存它的值。\n- `5` 字节：如果前一节点的长度大于等于 `254` 字节，那么将第 `1` 个字节的值设为 `254` ，然后用接下来的 `4` 个字节保存实际长度。\n\nRedis 基于压缩列表实现了 List、Hash 和 Sorted Set 集合类型，可以节省内存空间。\n\n### 跳表\n\n本质是一个有序链表，通过增加多级索引，实现数据的快速定位，时间复杂度为`O(LogN)`。\n\n### Hash\n\nRedis Hash 类型有两种底层实现，分别是压缩列表和哈希表。\n\n`hash-max-ziplist-entries`：用压缩列表保存哈希时的最大元素个数，默认 512\n\n`hash-max-ziplist-value`：用压缩列表保存哈希时单个元素的最大长度，默认 64\n\n二者满足一个超过阈值，Redis 就会自动把 Hash 的实现由压缩列表转为哈希表，之后就会一直用哈希表进行保存，不会转回为压缩列表。\n\n当使用 `REDIS_ENCODING_ZIPLIST` 编码哈希表时， 程序通过将键和值一同推入压缩列表， 从而形成保存哈希表所需的键-值对结构。\n\n![](https://xuhang.github.io/post-images/1733574355472.png)\n\n新添加的 key-value 对会被添加到压缩列表的表尾。\n\n当进行查找/删除或更新操作时，程序先定位到键的位置，然后再通过对键的位置来定位值的位置。\n\n## Redis 6.0新特性\n\n### 多线程处理网络请求\n\n只是用多线程处理网络请求，读写命令仍然使用单线程。可以有效解决网络IO的瓶颈，提升整体性能。\n\n```ini\nio-threads-do-reads yes\n# 启用多线程处理IO\n\nio-threads 6\n# 线程数\n```\n\n### Tracking\n\n业务应用中的Redis客户端可以把读取的数据缓存在业务应用本地。\n\n**普通模式**：实例会在服务端记录客户端读取过的key，并监控key是否有修改，如果发送变化，服务端会给客户端发送`invalidate`消息，通知客户端缓存失效。但是该消息只会发送一次，只有客户端再次从服务端读取数据时，可能再次发送。通过命令`CLIENT TRACKING ON|OFF`启用或关闭。\n\n**广播模式**：服务端会给客户端广播所有key的失效情况，如果key被频繁修改，可能会消耗大量网络带宽。\n\n```shell\nCLIENT TRACKING ON BCAST PREFIX user\n# 客户端指定监听的key的前缀，这个前缀的key被修改后，服务端会给客户端发送invalidate消息。需要key的命名规范。\n```\n\n上面的两种模式需要使用6.0新增的`RESP 3`通信协议，对于使用`RESP 2`\t协议的客户端，需要使用重定向模式，即客户端需要使用`SUBSCRIBE`命令订阅`_redis_:invalidate`频道用于监听key失效的消息，同时还需要执行`CLIENT TRACKING`命令，用于接收`RESP 2`客户端重定向过来的消息。\n\n### 权限控制\n\n6.0之前只能通过设置密码来控制安全访问，6.0提供了更加细粒度的访问权限控制。\n\n**用户**：6.0之前没有用户概念，6.0可以创建用户并单独设置密码\n\n```shell\nACL SETUSER <USER-NAME> on > <PSSWORD>\n```\n\n**权限**：可以将具体的操作权限，赋予给用户或撤销\n\n```shell\nACL SETUSER <USER-NAME> +@hash -@string\n```\n\n|      命令      | 说明                                   |\n| :------------: | -------------------------------------- |\n|  `+<COMMAND>`  | 将==一个命令==添加到用户的可执行列表中 |\n|  `-<COMMOND>`  | 将==一个命令==从用户可执行列表中移除   |\n| `+@<CATEGORY>` | 将==一类命令==添加到用户的可执行列表中 |\n| `-@<CATEGORY>` | 将==一类命令==从用户可执行列表中移除   |\n|    `+@ALL`     | 运行执行==所有==命令                   |\n|    `-@ALL`     | 禁止执行==所有==命令                   |\n\n还可以以key的粒度设置访问权限。\n\n```shell\nACL SETUSER <USER-NAME> ~KEY:* +@ALL\n```\n\n允许用户对KEY为前缀的key的所有操作。\n\n## rehash\n\nRedis使用2个全局哈希表，类似JVM堆内存中的Survivor区的S1和S2，始终只使用其中一个，rehash过程中使用另一个进行替换。\n\n1. 给哈希表2分配更大的空间\n2. 把哈希表1中的数据重新映射并拷贝到哈希表2中\n3. 释放哈希表1的空间\n\n但是步骤 2 在拷贝数据时，如果一次性拷贝大量数据会造成Redis线程阻塞，无法服务其他请求。为避免这个问题，Redis采用了**<u>渐进式rehash</u>**。\n\n具体步骤是在第2步拷贝数据时，仍然正常处理客户端请求，同时从原哈希表中的第一个索引位置开始，连同本次请求所在的索引位置上的所有entries拷贝到新的哈希表中，对于新增操作只会在新的哈希表中新增，保证原哈希表逐渐减小。但是即使没有新的请求，Redis也会以一定的频率定时地执行一次rehash，且每次时长不超过1ms。\n\nrehash时机：\n\nload factor >= 1 且 没有正在进行RDB生成和重新AOF\n\nload facotr >= 5 不管有没有正在进行RDB生成或重新AOF\n\n## bigkey\n\n通过命令`./redis-cli --bigkeys -i 0.1 -a 密码`来查看整个数据库中bigkey情况，该命令会输出每种类型的键值对个数和平均大小，还会输出每种数据类型中最大的bigkey信息。因为这个工具会扫描整个数据库，可能会对Redis性能产生影响，所以最后是在从库上执行，同时加上`-i`选项指定扫描间隔，避免长时间的扫描。但是整个命令只返回了每个数据类型中最大的bigkey，对于集合类型是元素个数，无法查看Top-N的情况。所以最好是通过`SCAN`命令扫描数据库，然后用`TYPE`命令获取值的类型；对于集合类型，使用`LLEN(List)`、`HLEN(Hash)`、`SCARD(Set)`、`ZCARD(Sorted Set)`获取元素个数，对于字符串，使用`STRLEN`获取字符串长度；或者使用`MEMROY USAGE <KEY>`获取占用的内存空间。\n\n## 高性能的IO模型\n\n通常说的Redis是单线程的，主要是值Redis的网络IO和键值对的读写是由一个线程完成的。Redis在6.0中加入了多线程IO，提高网络请求处理的并行度，但是仅仅是对于网络请求的处理是使用多线程，命令的读写仍然是单线程。\n\n## AOF日志\n\nAOF日志是写后日志，即先执行命令，数据写入内存后再记录日志。AOF记录的是Redis收到的每一条命令，以文本形式保存。\n\n比如命令`set testkey testvalue`，其AOF日志形式为：\n\n| AOF记录   | 说明                                                         |\n| --------- | ------------------------------------------------------------ |\n| *3        | 当前命令有三部分组成，每部分由`$数字`开头，其中数字表示命令、键、值的字节长度 |\n| $3        | 这部分包含3字节命令，也就是set                               |\n| set       | 命令                                                         |\n| $7        | 7字节，也就是testkey                                         |\n| testkey   |                                                              |\n| $9        | 9字节，也就是testvalue                                       |\n| testvalue |                                                              |\n\n但是为了避免额外的检查开销，Redis不会检查这些命令的语法，所以只有先执行命令，命令执行成功后才会记录AOF日志，这样可以保证记录的是正确的命令。此外还有一个好处是不会阻塞当前的写操作（可能会阻塞下一个操作，因为AOF日志是在主线程中执行的）。\n\nAOF的写回策略：\n\n| 策略         | 过程                                                         | 说明                               |\n| ------------ | ------------------------------------------------------------ | ---------------------------------- |\n| **Always**   | 同步写回，每个写命令执行完，立即同步写回磁盘                 | 基本不丢数据，但是会影响主线程性能 |\n| **Everysec** | 每秒写回，每个写命令执行完，先把日志写到AOF文件的内存缓冲区，每隔一秒把缓冲区内容写入磁盘 | 宕机丢失数据                       |\n| **No**       | 由操作系统控制写回，每个写命令执行完，只把日志写到AOF文件缓冲区，由操作系统决定何时将内容写回磁盘 | 在性能和数据丢失取折中             |\n\nAOF文件过大导致的性能问题：\n\n1. 文件系统对文件大小有限制，不能无限大\n2. 过大的文件写入性能会受到影响\n3. 发生宕机利用AOF记录恢复数据，文件过大会导致整个过程很慢\n\n### AOF重写\n\n解决AOF文件过大的问题，重写时，根据Redis数据库现状生成一个新的AOF文件。对一个键进行的多次写操作，最终会生成一条写命令，可以有效减小AOF文件的大小。AOF重写过程由后台线程`bgrewriteaof`完成，可以避免阻塞主线程。\n\n**一个拷贝，两处日志**：主线程fork出后台的`bgrewriteaof`子进程，会把主线程的内存**拷贝（父进程的页表，而非物理内存）**一份给`bgrewriteaof`子进程，子进程把拷贝的数据计入重写日志。同时，新的写操作会写入**原来的AOF日志**（完整的AOF）和**新的AOF重写日志**（等拷贝生成的重写日志完成后，追加上这个重写期间的增量日志，凑成一份完整的日志，替换原来的AOF日志）\n\n## 内存快照RDB\n\n使用AOF进行故障恢复需要把操作日志重新执行一遍，耗时较长，使用内存快照将大大提升恢复速度。内存快照指的是内存中的数据在某一时刻的状态。\n\nRedis提供了两个命令生成RDB文件：`save` （在主线程中执行，会阻塞）和 `bgsave`（fork子进程执行，默认）\n\n`bgsave`执行过程中，主线程仍能处理写请求，Redis会借助操作系统提供的写时复制(Copy-on-write, COW)正常处理写请求。\n\n如果频繁地生成RDB文件，会给磁盘带来很大压力，而且fork创建`bgsave`子进程的创建过程会阻塞主线程，影响Redis性能。\n\nRedis4.0提出了AOF和内存快照的混合使用：内存快照以一定的频率执行，两次快照期间使用AOF日志记录。\n\n## 主从模式\n\n通常说Redis具有高可靠性，是指数据尽量少丢失、服务尽量少中断。前者通过AOF和RDB保证，后者则是通过冗余副本，即一份数据保存在多个实例上。\n\n主从读写分离中，写操作只在主库上执行，然后同步到从库，读操作则是主从库都可以。\n\n通过命令设置从库：\n\n```shell\nreplicaof MASTER_IP MASTER_PORT\n# 5.0之前使用slaveof命令\n```\n\n1. 从库向主库发送命令`psync ? -1`进行全量同步，命令的第一个参数是主库的`runID`，由于第一次复制从库不知道主库的`runID`，用`?`表示，第二个参数`-1`表示第一次复制；\n2. 主库响应`FULLRESYNC {runID} {offset}`给从库，`runID`为主库的实例ID，offset表示复制进度\n3. 主库执行`bgsave`生成RDB文件，并发送给从库，同时生成RDB文件过程中的写操作记录到`replication buffer`中\n4. 从库收到RDB文件后先清空当前数据库，然后加载RDB文件\n5. 主库将`replication buffer`发送给从库，从库执行操作\n\n如果从库数量很多，且都要和主库进行全量复制，主库就会忙于fork子进程生成RDB文件，且占用主库的网络带宽，可以采用“主-从-从”的模式，以级联的模式将压力分散到从库上。\n\n```shell\nreplicaof SLAVE_IP SLAVE_PORT\n```\n\n一旦完成主从全量复制之后，它们之间就会一直维护一个网络长连接，主库通过这个连接将后续收到的命令同步给从库，长连接可以避免频繁建立连接的开销。\n\n如果发送网络断连，Redis2.8之后，主从库会采用增量复制的方式继续同步。主库会把断连期间收到的写命令，写入`replication buffer`和`repl_backlog_buffer`缓冲区，`repl_backlog_buffer`是一个环形缓冲区，主库记录自己写到的位置(`master_repl_offset`)，从库记录自己读到的位置(`slave_repl_offset`)。主从库的连接恢复之后，从库给主库发送`psync`命令，带着`slave_repl_offset`，主库则将`slave_repl_offset`~ `master_repl_offset`之间的命令操作同步给从库。\n\n> 参数`repl_backlog_size`表示环形缓冲区的大小\n\n### 主从同步的问题\n\n主从数据不一致是指客户端从从库读到的值和主库中的最新值不一致。因为主从库间的命令是异步进行的，另外，主从库间的网络可能有延迟，导致从库接收到主库的同步命令延后；或者从库正在进行复杂度高的命令被阻塞，需要等命令结束了才能继续执行主库同步过来的命令。\n\n`INFO replication`命令可以查看主库接收写命令的进度信息(`master_repl_offset`)和从库同步写命令的进度信息(`slave_repl_offset`)，我们可以通过这个命令监控主从的同步进度，对于同步进度差太多的从库可以选择从客户端的连接信息中移除，等恢复正常了再添加进来。\n\n客户端可能在从库上读到过期的数据，Redis 采用惰性删除和定期删除的策略删除过期的数据，客户端在访问过期数据时，如果数据已过期，则会被删除，给客户端返回空；同时，Redis 会定期随机选出一定数量的数据，检查是否过期，并把过期的数据删除。但是客户端从从库读取到过期数据时，是不会主动删除的。（这个问题在 Redis3.2 之后的版本优化，从库读到过期数据，虽然不会删，但是返回的是空值）另外，如果设置过期时间使用的是`EXPIRE key 60`，命令同步到从库，再到执行，可能会延后一段时间，导致从库上的有效期在主库之后；如果使用的是`EXPIREAT key <TIMESTAMP>`，也可能由于主从节点上的时钟不一致，导致数据的过期时间不一致。不过，还是尽量使用`EXPIREAT`命令设置过期时间，同时主从节点和相同的 NTP 进行时钟同步。\n\n> 将`slave-serve-stale-data`设置为 no，这样从库就只能服务 INFO、SLAVEOF 命令，避免从库执行写命令导致数据不一致。\n>\n> `slave-read-only`为 yes 时，从库只能处理读请求，无法处理写请求，二者有区别\n\n### 脑裂\n\n在主从集群中，同时有两个主节点，都能接收写请求。结果是客户端可能往不同的主节点上写数据，导致数据丢失。\n\n如果是主库的数据还没同步到从库，主库故障，发生主从切换导致的数据丢失， 可以通过原主库的`master_repl_offset`和原从库的`slave_repl_offset`的差值来判断。——如果offset一致，则说明丢数据不是由脑裂导致的。\n\n脑裂发生可能是由于主库“假故障”导致的，主库CPU满载，导致无法响应心跳，哨兵会将主库标记为客观下线，然后开始执行主从切换。但是主库在切换过程中恢复正常，可以正常处理请求，主从切换完成后，客户端的写命令才会发送到新的主库上，主库也开始执行`slave of`和新主库进行全量同步，清空本地数据，导致数据丢失。\n\n通过参数`min-slaves-to-write`和`min-slaves-max-lag`限制至少有N个从库和主库进行数据复制时的ACK消息延迟不超过T秒，主库才会正常接收客户端请求。如果主库“假故障”时，无法响应哨兵心跳，也就无法保证上面的限制，主库就不再接收客户端请求了。\n\n## 哨兵机制\n\n哨兵机制是实现主从库自动切换的关键机制，有效解决主从复制模式下故障转移的问题。\n\n哨兵是运行在特殊模式下的Redis进程，通常以集群的模式运行，哨兵主要负责三个任务：监控、选主、通知。\n\n```shell\n#配置哨兵\nsentinel monitor MASTER_NAME MASTER_IP MASTER_PORT QUORUM\n```\n\n主库上有一个名为`__sentinel__:hello`的频道，所有的哨兵都会订阅这个频道，实现相互通信。哨兵会将自己的IP和端口发布到`__sentinel__:hello`频道，其他哨兵就只到了新哨兵的地址和端口，然后和其建立连接。\n\n哨兵和客户端进行信息同步通用是通过pub/sub机制\n\n|                 事件                  |          频道          |\n| :-----------------------------------: | :--------------------: |\n|           主库下线-主观下线           |        `+sdown`        |\n|         主库下线-退出主观下线         |        `-sdown`        |\n|           主库下线-客观下线           |        `+odown`        |\n|         主库下线-退出客观下线         |        `-odown`        |\n|   从库重新配置-哨兵发送SLAVEOF命令    |  `+slave-reconf-sent`  |\n| 从库重新配置-从库配置新主库，尚未完成 | `+slave-reconf-inprog` |\n|   从库重新配置-从库配置新主库，完成   |  `+slave_reconf-down`  |\n|               主从切换                |    `+switch-master`    |\n\n哨兵把新的主库选出来后，客户端根据订阅得到的事件(`switch-master`)和新主库进行通信\n\n```shell\nswitch-master MASTER_NAME OLD_IP OLD_PORT NEW_IP NEW_PORT\n```\n\n哨兵会向主库发送`INFO`命令，该命令会返回从库列表，哨兵根据返回的信息和从库建立连接，通过此连接持续地对从库进行监控。\n\n哨兵在运行时，周期性地给所有**主从**库发送PING命令，检查它们是否在线，如果没有在规定时间内响应哨兵的PING命令，哨兵就会将其标记为“主观下线”，如果是主库“主观下线”，该哨兵就会给其他哨兵实例发送`is-master-down-by-addr`命令，其他实例会根据自己和主库的连接情况，响应`Y` 或 `N`，如果得到的赞成票达到配置的`QUORUM`值，就可以将主库标记为“客观下线”。然后哨兵给其他哨兵发送命令，表明希望由自己来执行主从切换，其他哨兵投票，这个过程称为“Leader选举”。但是同一时间可能有多个哨兵判断主库为“客观下线”，多个哨兵都会发起“Leader选举”，其他哨兵会给第一个向其发送命令的哨兵投Y，给后续其他哨兵投N。最终拿到赞成票`>= N/2 + 1 && >= QUORUM`的哨兵当选为“Leander”来执行主从切换。如果不满足“Leader”票数，哨兵集群会等待一段时间(哨兵故障转移时间*2)，再次重新选举。\n\n> 保证所有哨兵实例的配置是一致的，否则可能由于主观下线的时间(down-after-milliseconds)不一致导致主库故障切换不及时。\n\n## 切片集群\n\n如果Redis数据库数据量特别大，占用很大内存，会导致RDB非常耗时，同时fork创建子进程阻塞主线程的时间也会相对较长。可以使用切片集群，或者分片集群，即启动多个Redis实例组成一个集群，按照一定的规则将数据划分成多份，每一份保存在一个实例上。这样每一个实例生成RDB的文件就小了很多。\n\n纵向扩展(scale up)：升级单个Redis实例的配置，如内存、磁盘，CPU\n\n横向扩展(scale out)：增加Redis实例个数\n\n切片集群是一种保存大量数据的通用机制，Redis3.0之后，官方提供了名为Redis Cluster的实现方案。Redis Cluster方案采用哈希槽(Hash Slot)来处理数据和实例之间的映射关系。一个切片集群共有16384($2^{14}$)个哈希槽，类似于数据分区，每个数据都会根据其key(CRC16算法)被映射到一个槽中。\n\nRedis实例会把自己的哈希槽信息发送给和它相连的其他实例，来完成哈希槽分配信息的扩散，实例之间互联后，所有实例就都有哈希槽的映射关系了。客户端收到哈希槽信息后会缓存在本地，请求时先计算键对应的哈希槽，然后给响应的实例发送请求。但是如果集群实例有删减、或者实例负载不均衡都可能需要重新分配哈希槽。重新分配之后，各实例之间仍然可以通过连接相互传递分配信息实现同步，但是客户端无法感知这些变化。所以Redis Cluster提供了一种重定向机制，即客户端按原来的流程给实例发送命令，但是实例上没有该键映射的哈希槽，就会返回一个`MOVED`响应，包含新实例的地址和端口，客户端重新向新实例发送请求，同时更新缓存的映射关系。\n\n使用`cluster create`命令创建集群，Redis默认会把这些槽平均分布在集群实例上。也可以通过命令`cluster meet`手动建立实例间的连接，再用`cluster addslots`指定每个实例上的slot。\n\n```shell\n#加入集群\nredis-cli -p 6379 cluster meet 127.0.0.1 6380\n# 查看集群节点信息\ncluster nodes\ncluster info\n# 分配槽\nredis-cli -h IP -p PORT cluster addslots {0..5461}\n```\n\n**重新分片的流程**\n\n1. 对目标节点发送`cluster setslot <slot> importing <source-node-id>`命令，让目标节点准备导入槽的数据。\n\n2. 对源节点发送`cluster setslot <slot> migrating <destination-node-id>`命令，让源节点准备迁出槽的数据。\n\n3. 源节点循环执行`cluster getkeysinslot {slot} {count}`命令，获取count个属于槽{slot}的键。\n\n4. 对于步骤3中获取的每个key，`redis-trib.rb`都向源节点发送一个`MIGRATE <target_ip> <target_port> <key_name> 0 <timeout> `命令，将被选中的键原子性地从源节点迁移至目标节点。\n\n5. 重复执行步骤3和4，直到源节点保存的所有属于槽slot的键值对都被迁移到目标节点为止。\n\n6. `redis-trib.rb`向集群中的任意一个节点发送`CLUSTER SETSLOT <slot> NODE <node-id>`命令，将槽slot指派给目标节点。这一消息会发送给整个集群。\n\n**客户端ASK重定向流程**\n\nRedis集群支持在线迁移slot和数据来完成水平伸缩，当slot对应的数据从源节点到目标节点迁移过程中，客户端需要做到智能识别，保证键命令可正常执行。例如，当一个slot数据从源节点迁移到目标节点时，可能会出现一部分数据在源节点，另一部分在目标节点。\n\n如果出现这种情况，客户端键执行流程将发生变化，如下所示，\n\n1. 客户端根据slot缓存发送命令到源节点，如果存在key则直接执行并返回结果。\n\n2. 如果key不存在，则可能存在于目标节点，这时会回复ASK重定向异常，格式如下：`(error) ASK {slot} {targetIP}:{targetPort}`。\n\n3. 客户端从ASK重定向异常提出目标节点信息，发送asking命令到目标节点打开客户端连接标识，再执行键命令。如果存在则执行，不存在则返回不存在信息。\n\nASK与MOVED虽然都是对客户端进的重定向，但是有着本质区别，前者说明集群正在进行slot数据迁移，所以只是临时性的重定向，不会更新slot缓存，但是MOVED重定向说明键对应的槽已经明确指定到新的节点，会更新slot缓存。\n\n## 字符串\n\n简单动态字符串 SDS 的结构：\n\n```c\nstruct __attribute__((__packed)) hisdshdr8 {\n    uint8_t len;\n    uint8_t alloc;\n    unsigned char flags;\n    char buf[];\n}\n```\n\nRedis 中字符串有 5 种结构体：~~`hisdshdr5`~~、`hisdshdr8`、`hisdshdr16`、`hisdshdr32`、`hisdshdr64`，分别表示 len 和 alloc 占用的位数，其中`len`表示 buf 已用长度，`alloc`表示分配的长度，`flags`表示字符串类型，`buf`数组存储实际的数据，为了表示数组的结束，Redis 会自动在数组最后添加`'\\0'`，会额外占用一个字节。对于 String 类型来说，除了 SDS 的开销，还有一个来自`RedisObject`结构体的开销。\n\n```c\ntypedef struct redisObject {\n    unsigned type:4; // 数据类型，如字符串、列表、哈希等\n    unsigned encoding:4; // 编码方式，如 int、raw、hashtable 等\n    unsigned lru:LRU_BITS; // Least Recently Used，用于记录对象最近被访问的时间\n    int refcount; // 引用计数，用于自动内存管理\n    void *ptr; // 指向实际存储数据的指针\n} robj;\n```\n\n当保存的是 Long 类型整数时，RedisObject 中的指针就直接赋值为整数数据，就不用额外指向整数了（int 编码）；当保存的是字符串，并且长度小于等于 44 字节时，RedisObject 中元数据、指针和 SDS是一块连续的内存区域（embstr 编码）；当保存的字符串长度大于44 字节时，RedisObject 和 SDS 就不在连续的内存区域了，而是给 SDS 分配独立的空间，并用指针指向它（raw 编码）\n\n![image-20241009210128383](%E5%9B%BE%E7%89%87%E7%B4%A0%E6%9D%90%E6%96%87%E4%BB%B6%E5%A4%B9/image-20241009210128383.png)\n\n> 字符串长度小于等于 44 字节时，使用 hisdshdr8即可保存，根据hisdshdr8的结构体可知，SDS 占用内存：1(len) + 1(alloc) + 1(flags) + 44(字符串) + 1('\\0') = 48 Bytes，RedisObject 占用内存：8(元数据) + 8 (指针) = 16Bytes，一共 48+16 = 64 字节。\n\nRedis 使用的内存分配库是`jemalloc`，分配内存时，会根据申请的字节数N，找一个>=N，但是最接近 N 的 $2^{n}$数作为分配的空间。\n\n## 消息队列\n\n消息队列的3个基本需求：消息保序、重复消息处理、消息可靠性。\n\n保序就是消费者需要按照生产者发送到消息队列的顺序处理消息；\n\n重复消息处理是在网络堵塞时出现消息重传的情况，消费者可能会收到多条重复的消息；\n\n消息可靠性是消费者出现消息时可能出现故障导致宕机，消息没有处理完成的情况；\n\nRedis的List和Stream两种数据类型满足作为消息队列的三种需求。\n\nList作为队列使用时是按照先进先出的顺序进行存取的，比如`LPUSH`写入消息，`RPOP`消费消息，如果消息队列为空，则客户端需要循环地去请求。Redis提供了`BRPOP`命令，如果消息队列为空，客户端则会阻塞直到有消息写入队列。对于消息的重复处理，则是靠生产者和消费者约定的全局唯一ID实现，生产者给每个写入的消息一个全局唯一ID，消费消费时则记录处理过的ID，对每个ID的消息只处理一次。Redis对消息的可靠性是通过命令`BRPOPLPUSH`命令实现的，这个命令让消费者从队列中读取消息的同时，把这个消息插入到另一个List中备份。当宕机重启后，没处理完成的消息从备份列表中读取再次进行处理。`BRPOPLPUSH MQ_LIST BACKUP_LIST TIMEOUT`\n\n> Redis在6.2.0版本只会引入了BLMOVE用来替代上面的命令\n\nStreams是Redis5.0专为消息队列设计的数据类型。\n\n```shell\nXADD MQ_LIST * KEY VALUE\n# 向消息队列中加入消息，*表示由Redis生成全局唯一ID，也可以自行设定，但是要保证全局唯一\nXREAD BLOCK 100 STREAMS MQ_LIST <ID>|$\n# 从指定的ID开始，读取后面的所有消息，$表示读取最新消息，BLOCK表示阻塞读，阻塞超时时长100ms\n```\n\nStreams支持以消费组的形式消费消息\n\n```shell\nXGROUP create MQ_LIST <GROUP_NAME> 0\n# 创建消费组消费消息队列MQ_LIST\nXREADGROUP group <GROUP_NAME> <CONSUMER_NAME> streams MQ_LIST >\n# 让组内的消费组从MQ_LIST中读取消息，> 表示从第一条未被消费的消息开始读取\n```\n\n为了保证消费组在故障或宕机重启后，仍然能读取未处理完的消息，Streams会自动使用内部队列(PENDING list)留存组里每个消费者读取的消息，直到消费者用`XACK`命令通知Streams消息已处理完。消费者重启后，可以用`XPENDING`命令查看已读取但未ACK的消息。\n\n```shell\nXPENDING MQ_LIST <GROUP_NAME> - + 10 <CUSOMER_NAME>\n# 查询某个消费者已读取，但是未ACK的消息\nXACK MQ_LIST <GROUP_NAME> <ID>\n```\n\n## 异步机制\n\n| 阻塞点                                    | 阻塞结果                                            | 能否异步处理                                                 |\n| ----------------------------------------- | --------------------------------------------------- | ------------------------------------------------------------ |\n| 网络IO                                    | 使用IO多路复用，不会阻塞主线程                      |                                                              |\n| 增删改查                                  | ✖️O(N)复杂度的操作，删除大集合（bigkey）会导致阻塞   | ✖️读操作不能异步，但是删除只用返回给客户端OK，可以异步进行（惰性删除） |\n| 数据库操作                                | ✖️清空数据库（FLUSHDB、FLUSHALL）会阻塞              | 只用返回给客户端OK，可以异步进行                             |\n| 生成RDB文件                               | fork子进程进行，创建子进程过程会短暂阻塞，影响不大  |                                                              |\n| 记录AOF文件                               | ✖️always同步写回策略会阻塞主线程                     | 写回策略改为everysec，Redis会启动子线程来执行AOF日志的写盘   |\n| 重写AOF                                   | fork子进程进行，创建子进程过程会短暂阻塞，影响不大  |                                                              |\n| 主从复制-生成、传输RDB文件                | 子进程处理，不阻塞主线程                            |                                                              |\n| 主从复制-从库接收RDB、清空数据库、加载RDB | ✖️FLUSHDB清空数据库会阻塞，加载RDB会阻塞             | ✖️加载RDB必须主线程执行，不能异步                             |\n| 切片集群-同步哈希槽信息                   | 哈希槽信息量不大，影响不大                          |                                                              |\n| 切片集群-数据迁移                         | Redis Cluster方案使用同步迁移，在迁移bigkey时会阻塞 |                                                              |\n\n> 异步的键值对删除和数据库清空是Redis4.0之后提供的.\n>\n> 删除大量元素的集合时，使用`UNLINK`，清空数据库使用`FLUSHDB ASYNC`和`FLUSHALL ASYNC`\n\n## Redis变慢的原因\n\n### 慢查询命令\n\n在Redis中执行速度慢的命令，会导致Redis延迟增加。\n\n针对慢查询的命令，可以选择其他高效的命令进行替代，比如`SSCAN`多次迭代替换`SMEMBERS`用于查询Set中所有成员。或者针对Set集合的排序、交集、并集可以在客户端完成。\n\n> `KEYS *`命令会遍历所有键值对，延时高，应该避免使用\n\n```ini\nslowlog-log-slower-than\n# 慢查询日志对执行时间大于多少微秒的命令进行记录\n\nslowlog-max-len 1000\n# 慢查询日志最多能记录1000条命令，默认128\n```\n\n使用`SLOWLOG GET`命令查看慢查询日志中记录的操作命令。Redis2.8.13开始提供`latency monitor`监控工具，用来监控Redis运行期间峰值延迟情况，首先设置要监控的阈值`config set latency-monitor-threshold 1000`，然后使用命令`latency latest`命令查看超过1000微秒的延迟情况。\n\n### 过期Key操作\n\n默认情况下，Redis每100ms会删除一些过期的key。步骤如下：采样`ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOK`个数（默认20）的key，将其中过期的key删除；如果过期的key占比找过`25%`，则重复此步骤，直到占比低于`25%`。\n\n> 除了上面的定时任务，当访问一个键时，也会对键的过期时间进行检查，如果过期，就将键删掉。\n>\n> 两种方式删除过期键时，都会产生一个expired通知，产生 expired 通知的时间为过期键被删除的时候， 而不是键的生存时间变为 0 的时候。\n\n如果使用`EXPIREAT`命令设置大量同一时间过期的键，会导致Redis一直忙于删除过期键来释放空间，这个过程会阻塞主线程，导致时延增加。所以在使用`EXPIREAT`或`EXPIRE`设置过期参数时加一个可以接收的随机数。\n\n### AOF写盘\n\nAOF重写时，会进行大量的磁盘IO，可能会导致AOF日志(`everysec`或`always`策略)的`fsync`被阻塞，虽然`fsync`由子线程执行，但是主线程会监控`fsync`的进度，如果上一次的`fsync`未完成，主线程就会阻塞。\n\n`no-appendfsync-on-rewrite yes`配置（默认`no`）表示在AOF重写时不进行`fsync`操作，但是此时宕机会导致数据丢失。\n\n### swap\n\n内存swap是操作系统将内存数据在内存和磁盘间来回换入换出的机制，涉及磁盘的读写，影响性能。当Redis实例占用大量内存，或者同一机器上运行其他进程占用大量内存时，会导致分配给Redis实例的内存不足，进而触发swap。\n\n## 内存碎片\n\n`INFO memory`命令用于查看内存使用情况，`used_memory`是Redis为保存数据实际使用的空间，`used_memory_rss`是操作系统实际分配给Redis的空间，`mem_fragmentation_ratio = used_memory_rss / used_memory` 是表示当前内存碎片率的指标。\n\nRedis从4.0-RC3版本开始，提供了自动清理内存碎片机制，通过命令`config set activedefrag yes`开启该机制。同时满足下面两个配置参数时自动触发内存清理：\n\n```ini\n# 内存碎片字节数达到100MB\nactive-defrag-ignore-bytes 100mb\n# 内存碎片空间占比达到 10%\nactive-defrag-threshold-lower 10\n```\n\n但是内存碎片清理时，需要把多份数据拷贝到新的位置，会阻塞其他操作。\n\n```ini\n# 内存清理过程中所用CPU时间比例最低 25%， 保证清理能正常开展\nactive-defrag-cycle-min 25\n# 内存清理过程中所用CPU时间占比最高 75%， 保证不影响其他操作\nactive-defrag-cycle-max 75\n```\n\n## 缓冲区\n\n服务器给每个连接的客户端配置了一个输入输出缓冲区，输入缓冲区会把客户端发过来的命令暂存起来，Redis主线程从输入缓冲区读取命令再执行；执行完成后将结果写入输出缓冲区，返回给客户端。\n\n### 输入缓冲区\n\n```shell\nclient list\n\n# 查看和Redis服务器相连的每个客户端对输入缓冲区的使用情况\n# ... qbuf=xxx qbuf-free=xxx cmd=xxx\n# qbuf表示缓冲区已使用大小，qbuf-free表示缓冲区未使用的大小，cmd表示最新执行的命令\n```\n\n当客户端写入`bigkey`，或者Redis主线程阻塞无法及时处理客户端命令，导致堆积在缓冲区溢出，Redis就会将连接关闭。输入缓冲区的大小无法调整，Redis设定的是`1G`。\n\n### 输出缓冲区\n\nRedis为每个客户端设置的输出缓冲区包含两部分：大小固定为`16KB`的固定缓冲区，暂存`OK`和出错信息；可以动态增加的缓冲区，存放大小可变的响应结果。\n\n`MONITOR`命令是用来持续监测Redis执行的，该命令的输出会持续占用输出缓冲区，最终发生溢出，所以不要在生产中使用。\n\n输出缓冲区可以通过参数`client-output-buffer-limit`来进行配置\n\n```ini\n# normal: 普通client,包括monitor 的 buffer限制 ，3个0表示不做限制\nclient-output-buffer-limit normal 0 0 0\n# slave：主从的slave client buffer限制，超过256m，或者超过64m持续60s，则关闭客户端连接\nclient-output-buffer-limit slave 256mb 64mb 60\n# pubsub：pubsub模式中的 client buffer限制，超过32m，或者超过8m持续60s，则关闭客户端连接\nclient-output-buffer-limit pubsub 32mb 8mb 60\n```\n\n一般情况下，对于普通客户端，client-output-buffer 是不设限制的，因为客户端每发送一个请求，会等到响应结果后，再进行下一个请求，除非`bigkey`，一般不会积压在缓冲区。对于用作 Pub/Sub 和 slave 的客户端，server 会主动把数据推送给他们，故需要设置 client-output-buffer 的限制。\n\n### 复制缓冲区\n\n主从全量复制过程中，主节点向从节点传输RDB文件的同时，将接收到的客户端写命令保存在复制缓冲区中，等RDB文件传输完成后，再发生给从节点，主节点会为每个从节点维护一个复制缓冲区。\n\n所以复制缓冲区本质上还是一个用于从节点使用的输出缓冲区，发生溢出也会直接关闭。设置缓冲区大小见上面(slave)。\n\n### 复制积压缓冲区\n\n主节点在接收到命令同步给从节点时，会同时写到复制积压缓冲区，用于在网络断开恢复后，将断连期间的写命令，增量同步给从节点。复制积压缓冲区是一个有限的环形缓冲区，写满后会覆盖之前的旧数据，如果从节点还没有同步这些旧命令，会导致主从节点间重新开始全量复制。对比复制缓冲区，发送出去的数据会清除，复制积压缓冲区只会覆盖旧数据，仍然保留了最新的命令。通过`repl_backlog_size`配置复制积压缓冲区的大小。\n\n## 淘汰机制\n\n```shell\nconfig set maxmemory 4gb\n```\n\n设置Redis使用的最大内存，缓存写满后会触发缓存淘汰策略。\n\n| 策略            | 说明                                               |\n| --------------- | -------------------------------------------------- |\n| noeviction      | 不进行数据淘汰(默认)                               |\n| volatile-random | 针对设置了过期时间的键值对，随机删除               |\n| volatile-ttl    | 针对设置了过期时间的键值对，根据过期的先后进行删除 |\n| volatile-lru    | 针对设置了过期时间的键值对，删除最近最没使用的     |\n| volatile-lfu    | 针对设置了过期时间的键值对，删除最近最少使用的     |\n| allkeys-random  | 从所有键值对中随机选择删除                         |\n| allkeys-lru     | 从所有键值对中删除最近最没有使用的                 |\n| allkeys-lfu     | 从所有键值对中删除最近最少使用的                   |\n\nRedis默认记录每个数据的最近一次访问时间戳，保存在RedisObject的lru字段，在决定淘汰数据时，第一次会随机选出N个数据，比较这N个数据的lru字段，把lru最小的删除。通过配置参数`CONFIG SET maxmemory-samples 100`设置选出的样本数N。再次淘汰数据时，Redis需要挑选数据进入第一次淘汰时创建的候选集合，能进入候选集合的数据lru必须小于集合中最小的lrj值。当候选数据个数达到`maxmemory-samples`时，把lru最小的淘汰出去。\n\n## 缓存异常\n\n### 缓存雪崩\n\n缓存雪崩是指大量的应用请求无法在Redis缓存中进行处理，大量的请求送到数据库层，导致数据库压力激增。一般是由于缓存中大量数据同时过期，导致大量请求无法得到处理，或者Redis缓存实例发生故障宕机了，无法处理请求。\n\n针对大量缓存同时过期，可以在给缓存设置过期时间时，加一个较小的随机数，避免大量数据同时过期；\n\n处理方式一般是服务降级：非核心业务，暂时停止从缓存中查询，而是返回预定义信息或者错误信息；核心业务，仍然运行查询缓存，缓存缺失则访问数据库。\n\n如果是服务宕机，为了防止引发连锁的数据库雪崩，甚至整个系统的崩溃，在业务系统中实现服务熔断或请求限流。\n\n### 缓存击穿\n\n某个热点数据失效的场景。相比缓存雪崩，缓存击穿的数据量要小很多。\n\n处理方式是针对热点数据，不设置过期时间。\n\n### 缓存穿透\n\n要访问的数据既不在Redis缓存中，也不在数据库中，导致每次请求都要访问数据库。处理方式是缓存空值或默认值，这样避免请求到底数据库；或者使用布隆过滤器判断数据是否存在，如果不存在就不用访问数据库了；或者针对恶意请求访问不存在的数据，在前端对请求进行过滤。\n\n### 缓存污染\n\n留存在缓存中，但是访问次数很少，或者以后不会再被访问的数据。使用LFU算法的淘汰策略能有效解决缓存污染的问题。Redis在实现LFU策略时，将`lru`字段拆分为两部分：前16bit的时间戳(分钟级别)和后8bit的访问次数。\n\n8bit的访问次数理论最大只有255，但是Redis的实现是一个相对值。计算 $ 1 / (counter * lfu\\_log\\_factor + 1) $ 得到的值和一个$(0, 1)$区间的随机数`r`比较，比 `r`大`counter++`，所以计数器增加是一个概率事件，而且计数器越大，这个概率越低。同时，Redis还加入了计数器衰减的机制，首先计算当前时间和`lru`时间戳的分钟差值，用差值除以`lfu_decay_time`得到计数器要衰减的值。\n\n> 用`lfu_log_factor`限制计数器增长的速度，用`lfu_decay_time`控制计数器衰减的速度。\n>\n> 这里猜测可能有个问题，前16bit表示的`ldt`上次衰减时间计算为` ((unixtime / 60) & 65535)`，最大时间间隔是65535分钟，约45天。若刚好65535分钟后，访问触发衰减时，衰减值` (now >= ldt ? (now - ldt) : (65535 - ldt + now)) / lfu_decay_time ​` 很小，导致本该被淘汰的值继续留存下来。\n\n## 原子操作\n\n### 单命令操作\n\nRedis是单线程处理客户端命令的，所以对于单条命令是原子操作。如果多个操作可以用一条命令实现相同的效果，那么就可以用这个单命令来实现原子操作。比如`DECR <KEY>`用来替代\"GET, -1, SET\"的操作。\n\n### LUA脚本\n\n对于较为复杂的操作，无法用单条命令实现的，用LUA脚本来保证原子操作。\n\n```lua\nlocal current\ncurrent = redis.call(\"incr\", KEYS[1])\nif tonumber(current) == 1 then\n    redis.call(\"expire\", KEYS[1], 60)\nend\n```\n\n然后调用脚本`redis-cli --eval xxx.lua keys, args`\n\n### 分布式锁\n\n**基于单个Redis节点的分布式锁**\n\n加锁：`SET lock_key <RANDOM_STR> NX EX 5`，单命令保证原子性，`NX`只有键不存在时才能创建成功，`EX`设置过期时间5s，随机字符串避免被其他客户端删除。\n\n解锁：`redis-cli --eval unlock.lua lock_key, <RANDOM_STR>`，lua脚本保证原子性，参数包含加锁时的随机字符串，避免锁的误删。\n\n```lua\nif redis.call(\"get\", KEYS[1]) == ARGV[1] then\n    return redis.call(\"del\", KEYS[1])\nelse\n    return 0\nend\n```\n\n**基于多个Redis节点的高可靠分布式锁**\n\n分布式锁算法Redlock：\n\n1. 客户端获取当前时间\n2. 客户端依次向N个Redis实例执行加锁操作，加锁操作的超时时间远小于锁的有效时间\n3. 客户端完成向所有Redis实例的加锁操作，计算整个过程的耗时\n\n同时满足超过半数($N/2 + 1$)的实例加锁超过和总耗时不超过锁的有效时间才算加锁超过。如果不成功，客户端就向所有Redis实例发起释放锁的操作。\n\n## 事务\n\n事务的ACID特性：\n\n+ Atomicity原子性\n+ Consistency一致性\n+ Isolation隔离性\n+ Durability持久性\n\nRedis通过`MULTI`和`EXEC`命令提供事务的支持。\n\n通过这两个命令提交的事务命令要么都**执行**，要么都**不执行**。强调的是执行，而不管成不成功，如果：命令有语法错误，命令入队时会报错，则会放弃，都不执行，保证原子性；如果非语法错误，但是命令执行时报错，全部执行，保证原子性；如果使用`DISCARD`放弃事务，则全部不执行，保证原子性；如果Redis实例故障，在开启AOF日志的前提下，使用工具`redis-check-aof`可以检测未完成的事务，恢复时不执行，保证原子性。\n\nRedis 提供了 `WATCH`命令来保证事务的隔离性，使用`WATCH`对一个或多个key 进行监控，在`EXEC`之前，如果有其他客户端修改了监控的 key，事务会被放弃。\n\nRedis 在两次 RDB 之间，或者 AOF 刷盘前宕机，是保证不了持久性的。\n",
      "data": {
        "title": "Redis核心技术与实战笔记",
        "date": "2024-12-07 20:23:23",
        "tags": [
          "redis",
          "geektime",
          "note"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "## 数据类型和数据结构\n\n| 数据类型   | 数据结构                             |\n| ---------- | ------------------------------------ |\n| String     | SDS（简单动态字符串）                |\n| List       | 双向链表、压缩列表(ZipList)          |\n| Hash       | 压缩列表(ZipList)、哈希表(HashTable) |\n| Set        | 整数数组(IntSet)、哈希表(HashTable)  |\n| Sorted Set | 压缩列表(ZipList)、跳表(SkipList)    |\n\n### 压缩列表\n\n压缩列表类似数组，不同的是压缩列表表头有3个字段`zlbytes`、`zltail`、`zllen`，分别表示列表长度（字节数）、列表尾元素偏移量、列表中元素个数。此外列表尾还有一个`zlend`表示列表结束，固定值`0xFF`。",
      "fileName": "redis-he-xin-ji-zhu-yu-shi-zhan-bi-ji"
    },
    {
      "content": "> 欢迎来到我的小站呀，很高兴遇见你！🤝\n\n## 🏠 关于本站\n\n## 👨‍💻 博主是谁\n\n## ⛹ 兴趣爱好\n\n## 📬 联系我呀\n",
      "data": {
        "title": "关于",
        "date": "2019-01-25 19:09:48",
        "tags": null,
        "published": true,
        "hideInList": true,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "about"
    }
  ],
  "tags": [
    {
      "name": "debug",
      "slug": "bYJHrIt4iL",
      "used": true
    },
    {
      "name": "Java",
      "slug": "UcD7YoRwr9G",
      "used": true
    },
    {
      "name": "James",
      "slug": "7fpwmYgCIx",
      "used": true
    },
    {
      "name": "邮件服务器",
      "slug": "8wyKh9Ri_L5",
      "used": true
    },
    {
      "name": "JavaScript",
      "slug": "1_sYLEQcq5",
      "used": true
    },
    {
      "name": "爬虫",
      "slug": "b5Ycy1BiBE",
      "used": true
    },
    {
      "name": "chrome 插件",
      "slug": "Bv1ZpNk-8gU",
      "used": true
    },
    {
      "name": "python",
      "slug": "3DI6gYWcbcS",
      "used": true
    },
    {
      "name": "设计模式",
      "slug": "JBWj8Qw6SL",
      "used": true
    },
    {
      "name": "kafka",
      "slug": "UyO6XEvkoC",
      "used": true
    },
    {
      "name": "electron",
      "slug": "WetOSYckMA",
      "used": true
    },
    {
      "name": "app",
      "slug": "wtiPysiBope",
      "used": true
    },
    {
      "name": "fily",
      "slug": "85SfQ7PAmp-",
      "used": true
    },
    {
      "name": "mysql",
      "slug": "2d7R1MdKlx",
      "used": true
    },
    {
      "name": "redis",
      "slug": "hxSlzNMtB2",
      "used": true
    },
    {
      "name": "geektime",
      "slug": "YAtQZtegtFw",
      "used": true
    },
    {
      "name": "note",
      "slug": "cwZ4pi0gPZa",
      "used": true
    },
    {
      "name": "Gridea",
      "slug": "GoYwNhP9tS",
      "used": false
    }
  ],
  "menus": [
    {
      "link": "/",
      "name": "首页",
      "openType": "Internal"
    },
    {
      "link": "/archives",
      "name": "归档",
      "openType": "Internal"
    },
    {
      "link": "/tags",
      "name": "标签",
      "openType": "Internal"
    },
    {
      "link": "/post/about",
      "name": "关于",
      "openType": "Internal"
    },
    {
      "link": "https://popgame.fun",
      "name": "POPGAME",
      "openType": "External"
    }
  ]
}
{"posts":[{"title":"Chrome 插件开发","content":"Manifest Chrome扩展的Manifest必须包含name、version和manifest_version属性，对于应用来说，还必须包含app属性。 Manifest.json模板： { &quot;app&quot;: { &quot;background&quot;: { &quot;scripts&quot;: [&quot;background.js&quot;] } }, &quot;manifest_version&quot;: 2, &quot;name&quot;: &quot;My Extension&quot;, &quot;version&quot;: &quot;versionString&quot;, &quot;default_locale&quot;: &quot;en&quot;, &quot;description&quot;: &quot;A plain text description&quot;, &quot;icons&quot;: { &quot;16&quot;: &quot;images/icon16.png&quot;, &quot;48&quot;: &quot;images/icon48.png&quot;, &quot;128&quot;: &quot;images/icon128.png&quot; }, &quot;browser_action&quot;: { &quot;default_icon&quot;: { &quot;19&quot;: &quot;images/icon19.png&quot;, &quot;38&quot;: &quot;images/icon38.png&quot; }, &quot;default_title&quot;: &quot;Extension Title&quot;, &quot;default_popup&quot;: &quot;popup.html&quot; }, &quot;page_action&quot;: { &quot;default_icon&quot;: { &quot;19&quot;: &quot;images/icon19.png&quot;, &quot;38&quot;: &quot;images/icon38.png&quot; }, &quot;default_title&quot;: &quot;Extension Title&quot;, &quot;default_popup&quot;: &quot;popup.html&quot; }, &quot;background&quot;: { &quot;scripts&quot;: [&quot;background.js&quot;] }, &quot;content_scripts&quot;: [ { &quot;matches&quot;: [&quot;http://www.google.com/*&quot;], &quot;css&quot;: [&quot;mystyles.css&quot;], &quot;js&quot;: [&quot;jquery.js&quot;, &quot;myscript.js&quot;] } ], &quot;options_page&quot;: &quot;options.html&quot;, &quot;permissions&quot;: [ &quot;*://www.google.com/*&quot; ], &quot;web_accessible_resources&quot;: [ &quot;images/*.png&quot; ] } 通过Chrome扩展我们可以对用户当前浏览的页面进行操作，实际上就是对用户当前浏览页面的DOM进行操作。通过Manifest中的content_scripts属性可以指定将哪些脚本何时注入到哪些页面中，当用户访问这些页面后，相应脚本即可自动运行，从而对页面DOM进行操作。 Manifest的content_scripts属性值为数组类型，数组的每个元素可以包含matches、exclude_matches、css、js、run_at、all_frames、include_globs和exclude_globs等属性。其中matches属性定义了哪些页面会被注入脚本，exclude_matches则定义了哪些页面不会被注入脚本，css和js对应要注入的样式表和JavaScript，run_at定义了何时进行注入，all_frames定义脚本是否会注入到嵌入式框架中，include_globs和exclude_globs则是全局URL匹配，最终脚本是否会被注入由matches、exclude_matches、include_globs和exclude_globs的值共同决定。简单的说，如果URL匹配mathces值的同时也匹配include_globs的值，会被注入；如果URL匹配exclude_matches的值或者匹配exclude_globs的值，则不会被注入。 跨域请求： URL 说明 是否允许请求 http://a.example.com/http://a.example.com/a.txt 同域下 允许 http://a.example.com/http://a.example.com/b/a.txt 同域下不同目录 允许 http://a.example.com/http://a.example.com:8080/a.txt 同域下不同端口 不允许 http://a.example.com/https://a.example.com/a.txt 同域下不同协议 不允许 http://a.example.com/http://b.example.com/a.txt 不同域下 不允许 http://a.example.com/http://a.foo.com/a.txt 不同域下 不允许 Google允许Chrome扩展应用不必受限于跨域限制。但出于安全考虑，需要在Manifest的permissions属性中声明需要跨域的权限。 { ... &quot;permissions&quot;: [ &quot;*://*.wikipedia.org/*&quot; ] } 常驻后台 在Manifest中指定background域可以使扩展常驻后台。background可以包含三种属性，分别是scripts、page和persistent。如果指定了scripts属性，则Chrome会在扩展启动时自动创建一个包含所有指定脚本的页面；如果指定了page属性，则Chrome会将指定的HTML文件作为后台页面运行。通常我们只需要使用scripts属性即可，除非在后台页面中需要构建特殊的HTML——但一般情况下后台页面的HTML我们是看不到的。persistent属性定义了常驻后台的方式——当其值为true时，表示扩展将一直在后台运行，无论其是否正在工作；当其值为false时，表示扩展在后台按需运行，这就是Chrome后来提出的Event Page。Event Page可以有效减小扩展对内存的消耗，如非必要，请将persistent设置为false。persistent的默认值为true。 如果想在用户打开浏览器之前就让扩展运行，可以在Manifest的permissions属性中加入&quot;background&quot;，但除非必要，否则尽量不要这么做，因为大部分用户不喜欢这样。 带选项页面的扩展 有一些扩展允许用户进行个性化设置，这样就需要向用户提供一个选项页面。Chrome通过Manifest文件的options_page属性为开发者提供了这样的接口，可以为扩展指定一个选项页面。当用户在扩展图标上点击右键，选择菜单中的“选项”后，就会打开这个页面。 扩展页面间的通信 有时需要让扩展中的多个页面之间，或者不同扩展的多个页面之间相互传输数据，以获得彼此的状态。比如音乐播放器扩展，当用户鼠标点击popup页面中的音乐列表时，popup页面应该将用户这个指令告知后台页面，之后后台页面开始播放相应的音乐。 Chrome提供了4个有关扩展页面间相互通信的接口，分别是runtime.sendMessage、runtime.onMessage、runtime.connect和runtime.onConnect。 Chrome提供的大部分API是不支持在content_scripts中运行的，但runtime.sendMessage和runtime.onMessage可以在content_scripts中运行，所以扩展的其他页面也可以同content_scripts相互通信。 chrome.runtime.sendMessage(extensionId, message, options, callback) : extensionId为所发送消息的目标扩展，如果不指定这个值，则默认为发起此消息的扩展本身，message为要发送的内容，options为对象类型，callback是回调函数。 chrome.runtime.onMessage.addListener(callback)：callback接收到的参数有三个，分别是message、sender和sendResponse，即消息内容、消息发送者相关信息和相应函数。 储存数据 localStorage localStorage是HTML5新增的方法，它允许JavaScript在用户计算机硬盘上永久储存数据（除非用户主动删除）。但localStorage也有一些限制，首先是localStorage和Cookies类似，都有域的限制，运行在不同域的JavaScript无法调用其他域localStorage的数据；其次是单个域在localStorage中存储数据的大小通常有限制（虽然W3C没有给出限制），对于Chrome这个限制是5MB(通过声明unlimitedStorage权限，Chrome扩展和应用可以突破这一限制)；最后localStorage只能储存字符串型的数据，无法保存数组和对象，但可以通过join、toString和JSON.stringify等方法先转换成字符串再储存。 Chrome存储API Chrome为扩展应用提供了存储API，以便将扩展中需要保存的数据写入本地磁盘。Chrome提供的存储API可以说是对localStorage的改进，它与localStorage相比有以下区别： 如果储存区域指定为sync，数据可以自动同步； content_scripts可以直接读取数据，而不必通过background页面； 在隐身模式下仍然可以读出之前存储的数据； 读写速度更快； 用户数据可以以对象的类型保存。 首先localStorage是基于域名的，这在前面的小节中已经提到过了。而content_scripts是注入到用户当前浏览页面中的，如果content_scripts直接读取localStorage，所读取到的数据是用户当前浏览页面所在域中的。所以通常的解决办法是content_scripts通过runtime.sendMessage和background通信，由background读写扩展所在域（通常是chrome-extension://extension-id/）的localStorage，然后再传递给content_scripts。 使用Chrome存储API必须要在Manifest的permissions中声明&quot;storage&quot;，之后才有权限调用。Chrome存储API提供了2种储存区域，分别是sync和local。两种储存区域的区别在于，sync储存的区域会根据用户当前在Chrome上登陆的Google账户自动同步数据，当无可用网络连接可用时，sync区域对数据的读写和local区域对数据的读写行为一致。 StorageArea = sync / local chrome.storage.StorageArea.get(keys, callback)：keys可以是字符串、包含多个字符串的数组或对象。如果keys是字符串，则和localStorage的用法类似；如果是数组，则相当于一次读取了多个数据；如果keys是对象，则会先读取以这个对象属性名为键值的数据，如果这个数据不存在则返回keys对象的属性值（比如keys为{'name':'Billy'}，如果name这个值存在，就返回name原有的值，如果不存在就返回Billy）。如果keys为一个空数组（[]）或空对象（{}），则返回一个空列表，如果keys为null，则返回所有存储的数据。 chrome.storage.StorageArea.getBytesInUse(keys, callback)：此处的keys只能为null、字符串或包含多个字符串的数组。 chrome.storage.StorageArea.set(items, callback)：items为对象类型，形式为键/值对。items的属性值如果是字符型、数字型和数组型，则储存的格式不会改变，但如果是对象型和函数型的，会被储存为“{}”，如果是日期型和正则型的，会被储存为它们的字符串形式。 chrome.storage.StorageArea.remove(keys, callback)：其中keys可以是字符串，也可以是包含多个字符串的数组。 chrome.storage.StorageArea.clear(callback)：删除所有数据。 Chrome同时还为存储API提供了一个onChanged事件，当存储区的数据发生改变时，这个事件会被激发。callback会接收到两个参数，第一个为changes，第二个是StorageArea。changes是词典对象，键为更改的属性名称，值包含两个属性，分别为oldValue和newValue；StorageArea为local或sync。 position属性还有另外的三个值，分别是absolute、relative和fixed。如果元素的位置属性为absolute，则它的位置是相对于除static定位以外的父系元素的，如果没有这样的父系元素，则相对于body；如果元素的位置属性为relative，则它的位置是相对于它默认在HTML流中位置的；如果元素的位置属性为fixed，则它的位置是相对于浏览器窗口的。 Browser Actions 图标 Browser Actions可以在Manifest中设定一个默认的图标 &quot;browser_action&quot;: { &quot;default_icon&quot;: { &quot;19&quot;: &quot;images/icon19.png&quot;, &quot;38&quot;: &quot;images/icon38.png&quot; } } 一般情况下，Chrome会选择使用19像素的图片显示在工具栏中，但如果用户正在使用视网膜屏幕的计算机，则会选择38像素的图片显示。两种尺寸的图片并不是必须都指定的，如果只指定一种尺寸的图片，在另外一种环境下，Chrome会试图拉伸图片去适应，这样可能会导致图标看上去很难看。另外，default_icon也不是必须指定的，如果没有指定，Chrome将使用一个默认图标。 通过setIcon方法可以动态更改扩展的图标:chrome.browserAction.setIcon(details, callback),其中details的类型为对象，可以包含三个属性，分别是imageData、path和tabId。 imageData是图片的像素数据，可以通过HTML的canvas标签获取到。 path的值可以是字符串，也可以是对象。如果是对象，结构为{size: imagePath}。imagePath为图片在扩展根目录下的相对位置。 tabId的值限定了浏览哪个标签页时，图标将被更改。 Popup页面 popup在关闭后，就相当于用户关闭了相应的标签页，这个页面不会继续运行。当用户再次打开这个页面时，所有的DOM和js空间变量都将被重新创建。 使用带有滚动条的DIV容器。 设计一个更好的滚动条样式。 考虑屏蔽右键菜单。 使用外部引用的脚本。 不要在popup页面的js空间变量中保存数据。 标题和badge 将鼠标移至扩展图标上，片刻后所显示的文字就是扩展的标题，在Manifest中，browser_action的default_title属性可以设置扩展的默认标题。还可以用JavaScript来动态更改扩展的标题：chrome.browserAction.setTitle({title: 'This is a new title'});。 Badge是扩展为用户提供有限信息的另外一种方法，这种方法较标题优越的地方是它可以一直显示，其缺点是只能显示大约4字节长度的信息。Badge目前只能够通过JavaScript设定显示的内容，同时Chrome还提供了更改badge背景的方法。如果不定义badge的背景颜色，默认将使用红色： chrome.browserAction.setBadgeBackgroundColor({color: '#0000FF'}); chrome.browserAction.setBadgeText({text: 'Dog'}); Content Scripts &quot;content_scripts&quot;: [ { //&quot;matches&quot;: [&quot;http://*/*&quot;, &quot;https://*/*&quot;], // &quot;&lt;all_urls&gt;&quot; 表示匹配所有地址 &quot;matches&quot;: [&quot;&lt;all_urls&gt;&quot;], // 多个JS按顺序注入 &quot;js&quot;: [&quot;js/jquery-1.8.3.js&quot;, &quot;js/content-script.js&quot;], // JS的注入可以随便一点，但是CSS的注意就要千万小心了，因为一不小心就可能影响全局样式 &quot;css&quot;: [&quot;css/custom.css&quot;], // 代码注入的时间，可选值： &quot;document_start&quot;, &quot;document_end&quot;, or &quot;document_idle&quot;，最后一个表示页面空闲时，默认document_idle &quot;run_at&quot;: &quot;document_start&quot; } ] 如果没有主动指定run_at为document_start（默认为document_idle），下面这种代码不会生效： document.addEventListener('DOMContentLoaded', function(){ console.log('我被执行了！'); }); content_scripts只能访问下面4种chrome api chrome.extension(getURL , inIncognitoContext , lastError , onRequest , sendRequest) chrome.i18n chrome.runtime(connect , getManifest , getURL , id , onConnect , onMessage , sendMessage) chrome.storage 通信 参考 chrome插件开发攻略 popup和background popup可以直接调用background中的JS方法，也可以直接访问background的DOM function test(){ alert('我是background！'); } // popup.js var bg = chrome.extension.getBackgroundPage(); bg.test(); background访问popup（popup是显示状态） var views = chrome.extension.getViews({type:'popup'}); if(views.length &gt; 0) { console.log(views[0].location.href); } popup和content popup和background其实几乎可以视为一种东西，因为它们可访问的API都一样、通信机制一样、都可以跨域。 background和content // background或popup function sendMessageToContentScript(message, callback){ chrome.tabs.query({active: true, currentWindow: true}, function(tabs) { chrome.tabs.sendMessage(tabs[0].id, message, function(response) { if(callback) callback(response); }); }); } sendMessageToContentScript({cmd:'test', value:'你好，我是popup！'}, function(response){ console.log('来自content的回复：'+response); }); // content-script.js 接收 chrome.runtime.onMessage.addListener(function(request, sender, sendResponse){ // console.log(sender.tab ?&quot;from a content script:&quot; + sender.tab.url :&quot;from the extension&quot;); if(request.cmd == 'test') alert(request.value); sendResponse('我收到了你的消息！'); }); content和background // content-script.js 发送 chrome.runtime.sendMessage({greeting: '你好，我是content-script呀，我主动发消息给后台！'}, function(response) { console.log('收到来自后台的回复：' + response); }); // background.js 或者 popup.js监听来自content-script的消息 chrome.runtime.onMessage.addListener(function(request, sender, sendResponse) { console.log('收到来自content-script的消息：'); console.log(request, sender, sendResponse); sendResponse('我是后台，我已收到你的消息：' + JSON.stringify(request)); }); content_scripts向popup主动发消息的前提是popup必须打开！否则需要利用background作中转； 如果background和popup同时监听，那么它们都可以同时收到消息，但是只有一个可以sendResponse，一个先发送了，那么另外一个再发送就无效； content和popup 同content和background injected script和content-script content-script和页面内的脚本（injected-script自然也属于页面内的脚本）之间唯一共享的东西就是页面的DOM元素，有2种方法可以实现二者通讯： 可以通过window.postMessage和window.addEventListener来实现二者消息通讯； 通过自定义DOM事件来实现； 方法一：（推荐） //injected-script window.postMessage({&quot;test&quot;: '你好！'}, '*'); //content script window.addEventListener(&quot;message&quot;, function(e){ console.log(e.data); }, false); 方法二： //injected-script var customEvent = document.createEvent('Event'); customEvent.initEvent('myCustomEvent', true, true); function fireCustomEvent(data) { hiddenDiv = document.getElementById('myCustomEventDiv'); hiddenDiv.innerText = data hiddenDiv.dispatchEvent(customEvent); } fireCustomEvent('你好，我是普通JS！'); //content script var hiddenDiv = document.getElementById('myCustomEventDiv'); if(!hiddenDiv) { hiddenDiv = document.createElement('div'); hiddenDiv.style.display = 'none'; document.body.appendChild(hiddenDiv); } hiddenDiv.addEventListener('myCustomEvent', function() { var eventData = document.getElementById('myCustomEventDiv').innerText; console.log('收到自定义事件消息：' + eventData); }); 长连接 短连接：chrome.tabs.sendMessage和chrome.runtime.sendMessage 长连接：chrome.tabs.connect和chrome.runtime.connect // popup.js getCurrentTabId((tabId) =&gt; { var port = chrome.tabs.connect(tabId, {name: 'test-connect'}); port.postMessage({question: '你是谁啊？'}); port.onMessage.addListener(function(msg) { alert('收到消息：'+msg.answer); if(msg.answer &amp;&amp; msg.answer.startsWith('我是')) { port.postMessage({question: '哦，原来是你啊！'}); } }); }); // content-script.js // 监听长连接 chrome.runtime.onConnect.addListener(function(port) { console.log(port); if(port.name == 'test-connect') { port.onMessage.addListener(function(msg) { console.log('收到长连接消息：', msg); if(msg.question == '你是谁啊？') port.postMessage({answer: '我是你爸！'}); }); } }); 动态注入 script 虽然在background和popup中无法直接访问页面DOM，但是可以通过chrome.tabs.executeScript来执行脚本，从而实现访问web页面的DOM（注意，这种方式也不能直接访问页面JS） { &quot;name&quot;: &quot;动态JS注入演示&quot;, ... &quot;permissions&quot;: [ &quot;tabs&quot;, &quot;http://*/*&quot;, &quot;https://*/*&quot; ], ... } // 动态执行JS代码 chrome.tabs.executeScript(tabId, {code: 'document.body.style.backgroundColor=&quot;red&quot;'}); // 动态执行JS文件 chrome.tabs.executeScript(tabId, {file: 'some-script.js'}); css { &quot;name&quot;: &quot;动态CSS注入演示&quot;, ... &quot;permissions&quot;: [ &quot;tabs&quot;, &quot;http://*/*&quot;, &quot;https://*/*&quot; ], ... } // 动态执行CSS代码，TODO，这里有待验证 chrome.tabs.insertCSS(tabId, {code: 'xxx'}); // 动态执行CSS文件 chrome.tabs.insertCSS(tabId, {file: 'some-style.css'}); 获取标签和窗口 // 获取当前窗口ID chrome.windows.getCurrent(function(currentWindow){ console.log('当前窗口ID：' + currentWindow.id); }); //获取当前标签页ID // 1 function getCurrentTabId(callback){ chrome.tabs.query({active: true, currentWindow: true}, function(tabs){ if(callback) callback(tabs.length ? tabs[0].id: null); }); } //2 function getCurrentTabId2(){ chrome.windows.getCurrent(function(currentWindow){ chrome.tabs.query({active: true, windowId: currentWindow.id}, function(tabs){ if(callback) callback(tabs.length ? tabs[0].id: null); }); }); } ##右键菜单 要将扩展加入到右键菜单中，首先要在Manifest的permissions域中声明contextMenus权限。 &quot;permissions&quot;: [ &quot;contextMenus&quot; ] 同时还要在icons域声明16像素尺寸的图标，这样在右键菜单中才会显示出扩展的图标。 &quot;icons&quot;: { &quot;16&quot;: &quot;icon16.png&quot; } Chrome提供了三种方法操作右键菜单，分别是create、update和remove，对应于创建、更新和移除操作。 通常create方法由后台页面来调用，即通过后台页面创建自定义菜单。如果后台页面是Event Page，通常在onInstalled事件中调用create方法。 右键菜单提供了4种类型，分别是普通菜单、复选菜单、单选菜单和分割线，其中普通菜单还可以有下级菜单。连续相邻的单选菜单会被自动认为是对同一设置的选项，同时单选菜单会自动在两端生成分割线。下面的代码生成了一系列的菜单： 我们还可以定义自定义的右键菜单在何时显示，比如当用户选择文本时，或者在超级链接上单击右键时。下面的代码定义当用户在超级链接上点击右键时，在菜单中显示“My Menu”菜单： chrome.contextMenus.create({ type: 'normal', title: 'My Menu', contexts: ['link'] }); contexts域的值是数组型的，也就是说我们可以定义多种情况下显示自定义菜单，完整的选项包括all、page、frame、selection、link、editable、image、video、audio和launcher，默认情况下为page，即在所有的页面唤出右键菜单时都显示自定义菜单。其中launcher只对Chrome应用有效，如果包含launcher选项，则当用户在chrome://apps/或者其他地方的应用图标点击右键，将显示相应的自定义菜单。需要注意的是，all选项不包括launcher。 update方法可以动态更改菜单属性，指定需要更改菜单的id和所需要更改的属性即可。remove方法可以删除指定的菜单，removeAll方法可以删除所有的菜单。 桌面提醒 要使用桌面提醒功能，需要在Manifest中声明notifications权限。 &quot;permissions&quot;: [ &quot;notifications&quot; ] 创建桌面提醒非常容易，只需指定标题、内容和图片即可。桌面系统窗口创建之后是不会立刻显示出来的，为了让其显示，还要调用show方法：notification.show(); 需要注意的是，对于要在桌面窗口中显示的图片，必须在Manifest的web_accessible_resources域中进行声明，否则会出现图片无法打开的情况: &quot;web_accessible_resources&quot;: [ &quot;icon48.png&quot; ] 如果希望images文件夹下的所有png图片都可被显示，可以通过如下声明实现： &quot;web_accessible_resources&quot;: [ &quot;images/*.png&quot; ] 桌面提醒窗口提供了四种事件：ondisplay、onerror、onclose和onclick。 桌面通知 Omnibox 要使用omnibox需要在Manifest的omnibox域指定keyword：&quot;omnibox&quot;: { &quot;keyword&quot; : &quot;hamster&quot; }。同时最好指定一个16像素的图标，当用户键入关键字后，这个图标会显示在地址栏的前端。&quot;icons&quot;: {&quot;16&quot;: &quot;icon16.png&quot;}。 Omnibox有四种事件：onInputStarted、onInputChanged、onInputEntered和onInputCancelled，分别用于监听用户开始输入、输入变化、执行指令和取消输入行为。其中执行指令是指用户敲击回车键或用鼠标点击建议结果。 书签 要在扩展中操作书签，需要在Manifest中声明bookmarks权限： &quot;permissions&quot;: [ &quot;bookmarks&quot; ] 书签对象有8个属性，分别是id、parentId、index、url、title、dateAdded、dateGroupModified和children。这8个属性并不是每个书签对象都具有的，比如书签分类，即一个文件夹，它就不具有url属性。index属性是这个书签在其父节点中的位置，它的值是从0开始的。children属性值是一个包含若干书签对象的数组。dateAdded和dateGroupModified的值是自1970年1月1日至修改时间所经过的毫秒数。只有id和title是书签对象必有的属性，其他的属性都是可选的。id不需要人为干预，它是由Chrome管理的。根的id为'0'。 '0'为根节点id，根节点下是不允许创建书签和书签分组的，它的下面默认只有三个书签分组：书签栏、其他书签和移动设备书签，如果创建时不指定parentId，则所创建的书签会默认加入到其他书签中。 如果创建的书签不包含url属性，则Chrome自动将其视作为书签分类。 通过move方法可以调整书签的位置，这种调整可以是跨越父节点的。 通过update方法可以更改书签属性，包括标题和URL，更新时未指定的属性值将不会更改。 通过remove和removeTree可以删除书签，remove方法可以删除书签和空的书签分组，removeTree可以删除包含书签的书签分组。 通过getTree方法可以获得用户完整的书签树。 search方法可以返回匹配指定条件的书签对象，匹配的条件只能字符串。 书签的事件：onCreated、onRemoved、onChanged、onChildrenReordered、onImportBegan、onImportEnded。 chrome.bookmarks.onCreated.addListener(function(bookmark){ console.log(bookmark); }); Cookies 要管理Cookies，需要在Manifest中声明cookies权限，同时也要声明所需管理Cookies所在的域： &quot;permissions&quot;: [ &quot;cookies&quot;, &quot;*://*.google.com&quot; ] 如果想要管理所有的Cookies可以声明如下权限： &quot;permissions&quot;: [ &quot;cookies&quot;, &quot;&lt;all_urls&gt;&quot; ] Chrome定义的Cookie对象包含如下属性：name（名称）、value（值）、domain（域）、hostOnly（是否只允许完全匹配domain的请求访问）、path（路径）、secure（是否只允许安全连接调用）、httpOnly（是否禁止客户端调用）、session（是否是session cookie）、expirationDate（过期时间）和storeId（包含此cookie的cookie store的id）。 Chrome提供了get和getAll两个方法读取Cookies，get方法可以读取指定name、url和storeId的Cookie，其中storeId可以不指定，但是name和url必须指定。如果在同一URL中包含多个name相同的Cookies，则会返回path最长的那个，如果有多个Cookies的path长度相同，则返回创建最早的那个。 如果cookie的secure属性值为true，那么通过get获取时url应该是https协议。 set方法可以设置Cookie： chrome.cookies.set({ 'url':'http://github.com/test_cookie', 'name':'TEST', 'value':'foo', 'secure':false, 'httpOnly':false }, function(cookie){ console.log(cookie); }); 历史 要使用history接口，需要在Manifest中声明history权限： &quot;permissions&quot;: [ &quot;history&quot; ] 管理历史的方法包括search、getVisits、addUrl、deleteUrl、deleteRange和deleteAll。其中search和getVisits用于读取历史，addUrl用于添加历史，deleteUrl、deleteRange和deleteAll用于删除历史。 管理扩展与应用 除了通过chrome://extensions/管理Chrome扩展和应用外，也可以通过Chrome的management接口管理。management接口可以获取用户已安装的扩展和应用信息，同时还可以卸载和禁用它们。通过management接口可以编写出智能管理扩展和应用的程序。 要使用management接口，需要在Manifest中声明management权限： &quot;permissions&quot;: [ &quot;management&quot; ] 读取用户已安装扩展和应用的信息。Management提供了两个方法获取用户已安装扩展应用的信息，分别是getAll和get。 exInfo是扩展信息对象，其结构如下： { id: 扩展id, name: 扩展名称, shortName: 扩展短名称, description: 扩展描述, version: 扩展版本, mayDisable: 是否可被用户卸载或禁用, enabled: 是否已启用, disabledReason: 扩展被禁用原因, type: 类型, appLaunchUrl: 启动url, homepageUrl: 主页url, updateUrl: 更新url, offlineEnabled: 离线是否可用, optionsUrl: 选项页面url, icons: [{ size: 图片尺寸, url: 图片URL }], permissions: 扩展权限, hostPermissions: 扩展有权限访问的host, installType: 扩展被安装的方式 } 标签 标签对象的结构： { id: 标签id, index: 标签在窗口中的位置，以0开始, windowId: 标签所在窗口的id, openerTabId: 打开此标签的标签id, highlighted: 是否被高亮显示, active: 是否是活动的, pinned: 是否被固定, url: 标签的URL, title: 标签的标题, favIconUrl: 标签favicon的URL, status :标签状态，loading或complete, incognito: 是否在隐身窗口中, width: 宽度, height: 高度, sessionId: 用于sessions API的唯一id } Chrome通过tabs方法提供了管理标签的方法与监听标签行为的事件，大多数方法与事件是无需声明特殊权限的，但有关标签的url、title和favIconUrl的操作（包括读取），都需要声明tabs权限。 &quot;permissions&quot;: [ &quot;tabs&quot; ] 获取标签信息。Chrome提供了三种获取标签信息的方法，分别是get、getCurrent和query。get方法可以获取到指定id的标签，getCurrent则获取运行的脚本本身所在的标签，query可以获取所有符合指定条件的标签。 chrome.tabs.get(tabId, function(tab){ console.log(tab); }); chrome.tabs.getCurrent(function(tab){ console.log(tab); }); query方法可以指定的匹配条件如下： { active: 是否是活动的, pinned: 是否被固定, highlighted: 是否正被高亮显示, currentWindow: 是否在当前窗口, lastFocusedWindow: 是否是上一次选中的窗口, status: 状态，loading或complete, title: 标题, url: 所打开的url, windowId: 所在窗口的id, windowType: 窗口类型，normal、popup、panel或app, index: 窗口中的位置 } 创建标签。创建标签与在浏览器中打开新的标签行为类似，但可以指定更加丰富的信息，如URL、窗口中的位置和活动状态等。 chrome.tabs.create({ windowId: wId, index: 0, url: 'http://www.google.com', active: true, pinned: false, openerTabId: tId }, function(tab){ console.log(tab); }); 获取指定窗口活动标签可见部分的截图。 chrome.tabs.captureVisibleTab(windowId, { format: 'jpeg', quality: 50 }, function(dataUrl){ window.open(dataUrl, 'tabCapture'); }); 其中format还支持png，如果指定为png，则quality属性会被忽略。如果指定jpeg格式，quality的取值范围为0-100，数值越高，图片质量越好，体积也越大。扩展只有声明activeTab或&lt;all_url&gt;权限能获取到活动标签的截图： &quot;permissions&quot;: [ &quot;activeTab&quot; ] content_scripts可以向匹配条件的页面注入JS和CSS，但是却无法向用户指定的标签注入。通过executeScript和insertCSS可以做到向指定的标签注入脚本。 chrome.tabs.executeScript(tabId, { file: 'js/ex.js', allFrames: true, runAt: 'document_start' }, function(resultArray){ console.log(resultArray); }); 也可以直接注入代码： chrome.tabs.executeScript(tabId, { code: 'document.body.style.backgroundColor=&quot;red&quot;', allFrames: true, runAt: 'document_start' }, function(resultArray){ console.log(resultArray); }); 向指定的标签注入CSS： chrome.tabs.insertCSS(tabId, { file: 'css/insert.css', allFrames: false, runAt: 'document_start' }, function(){ console.log('The css has been inserted.'); }); executeScript和insertCSS方法中runAt的值可以是'document_start'、'document_end'或'document_idle'。 与指定标签中的内容脚本（content script）通信。前面章节介绍过扩展页面间的通信，我们也可以与指定的标签通信，方法如下： chrome.tabs.sendMessage(tabId, message, function(response){ console.log(response); }); 后台页面主动与content_scripts通信需要使用chrome.tabs.sendMessage方法 监控标签行为的事件包含onCreated、onUpdated、onMoved、onActivated、onHighlighted、onDetached、onAttached、onRemoved和onReplaced。 Override Pages 目前支持替换的页面包含Chrome的书签页面、历史记录和新标签页面。 只需在Manifest中进行声明即可（一个扩展只能替换一个页面）： &quot;chrome_url_overrides&quot; : { &quot;bookmarks&quot;: &quot;bookmarks.html&quot; } &quot;chrome_url_overrides&quot; : { &quot;history&quot;: &quot;history.html&quot; } &quot;chrome_url_overrides&quot; : { &quot;newtab&quot;: &quot;newtab.html&quot; } 下载 Chrome提供了downloads API，扩展可以通过此API管理浏览器的下载功能，包括暂停、搜索和取消等。扩展使用downloads接口需要在Manifest文件中声明downloads权限： &quot;permissions&quot;: [ &quot;downloads&quot; ] chrome.downloads.download(options, callback);其中options的完整结构如下： { url: 下载文件的url, filename: 保存的文件名, conflictAction: 重名文件的处理方式, saveAs: 是否弹出另存为窗口, method: 请求方式（POST或GET）， headers: 自定义header数组, body: POST的数据 } 网络请求 要对网络请求进行操作，需要在Manifest中声明webRequest权限以及相关被操作的URL。如需要阻止网络请求，需要声明webRequestBlocking权限。 &quot;permissions&quot;: [ &quot;webRequest&quot;, &quot;webRequestBlocking&quot;, &quot;*://*.google.com/&quot; ] chrome.webRequest.onBeforeRequest.addListener( function(details){ return {cancel: true}; }, { urls: [ &quot;*://bad.example.com/*&quot; ] }, [ &quot;blocking&quot; ] ); header中的如下属性是不支持更改的：Authorization、Cache-Control、Connection、Content-Length、Host、If-Modified-Since、If-None-Match、If-Range、Partial-Data、Pragma、Proxy-Authorization、Proxy-Connection和Transfer-Encoding。 所有事件中，回调函数所接收到的信息对象均包括如下属性：requestId、url、method、frameId、parentFrameId、tabId、type和timeStamp。其中type可能的值包括&quot;main_frame&quot;、&quot;sub_frame&quot;、&quot;stylesheet&quot;、&quot;script&quot;、&quot;image&quot;、&quot;object&quot;、&quot;xmlhttprequest&quot;和&quot;other&quot;。 代理 Chrome浏览器提供了代理设置管理接口，这样可以让扩展来做到更加智能的代理设置。要让扩展使用代理接口，需要声明proxy权限： &quot;permissions&quot;: [ &quot;proxy&quot; ] 通过chrome.proxy.settings.set方法可以设置代理服务器，该方法需要两个参数，一个是代理设置对象，另一个是回调函数。 代理设置对象包括mode属性、rules属性和pacScript属性。其中mode属性为代理模式，可选的值有'direct'（直接连接，即不通过代理）、'auto_detect'（通过WPAD协议自动获取pac脚本）、'pac_script'（使用指定的pac脚本）、'fixed_servers'（固定的代理服务器）和'system'（使用系统的设置）。rules属性和pacScript属性都是可选的，rules指定了不同的协议通过不同的代理。 API chrome.alarms.* 通过此制定周期性任务，需在manifest.json文件中声明权限 &quot;permissions&quot;: [ &quot;alarms&quot; ], chrome.alarms.create(string name, object alarmInfo) 属性 类型 注释 when double 触发alarm时间，ms delayInMinutes double 延迟时间，minute periodInMinutes double 周期性时间间隔，minute 获取指定名字的alarm chrome.alarms.get(string name, function(Alarm alarm) {...}) 获取所有alarm chrome.alarms.getAll(function(array of Alarm alarms) {...}) 通过名字删除alarm chrome.alarms.clear(string name, function(boolean wasCleared) {...}) 清除所有alarm chrome.alarms.clearAll(function(boolean wasCleared) {...}) 监听alarm发生的事件，用于event page chrome.alarms.onAlarm.addListener(function(Alarm alarm) {...}) ","link":"https://xuhang.github.io/post/chrome-cha-jian-kai-fa/"},{"title":"爬虫","content":"〇、从入门到入狱 中国爬虫违法违规案例汇总 一、什么是爬虫 二、爬虫的分类 搜索引擎：百度、谷歌 数据采集：天眼查、企查查 薅羊毛：抢票机器人、秒杀软件，比价软件，微博僵尸粉 …… 三、爬虫与反爬虫 君子协议：robots.txt www.baidu.com/robots.txt ![image-20220303093748092] 最简单的爬虫 Python版 import requests rsp = requests.get('http://www.httpbin.org/user-agent') Java版 @Test public void testHttpclient() throws IOException { CloseableHttpClient client = HttpClientBuilder.create().build(); HttpGet get = new HttpGet(&quot;http://www.httpbin.org/user-agent&quot;); CloseableHttpResponse response = client.execute(get); HttpEntity entity = response.getEntity(); String string = EntityUtils.toString(entity); System.out.println(string); } @Test public void testHtmlUnit() throws IOException { WebClient edge = new WebClient(BrowserVersion.FIREFOX); edge.getOptions().setCssEnabled(false); edge.getOptions().setJavaScriptEnabled(true); edge.getOptions().setThrowExceptionOnFailingStatusCode(false); edge.getOptions().setThrowExceptionOnScriptError(false); edge.waitForBackgroundJavaScript(600*1000); UnexpectedPage page = edge.getPage(&quot;http://www.httpbin.org/user-agent&quot;); System.out.println(page.getWebResponse().getContentAsString()); } 加入依赖 &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.5.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.sourceforge.htmlunit&lt;/groupId&gt; &lt;artifactId&gt;htmlunit&lt;/artifactId&gt; &lt;version&gt;2.45.0&lt;/version&gt; &lt;/dependency&gt; 但是这样的爬虫很容易通过检测UA头被发现，服务器就可以对这样的爬虫做出反爬的措施。 修改爬虫的UA Python版 header = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36' } rsp = requests.get('http://www.httpbin.org/user-agent', headers = header) Java版 @Test public void testHttpclient() throws IOException { CloseableHttpClient client = HttpClientBuilder.create().build(); HttpGet get = new HttpGet(&quot;http://www.httpbin.org/user-agent&quot;); get.setHeader(&quot;User-Agent&quot;, &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36&quot;); CloseableHttpResponse response = client.execute(get); HttpEntity entity = response.getEntity(); String string = EntityUtils.toString(entity); System.out.println(string); } 控制爬虫的频率 修改UA只是第一步，服务器还会针对每个ip地址的请求频率来识别爬虫，比如一分钟内请求几百几千次，一天24小时不间断的请求，这些特征都可以被识别为爬虫程序。所以在大规模抓取数据时，需要对降低抓取频率，比如每次请求后sleep 3~5 秒；但是这样会大大降低抓取的效率，所以这里就需要用到代理IP池——当然也可以通过部署集群的方式来提高爬的速度 代理服务器会转发爬虫请求，这样服务器针对IP的限制就会被绕过。 proxy = { 'http': '', 'https': '' } header = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36' } rsp = requests.get('http://www.httpbin.org/user-agent', headers = header, proxies = proxy) 代理IP分透明代理，匿名代理和高匿代理，透明代理其实并不会隐藏爬虫的真实IP，匿名代理会在请求头里带上爬虫的IP，有一定几率会被识别，高匿代理则会完全隐藏爬虫的IP，更推荐使用。 据说现在互联网上50%以上的流量都是由爬虫产生的，针对一些热门资源这一比例可以高达98%以上。针对这种情况，服务器会需要用户登录才能访问，而简单的登录验证就是将浏览器上的cookie和服务器session绑定起来。 验证码（Completely Automated Public Turing test to tell Computers and Humans Apart，简称CAPTCHA） 对于简单的字母和数字组成的验证码，可以通过自己训练模型来识别，或者使用第三方的打码平台来验证。 def bypassWithDama(): rec_url = &quot;http://pred.fateadm.com&quot; tm = str(int(time.time())) sign = CalcSign(pd_id, pd_key, tm) asign = CalcSign(app_id, app_key, tm) param = { &quot;user_id&quot;: pd_id, &quot;timestamp&quot;: tm, &quot;appid&quot;: app_id, &quot;sign&quot;:sign, &quot;asign&quot;: asign, &quot;predict_type&quot;: &quot;30600&quot;, &quot;up_type&quot;: &quot;mt&quot; } url = rec_url + &quot;/api/capreg&quot; img_data = open('/Users/xuhang/Desktop/captcha.jpeg', 'rb') files = { &quot;img_data&quot;: ('img_data', img_data) } header = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36' } rsp_data = requests.post(url, param, files=files, headers=header) print('【斐斐打码】' + rsp_data.text) return rsp_data.json() // 【斐斐打码】{&quot;RetCode&quot;:&quot;0&quot;,&quot;ErrMsg&quot;:&quot;&quot;,&quot;RequestId&quot;:&quot;20220303110050322d81ed0007a58386&quot;,&quot;RspData&quot;:&quot;{\\&quot;result\\&quot;: \\&quot;yfx5\\&quot;}&quot;} 对于像Google的CAPTCHA或者 Intuition的hCaptcha或者arkoselabs的FunCaptcha，这类复杂的验证码，需要识别图片中的物品并点击符合要求的图片，或者将图片旋转到正确的角度，可以使用打码平台的人工打码，由人工完成后将结果返回。 带上登录后的cookie 对于一般安全性不强的网站，并没有针对登录验证做太多的设计，所以爬虫很容易就能实现带cookie访问。 header = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36' } url = 'https://store.steampowered.com/account/' rsp = requests.get(url, headers=header, verify=False) 将响应的文本复制到文件中保存为html格式，然后用浏览器打开，虽然是乱码，但是可以看到Login的按钮，说明是未登录的状态。登录steam之后将请求中的cookie复制出来，修改header后再次请求 header = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36', 'Cookie': '...' } 可以从页面中找到该账号绑定的邮箱和余额等信息，说明登录成功。虽然可以爬到有限制的信息，但是爬虫和账号绑定了，这里代理就不起作用了，频繁爬取的话可能会导致被封号，这里就需要准备多个账号了，好在爬虫可以代替人工申请账号，只是成本会高一些。 虽然上面的操作或多或少可以骗过服务器的检测，但是相比与真实的浏览器，爬虫程序还是会在很多方面存在差异。比如真是浏览器通常会有document，setInterval等对象，爬虫程序缺少这些对象可能会导致某些关键性步骤无法执行，从而被服务器识别出来。另外有些网站需要post提交请求数据，但是某些必需的参数又是经过各种复杂加密混淆之后的结果，有时候可能追踪一个参数忙碌了好几天，成功的爬取了一天，第二天网站改参数了。针对这种情况，我们可以使用真实的浏览器的爬取。 如果是个人网站的站长，通常没有过多的精力来对抗爬虫，简单点的方法就是上面的随机生成验证码，然后加点干扰线。或者接入第三方的验证码平台，比如极验。但是验证码通常只用在有敏感操作的地方，不可能每个请求都要验证码，这时就可以使用网页防火墙之类的服务，通过检查客户端的引擎和一些特征来识别是不是爬虫，这样就可以拦截大部分的爬虫了。 该防火墙最开始是通过浏览器的引擎进行一些计算任务，只有计算正确才能成功跳转，但是已经有人成功用python模拟了计算过程，目前使用的是hCaptcha的验证码。 headless浏览器 PhantomJS是一个可以执行javascript脚本的无头网页浏览器，由于Chrome浏览器在17年开始支持无头模式，PhantomJS的作者已经停止维护了，推荐大家去使用Chrome。 下面是PhantomJS的一个demo var page = require('webpage').create(); page.open('http://www.google.com', function() { setTimeout(function() { page.render('google.png'); phantom.exit(); }, 200); }); 这里推荐使用Selenium，这个工具可以驱动Chrome、Firefox、IE、Safari、Opera和PhantomJS，并且提供多种语言的版本，只需要安装相应的浏览器并指定浏览器的驱动路径即可。 # -*- coding: utf-8 -*- from selenium import webdriver import time from selenium.webdriver.common.keys import Keys from selenium.webdriver import ActionChains &quot;&quot;&quot; 如果控制台出现乱码，尝试修改编码格式：chcp &lt;编码&gt; 65001 UTF-8 950 繁体中文 936 GBK 437 MS-DOS &quot;&quot;&quot; options = webdriver.ChromeOptions() # 以Headless模式启动 # options.add_argument('headless') # 窗口大小（通过截图可以反映窗口大小） options.add_argument('window-size=1200x600') # 设置User-Agent options.add_argument('user-agent=Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36') # 创建实例 browser = webdriver.Chrome(chrome_options = options, executable_path='/Users/xuhang/Downloads/chromedriver') # 窗口最大化 browser.maximize_window() # Part Ⅰ # 请求网址 browser.get(url = &quot;https://www.smzdm.com&quot;) time.sleep(2) # 截图 browser.save_screenshot('smzdm_1.png') # 下拉滑动页面 - 通过JS脚本 browser.execute_script(&quot;window.scrollTo(10000, document.body.scrollHeight)&quot;) # 下拉滑动页面 - 通过模拟鼠标移动 # from selenium.webdriver.support.wait import WebDriverWait # WebDriverWait(browser, 20, 0.5).until(lambda x: x.find_element_by_id('feed-main-list')) # actionChain = browser.find_element_by_css_selector(&quot;#feed-main-list li:last-child&quot;) # ActionChains(browser).move_to_element(actionChain).perform() browser.save_screenshot('smzdm_2.png') # 新开窗口打开百度首页 browser.execute_script(&quot;window.open('https://www.baidu.com')&quot;) handlers = browser.window_handles # print(handlers) # 切换到第2个窗口 browser.switch_to_window(handlers[1]) # 在第2个窗口操作 browser.find_element_by_id('kw').send_keys('selenium') browser.find_element_by_id('su').click() # 获取cookies cookies = browser.get_cookies() cookie = browser.get_cookie('BAIDUID') time.sleep(2) browser.save_screenshot('baidu_1.png') browser.switch_to_window(handlers[0]) time.sleep(2) print(cookie) browser.quit() 到这里，配合上代理IP和账号登陆已经可以绕开大部分的网站验证了，但是即使用上了真实的浏览器，Selenium操控的浏览器还是会暴露出一部分特征。比如window.navigator.webdriver属性，这个属性是正常浏览器没有的，但是Chrome Headless里有，虽然可以通过参数关闭该属性，但是仍有其他属性会暴露出来。 除了以上用到的Selenium，还有Pupperteer和Python版本的Pypperteer，器中Pupperteer是Google官方推出的基于Chrome DevTool protocol 协议的Nodejs包，通常在Selenium失败之后尝试使用Pupperteer，还是不行可以考虑开发Chrome的插件来爬，因为Chrome插件是运行在真正的浏览器上面，和平时使用的一样，它还能使用浏览器以往的缓存，不容易被识别出来。 四、扩展 反爬虫除了在服务端对可疑请求进行拦截，还可以在客户端增加爬虫的开发难度，其中就包括代码混淆、干扰调试、数据投毒、图片替换数据、字体乱序。 代码混淆 base62加密最明显的特征是以eval(function(p,a,c,k,e,r))开头，这样加密后的代码没有可读性，对于不熟悉此加密的人有一定难度，但是由于此方法最终要执行eval方法，所以只需要通过console.log将内容打印出来就是加密前的代码。 干扰调试 由于爬虫必须通过找出数据接口才能进行数据抓取，对于浏览器最基础的操作就是打开DevTools来分析请求和数据，所以一旦发现用户打开DevTools就可以做一些干扰来增加难度。比如debugger;本来是开发人员用来在调试代码时使用的命令，该命令会强制在此处打断点，所以可以对爬虫的开发人员进行一定的干扰。但是此方法也可以通过浏览器的停用断点使其失效。类似的方法还有检测到DevTools后立即删除所有关键的信息，这样也就不会暴露数据接口了。（微信读书用到了此方法） 数据投毒 此方法会有一定几率误伤到正常用户，所以使用得很少，而且使用也必须很谨慎。如果后台在检测到是爬虫之后，将原本正确的数据替换掉，让爬虫拿到的是毫无意义的数据，这样爬虫方就会因为这些异常数据做出错误的策略。 图片替换 因为爬虫主要抓取的是文本类型的数据，比如价格、邮箱等，而爬虫处理文本数据的成本是很低的。如果将关键的数据用图片进行替换，图片上展示的是正常的数据，这样不会对正常访问的用户造成影响，只会增加爬虫获取数据的难度。 字体乱序 这种方法和数据投毒的效果类似，但是不会误伤正常访问的用户。具体操作是在页面上加载乱序过的字体文件，但是乱序的规则后台是知道的，后台在返回数据的时候只要根据乱序的规则做一次反向的替换，就能让html的源数据和页面上展示的不一致，而爬虫是根据html源里的数据来进行处理的。 在线字体编辑器-JSON在线编辑器 (qqe2.com) 这个字体文件里，数字7和1交换了顺序（为演示方便，只乱序了2个数字，通常是越乱越好），在页面上定义这个字体并指定文件路径，然后使用定义的字体。 源文件里的数字是1234567890，页面上展示的是7234561890。 对于中文也是类似的，只不过中文字符过多，乱序之后映射关系复杂，会让维护变得困难。 ","link":"https://xuhang.github.io/post/pa-chong/"},{"title":"设计模式和开发原则","content":"软件开发的7条原则 开闭原则 开闭原则（Open Closed Principle，OCP）由勃兰特·梅耶（Bertrand Meyer）提出，他在 1988 年的著作《面向对象软件构造》（Object Oriented Software Construction）中提出：软件实体应当对扩展开放，对修改关闭（Software entities should be open for extension，but closed for modification），这就是开闭原则的经典定义。 当应用的需求改变时，在不修改软件实体的源代码或者二进制代码的前提下，可以扩展模块的功能，使其满足新的需求。 软件实体包括以下几个部分：项目中划分出的模块、类与接口、方法。 闭原则是面向对象程序设计的终极目标，它使软件实体拥有一定的适应性和灵活性的同时具备稳定性和延续性。具体来说，其作用如下： 对软件测试的影响 软件遵守开闭原则的话，软件测试时只需要对扩展的代码进行测试就可以了，因为原有的测试代码仍然能够正常运行。 可以提高代码的可复用性 粒度越小，被复用的可能性就越大；在面向对象的程序设计中，根据原子和抽象编程可以提高代码的可复用性。 可以提高软件的可维护性 遵守开闭原则的软件，其稳定性高和延续性强，从而易于扩展和维护。 里氏替换原则 里氏替换原则（Liskov Substitution Principle，LSP）由麻省理工学院计算机科学实验室的里斯科夫（Liskov）女士在 1987 年的“面向对象技术的高峰会议”（OOPSLA）上发表的一篇文章《数据抽象和层次》（Data Abstraction and Hierarchy）里提出来的，她提出：继承必须确保超类所拥有的性质在子类中仍然成立（Inheritance should ensure that any property proved about supertype objects also holds for subtype objects）。 里氏替换原是继承复用的基础，它反映了基类与子类之间的关系，是对开闭原则的补充，是对实现抽象化的具体步骤的规范。 子类可以扩展父类的功能，但不能改变父类原有的功能。 具体体现在以下几点： 子类可以实现父类的抽象方法，但不能覆盖父类的非抽象方法 子类中可以增加自己特有的方法 当子类的方法重载父类的方法时，方法的前置条件（即方法的输入参数）要比父类的方法更宽松 当子类的方法实现父类的方法时（重写/重载或实现抽象方法），方法的后置条件（即方法的的输出/返回值）要比父类的方法更严格或相等 里氏替换原则的主要作用如下： 里氏替换原则是实现开闭原则的重要方式之一。 它克服了继承中重写父类造成的可复用性变差的缺点。 它是动作正确性的保证。即类的扩展不会给已有的系统引入新的错误，降低了代码出错的可能性。 加强程序的健壮性，同时变更时可以做到非常好的兼容性，提高程序的维护性、可扩展性，降低需求变更时引入的风险。 依赖倒置原则 依赖倒置原则（Dependence Inversion Principle，DIP）是 Object Mentor 公司总裁罗伯特·马丁（Robert C.Martin）于 1996 年在 C++ Report 上发表的文章。依赖倒置原则的原始定义为：高层模块不应该依赖低层模块，两者都应该依赖其抽象；抽象不应该依赖细节，细节应该依赖抽象（High level modules shouldnot depend upon low level modules.Both should depend upon abstractions.Abstractions should not depend upon details. Details should depend upon abstractions）。其核心思想是：要面向接口编程，不要面向实现编程。 依赖倒置原则是实现开闭原则的重要途径之一，它降低了客户与实现模块之间的耦合。 开发中需要遵循以下几点： 每个类尽量提供接口或抽象类，或者两者都具备。 变量的声明类型尽量是接口或者是抽象类。 任何类都不应该从具体类派生。 使用继承时尽量遵循里氏替换原则。 依赖倒置原则的主要作用如下： 依赖倒置原则可以降低类间的耦合性。 依赖倒置原则可以提高系统的稳定性。 依赖倒置原则可以减少并行开发引起的风险。 依赖倒置原则可以提高代码的可读性和可维护性。 单一指责原则 单一职责原则（Single Responsibility Principle，SRP）又称单一功能原则，由罗伯特·C.马丁（Robert C. Martin）于《敏捷软件开发：原则、模式和实践》一书中提出的。这里的职责是指类变化的原因，单一职责原则规定一个类应该有且仅有一个引起它变化的原因，否则类应该被拆分（There should never be more than one reason for a class to change）。 单一职责原则将有以下优点。 降低类的复杂度。一个类只负责一项职责，其逻辑肯定要比负责多项职责简单得多。 提高类的可读性。复杂性降低，自然其可读性会提高。 提高系统的可维护性。可读性提高，那自然更容易维护了。 变更引起的风险降低。变更是必然的，如果单一职责原则遵守得好，当修改一个功能时，可以显著降低对其他功能的影响。 接口隔离原则 接口隔离原则（Interface Segregation Principle，ISP）要求程序员尽量将臃肿庞大的接口拆分成更小的和更具体的接口，让接口中只包含客户感兴趣的方法。 接口隔离原则应做到以下几点： 接口尽量小，但是要有限度。一个接口只服务于一个子模块或业务逻辑。 为依赖接口的类定制服务。只提供调用者需要的方法，屏蔽不需要的方法。 了解环境，拒绝盲从。每个项目或产品都有选定的环境因素，环境不同，接口拆分的标准就不同深入了解业务逻辑。 提高内聚，减少对外交互。使接口用最少的方法去完成最多的事情。 遵循接口隔离原则有以下 5 个优点。 将臃肿庞大的接口分解为多个粒度小的接口，可以预防外来变更的扩散，提高系统的灵活性和可维护性。 接口隔离提高了系统的内聚性，减少了对外交互，降低了系统的耦合性。 如果接口的粒度大小定义合理，能够保证系统的稳定性；但是，如果定义过小，则会造成接口数量过多，使设计复杂化；如果定义太大，灵活性降低，无法提供定制服务，给整体项目带来无法预料的风险。 使用多个专门的接口还能够体现对象的层次，因为可以通过接口的继承，实现对总接口的定义。 能减少项目工程中的代码冗余。过大的大接口里面通常放置许多不用的方法，当实现这个接口的时候，被迫设计冗余的代码。 迪米特法则 迪米特法则（Law of Demeter，LoD）又叫作最少知识原则（Least Knowledge Principle，LKP)，产生于 1987 年美国东北大学（Northeastern University）的一个名为迪米特（Demeter）的研究项目，由伊恩·荷兰（Ian Holland）提出，被 UML 创始者之一的布奇（Booch）普及，后来又因为在经典著作《程序员修炼之道》（The Pragmatic Programmer）提及而广为人知。迪米特法则的定义是：只与你的直接朋友交谈，不跟“陌生人”说话（Talk only to your immediate friends and not to strangers）。其含义是：如果两个软件实体无须直接通信，那么就不应当发生直接的相互调用，可以通过第三方转发该调用。其目的是降低类之间的耦合度，提高模块的相对独立性。 迪米特法则时要注意以下 6 点。 在类的划分上，应该创建弱耦合的类。类与类之间的耦合越弱，就越有利于实现可复用的目标。 在类的结构设计上，尽量降低类成员的访问权限。 在类的设计上，优先考虑将一个类设置成不变类。 在对其他类的引用上，将引用其他对象的次数降到最低。 不暴露类的属性成员，而应该提供相应的访问器（set 和 get 方法）。 谨慎使用序列化（Serializable）功能。 迪米特法则有以下两个优点。 降低了类之间的耦合度，提高了模块的相对独立性。 由于亲合度降低，从而提高了类的可复用率和系统的扩展性。 合成复用原则 合成复用原则（Composite Reuse Principle，CRP）又叫组合/聚合复用原则（Composition/Aggregate Reuse Principle，CARP）。它要求在软件复用时，要尽量先使用组合或者聚合等关联关系来实现，其次才考虑使用继承关系来实现。如果要使用继承关系，则必须严格遵循里氏替换原则。 通常类的复用分为继承复用和合成复用两种，继承复用虽然有简单和易实现的优点，但它也存在以下缺点。 继承复用破坏了类的封装性。因为继承会将父类的实现细节暴露给子类，父类对子类是透明的，所以这种复用又称为“白箱”复用。 子类与父类的耦合度高。父类的实现的任何改变都会导致子类的实现发生变化，这不利于类的扩展与维护。 它限制了复用的灵活性。从父类继承而来的实现是静态的，在编译时已经定义，所以在运行时不可能发生变化。 采用组合或聚合复用时，可以将已有对象纳入新对象中，使之成为新对象的一部分，新对象可以调用已有对象的功能，它有以下优点。 它维持了类的封装性。因为成分对象的内部细节是新对象看不见的，所以这种复用又称为“黑箱”复用。 新旧类之间的耦合度低。这种复用所需的依赖较少，新对象存取成分对象的唯一方法是通过成分对象的接口。 复用的灵活性高。这种复用可以在运行时动态进行，新对象可以动态地引用与成分对象类型相同的对象。 七大原则总结 设计原则 一句话归纳 目的 开闭原则 对扩展开放，对修改关闭 降低维护带来的新风险 依赖倒置原则 高层不应该依赖低层，要面向接口编程 更利于代码结构的升级扩展 单一职责原则 一个类只干一件事，实现类要单一 便于理解，提高代码的可读性 接口隔离原则 一个接口只干一件事，接口要精简单一 功能解耦，高聚合、低耦合 迪米特法则 不该知道的不要知道，一个类应该保持对其它对象最少的了解，降低耦合度 只和朋友交流，不和陌生人说话，减少代码臃肿 里氏替换原则 不要破坏继承体系，子类重写方法功能发生改变，不应该影响父类方法的含义 防止继承泛滥 合成复用原则 尽量使用组合或者聚合关系实现代码复用，少使用继承 降低代码耦合 设计模式分类概述 设计模式的本质是面向对象设计原则的实际运用，是对类的封装性、继承性和多态性以及类的关联关系和组合关系的充分理解。 可以提高程序员的思维能力、编程能力和设计能力。 使程序设计更加标准化、代码编制更加工程化，使软件开发效率大大提高，从而缩短软件的开发周期。 使设计的代码可重用性高、可读性强、可靠性高、灵活性好、可维护性强。 创建型模式：用于描述“怎样创建对象”，它的主要特点是“将对象的创建与使用分离”。 结构型模式：用于描述如何将类或对象按某种布局组成更大的结构。 行为型模式：用于描述类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，以及怎样分配职责。 设计模式总结 模式分类 模式名称 一句话总结 创建型模式 工厂方法 简单工厂（静态工厂）模式有一个具体的工厂类，可以生成多个不同的产品。工厂方法模式有多个工厂实现类，每个实现类生成具体的产品。 : 抽象工厂 为访问类提供一个创建一组相关或相互依赖对象的接口，且访问类无须指定所要产品的具体类就能得到同族的不同等级的产品的模式结构。 : 建造者 将一个复杂对象的构造与它的表示分离，使同样的构建过程可以创建不同的表示。 : 原型 用一个已经创建的实例作为原型，通过复制该原型对象来创建一个和原型相同或相似的新对象。（clone） : 单例 指一个类只有一个实例，且该类能自行创建这个实例的一种模式。 结构型模式 适配器 将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 : 桥接 将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现的，从而降低了抽象和实现这两个可变维度的耦合度。 : 组合 将对象组合成树状层次结构，使用户对单个对象和组合对象具有一致的访问性。 : 装饰 动态地给对象增加一些职责，即增加其额外的功能。 : 外观 为多个复杂的子系统提供一个一致的接口，使这些子系统更加容易被访问。 : 享元 运用共享技术来有效地支持大量细粒度对象的复用。 : 代理 为某对象提供一种代理以控制对该对象的访问。即客户端通过代理间接地访问该对象，从而限制、增强或修改该对象的一些特性。 行为模式 责任链 把请求从链中的一个对象传到下一个对象，直到请求被响应为止。通过这种方式去除对象之间的耦合。 : 命令 将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。 : 迭代器 提供一种方法来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 : 中介者 定义一个中介对象来简化原有对象之间的交互关系，降低系统中对象间的耦合度，使原有对象之间不必相互了解。 : 备忘录 在不破坏封装性的前提下，获取并保存一个对象的内部状态，以便以后恢复它。 : 观察者 多个对象间存在一对多关系，当一个对象发生改变时，把这种改变通知给其他多个对象，从而影响其他对象的行为。 : 状态 允许一个对象在其内部状态发生改变时改变其行为能力。 : 策略 定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的改变不会影响使用算法的客户。 : 模版方法 定义一个操作中的算法骨架，将算法的一些步骤延迟到子类中，使得子类在可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 : 访问者 在不改变集合元素的前提下，为一个集合中的每个元素提供多种访问方式，即每个元素有多个访问者对象访问。 : 解释器 提供如何定义语言的文法，以及对语言句子的解释方法，即解释器。 ","link":"https://xuhang.github.io/post/she-ji-mo-shi-he-kai-fa-yuan-ze/"},{"title":"Kafka 核心技术与实战笔记","content":"术语 发布订阅的对象是主题（Topic），向主题发布消息的客户端应用程序称为生产者（Producer），订阅主题消息的客户端应用程序称为消费者（Consumer），生产者和消费者统称为客户端（Clients）。 Kafka 的服务器端由被称为 Broker 的服务进程构成，即一个 Kafka 集群由多个 Broker 组成，Broker 负责接收和处理客户端发送过来的请求，以及对消息进行持久化。虽然多个Broker可以同时运行在一台机器上，但是通常将不同的Broker分散在不同机器上，以保证高可用。 实现高可用的另一个手段就是备份机制（Replication）。备份的思想很简单，就是把相同的数据拷贝到多台机器上，而这些相同的数据拷贝在 Kafka 中被称为副本（Replica）。副本的数量是可以配置的，这些副本保存着相同的数据，但却有不同的角色和作用。Kafka 定义了两类副本：领导者副本（Leader Replica）和追随者副本（Follower Replica）。前者对外提供服务，这里的对外指的是与客户端程序进行交互；而后者只是被动地追随领导者副本而已，不能与外界进行交互。 副本机制可以保证数据的持久化或消息不丢失，但没有解决伸缩性的问题（Scalability），如果领导者副本积累了太多的数据以至于单台 Broker 机器都无法容纳了，此时就需要把数据分割成多份保存在不同的 Broker 上。这种机制就是所谓的分区（Partitioning）。 Kafka 中的分区机制指的是将每个主题划分成多个分区（Partition），每个分区是一组有序的消息日志，生产者生产的每条消息只会被发送到一个分区中。 **副本是在分区这个层级定义的。**每个分区下可以配置若干个副本，其中只能有 1 个领导者副本和 N-1 个追随者副本。生产者向分区写入消息，每条消息在分区中的位置信息由一个叫==位移（Offset）==的数据来表征。分区位移总是从 0 开始，单调递增且不会改变。 Kafka 使用==消息日志（Log）来保存数据，一个日志就是磁盘上一个只能追加写（Append-only）消息的物理文件。因为只能追加写入，故避免了缓慢的随机 I/O 操作，改为性能较好的顺序I/O 写操作，这也是实现 Kafka 高吞吐量特性的一个重要手段。不过如果你不停地向一个日志写入消息，最终也会耗尽所有的磁盘空间，因此 Kafka 必然要定期地删除消息以回收磁盘。怎么删除呢？简单来说就是通过日志段（Log Segment）==机制。在 Kafka 底层，一个日志又近一步细分成多个日志段，消息被追加写到当前最新的日志段中，当写满了一个日志段后，Kafka 会自动切分出一个新的日志段，并将老的日志段封存起来。Kafka 在后台还有定时任务会定期地检查老的日志段是否能够被删除，从而实现回收磁盘空间的目的。 消费者组（Consumer Group）指的是多个消费者实例共同组成一个组来消费一个主题，主题中的每个分区只会被组内的一个消费者实例消费。这样可以有效提升消费端的吞吐量。此外，Kafka如果检测到消费者组内某个实例挂了，会把这个实例之前负责的分区转移给其他消费者，这个过程就是重平衡（Rebalance）。 当数据在网络和磁盘进行传输时，避免昂贵的内核态数据拷贝，Kafka使用了Linux平台实现的==零拷贝（Zero Copy）==技术。 Kafka版本演进 0.8 引入副本机制，至此成为一个真正意义上完备的分布式高可靠消息队列解决方案，此版本API需要指定ZooKeeper地址； 0.8.2.0 社区版引入新版本Producer API，此版本需要指定Broker地址； 0.9.0.0 增加基础的安全认证/权限，使用Java重写消费者API，引入Kafka Connect组件，此版本Producer API比较稳定； 0.10.0.0 里程碑，引入Kafka Stream； 0.10.2.2 新版本Consumer API，修复了可能导致Producer API性能降低的bug； 0.11.0.0 提供幂等性Producer API和事务 API，对消息格式做了重构。 集群参数配置 Broker端参数 log.dirs: Broker需要使用的文件目录路径，逗号分割，最好是不同物理磁盘上的目录，能提高吞吐量 log.dir: Broker需要使用的单个文件路径 zookeeper.connect: zk地址 # chroot zk别名，加在zookeeper.connect参数最后，不用每个zk地址都加 # zookeeper.connect=zk1:2181,zk2:2181,zk3:2181/kafka1 listeners: 协议名称://主机:端口, 协议名称://主机:端口... 告诉外部连接着通过什么协议访问主机名和端口开放的kafka advertised.listeners: 这组监听器是Broker用于对外发布的（通常是双网卡的外网IP） # 协议可以自定义，如果自定义协议名称，必须用listener.security.protocol.map参数指定具体底层安全协议 listener.security.protocol.map=CONTROLLER:PLAINTEXT :表示CONTROLLER自定义协议底层使用明文不加密传输 auto.create.topics.enable: 是否允许自动创建Topic unclean.leader.election.enable: 是否允许Unclean Leader（落后很多的分区副本）选举 auto.leader.rebalance.enable: 是否允许定期进行Leader选举 log.retention.{hour|minutes|ms}: 控制一条消息数据被保存的时间 log.retention.bytes: Broker为消息保存的总磁盘容量大小 message.max.bytes: 控制Broker能够接收的最大消息大小 Topic级别参数 Topic 级别参数会覆盖全局Broker 参数的值 retention.ms: 该Topic消息被保存的时长，默认7days retention.bytes: 为该Topic预留的磁盘空间，默认-1，无限制 max.message.bytes: Broker能够正常接收该Topic的最大消息大小 JVM参数 KAFKA_HEAP_OPTS: 堆大小 KAFKA_JVM_PERFORMANCE_OPTS: GC参数 操作系统参数 ulimit -n 1000000: Linux操作系统能打开的最大文件描述符数量，尽可能大 操作系统其他参数包括swap调优和Flush落盘时间，swap如果设置为0，当物理内存耗尽，操作系统会随机选择一个进程 kill 掉，无法观测到 Broker 性能的下降和预警。Kafka只要将数据写入到操作系统的页缓存就可以任务消息写入成功，之后由操作系统定期将脏数据落盘到磁盘上。鉴于Kafka提供的多副本冗余机制，可以稍微将定期刷盘的间隔增大。 分区机制 分区策略（由生产者决定消息发送到主题的哪个分区） 自定义分区策略：通过生产者端partitioner.class参数指定分区策略，需要指定一个实现org.apache.kafka.clients.producer.Partitioner接口的类并实现其中的partition()方法。 轮询策略（Round-robin）：顺序分配，默认分区策略； 随机策略（Randomness）：随机地将消息放到某个分区中； 按消息键保序策略（Key-ordering）：同一个key的所有消息进入相同的分区 压缩 压缩可能发生在两个地方：生产者和Broker端 生产者程序中配置compression.type启用指定的压缩算法类型，比如 Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); props.put(&quot;acks&quot;, &quot;all&quot;); props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.seriailization.StringSerializer&quot;); props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); prosp.put(&quot;comression.type&quot;, &quot;gzip&quot;); // 启用GZIP压缩 Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); 大部分情况下，Broker端接收到Producer的消息后原封不动地保存，但是有两种例外情况可能让Broker重新压缩消息： Broker端指定了和Producer端不同的压缩算法。Broker端接收到消息后先用Producer端的压缩算法解压，然后用Broker端的压缩算法重新压缩。 Broker端发生了消息格式转换。主要是为了兼容老版本的消费者程序。 Kafka会将消息使用的压缩算法封装进消息集合中，当Consumer读取消息集合时，就知道使用哪种压缩算法解压了。Broker端也会进行解压缩，目的时为了对消息执行验证，但是会对Broker端性能造成一定影响。 Facebook Zstandard提供的各压缩算法benckmark结果 算法 压缩比 压缩吞吐量 MB/s 解压吞吐量 MB/s zstd 1.3.4-1 2.877 470 1380 zlib 1.2.11-1 2.743 110 400 brotli 1.0.2-0 2.701 410 430 quicklz 1.5.0-1 2.238 550 710 lzo1x2.09-1 2.108 650 830 lz4 1.8.1 2.101 750 3700 snappy 1.1.4 2.091 530 1800 lzf 3.6-1 2.077 400 860 无消息丢失配置 当Kafka的若干个（可以选择一个或者所有）Broker成功接收到一条消息并成功写入到日志文件后，会告诉生产者这条消息已成功提交。对于“已提交”的消息，Kafka会保证消息有限度的持久化。 如果生产者使用的是producer.send(msg)发送消息，这个API是异步的，调用后立即返回，无法知道消息是否发送成功。应该使用producer.send(msg, callback)，这个API能准确的告诉客户端消息是否提交成功。 acks=all表示Producer对“已提交”消息的定义，all表明需要所有副本Broker都接收到消息才算“已提交”。 retries=3表示Producer的自动重试消息发送次数。 拦截器 生产者拦截器，在消息发送前和消息成功提交后插入逻辑；消费者拦截器，在消息消费前和提交位移（offset）后插入逻辑。 自定义生产者拦截器需要实现org.apache.kafka.clients.producer.ProducerInterceptor接口，核心方法： onSend：在消息发送之前被调用 onAcknowledgement：在消息成功提交或发送失败后被调用，onAcknowledgement的调用要早于生产者发送消息的回调callback的调用 自定义消费者拦截器需要实现org.apache.kafka.clients.consumer.ConsumerInterceptor接口，核心方法： onConsume：在消息被Consumer处理之前调用 onCommit：Consumer在提交offset之后调用 连接 在==创建== KafkaProducer 实例时，生产者应用会在后台创建并启动一个名为 Sender 的线程，该 Sender 线程开始运行时首先会创建与 Broker 的连接。也就是说，在调用send方法前，Producer就已经连接到Broker了，连接的是bootstrap.servers指定的Broker地址。实际使用中，并不需要把所有Broker地址都配置在bootstrap.servers参数中，因为为 Producer 一旦连接到集群中的任一台Broker，会向某一台Broker发送METADATA请求，就能拿到整个集群的 Broker 信息。 当 Producer 更新了集群的元数据信息之后，如果发现与某些 Broker 当前没有连接，那么它就会创建一个 TCP 连接。同样地，当要发送消息时，Producer 发现尚不存在与目标 Broker 的连接，也会创建一个。 Producer端参数connections.max.idle.ms配置了Kafka关闭TCP连接的空闲时间，默认9分钟。如果设置为 -1则表示不关闭空闲的TCP连接。 和生产者不同的是，消费者端构建KafkaConsumer实例时是不会创建任何TCP连接的，而是在调用KafkaConsumer.poll方法时创建。再具体一点，poll方法内部有3个创建TCP连接的时机： 发起FindCoordinator请求时。消费者向集群中负载最小（待发送请求最少）的Broker发送请求，查询Coordinator对于的Broker，这一步会创建Socket连接。这个TCP之后会被关闭。 完成上一步的FindCoordinator请求后，连接正真的Coordinator，此时会创建Socket连接。 消费数据时。消费者会为每个要消费的分区创建与该分区Leader副本所在的Broker连接的TCP。 消费者关闭Socket也分为主动关闭和Kafka自动关闭。主动关闭是调用API关闭，如KafkaCosnumer.close()，自动关闭由消费者端参数connection.max.idle.ms控制，默认9分钟。 消息可靠性保障 Kafka消息交付可靠性保障： 最多一次：消息可能会丢失，但不会重复发送（禁止Producer重试） 至少一次：消息不会丢失，但可能重复发送（Kafka默认） 精确一次：消息不会丢失，也不会重复发送（幂等和事务） 幂等性（Idempotence） 指的是某些操作或函数能够被执行多次，但每次得到的结果都是不变的。 0.11.0.0之后引入了幂等性Producer，开启方法： props.put(&quot;enable.idempotence&quot;, true); // or props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true); 为实现幂等性，Kafka底层设计架构中引入了ProducerID和SequenceNumber ProducerID：在每个新的Producer初始化时，会被分配一个唯一的ProducerID，这个ProducerID对客户端使用者是不可见的。 SequenceNumber：对于每个ProducerID，Producer发送数据的每个Topic和Partition都对应一个从0开始单调递增的SequenceNumber值。 Producer在每条消息中附带了PID（ProducerID）和SequenceNumber。Broker 为每个 Topic 的每个 Partition 都维护了一个当前写成功的消息的最大 PID-SequenceNumber 元组，当 Broker 收到一个比当前最大 PID-Sequence Number 元组小（或等于）的 SequenceNumber 消息时，就会丢弃该消息，以避免造成数据重复存储。 幂等性Producer 能够保证某个主题的一个分区上不出现重复消息，它无法实现多个分区的幂等性。或者当Producer进程重启后，这种幂等性也会丢失。 要实现多分区及多会话上的消息不重复，需要事务（Transaction）或依赖事务型Producer。 事务（Transaction） 主要是在 read committed 隔离级别上做事情。它能保证多条消息原子性地写入到目标分区，同时也能保证 Consumer 只能看到事务成功提交的消息。 事务型 Producer 事务型 Producer 能够保证将消息原子性地写入到多个分区中。这批消息要么全部写入成功，要么全部失败。另外，事务型 Producer 也不惧进程的重启。Producer 重启回来后，Kafka 依然保证它们发送消息的精确一次处理。 // 开启事务型Producer enable.idempotence=true 在Consumer端读取事务型Producer发送的消息，设置 isolation.level参数： read_uncommitted：默认值，Consumer能读取Kafka写入的任何消息 read_committed：Consumer只会读取事务型Producer成功提交的事务写入的消息，也能看到非事务型Producer写入的所有消息。 消费者组 Consumer Group 是 Kafka 提供的可扩展且具有容错性的消费者机制，组内的多个消费者或消费者实例共享一个公共的ID（Group ID），组内所有消费者协调在一起消费订阅主题的所有分区。 如果所有实例都属于同一个 Group，那么它实现的就是消息队列模型；如果所有实例分别属于不同的 Group，那么它实现的就是发布 / 订阅模型。· 理想情况下，Consumer 实例的数量应该等于该 Group 订阅主题的分区总数。 在新版本的 Consumer Group 中，Kafka 社区重新设计了 Consumer Group 的位移管理方式，采用了将位移保存在 Kafka 内部主题（ __consumer_offsets）的方法。 Rebalance 本质上是一种协议，规定了一个 Consumer Group 下的所有 Consumer 如何达成一致，来分配订阅Topic 的每个分区。 触发Rebalance的3个条件： 组成员数发生变更。有新的Consumer加入或有Consumer崩溃下线 订阅主题数发生变更。Consumer Group 可以使用正则表达式的方式订阅主题，比如consumer.subscribe(Pattern.compile(“t.*c”)) ，当有新的主题匹配正则表达式则会触发 订阅主题的分区数发生变更。 每个Consumer会定期地向Coordinator发送心跳，如果Consumer不能及时地发送心跳请求，Coordinator就会将其从Group中移除，开启新一轮的Rebalance。这个时间由参数session.timeout.ms控制，默认10s。参数heartbeat.interval.ms控制心跳请求的频率，Coordinator通过心跳请求的响应通知Consumer开启Rebalance。参数max.poll.interval.ms参数限定Consumer端两次poll调用的最大间隔，默认5min，如果超过这个时间，表明Consumer无法顺利地消费完poll的消息，Consumer会主动离开Group，Coordinator会开启新一轮的Rebalance。 但是，Rebalance发生时，所有Consumer实例都会停止消费，直到Rebalance完成。Rebalance过程中，由协调者Coordinator组件帮助完成订阅主题分区的分配，Coordinator专门为Consumer Group 服务，负责为Group执行Rebalance以及提供位移管理和组成员管理等。所有Broker在启动时，都会创建和开启相应的Coordinator组件。 协调者Coordinator 协调者专门为Consumer Group 服务，负责为Group执行Rebalance以及提供位移管理和组成员管理。Consumer在提交位移时，其实是向Coordinator所在的Broker提交位移；当Consumer启动时，也是向Coordinator所在的Broker发送各种请求，然后由Coordinator执行消费者组的注册、成员管理等元数据管理。 所有的Broker都有各自的Coordinator组件。 通过计算partitionId = Math.abs(groupId.hashCode() % offsetTopicPartitionCount)确定由位移主题的哪个分区来保存Group的数据。 找到该分区的Leader副本，该Broker就是对于的Coordinator。 位移主题 Offset Topic __consumer_offsets使用Kafka自定义的消息格式，主题中的key包含3部分内容：GroupID，主题名，分区号。 主题消息体的格式： 包含位移数据、时间戳和自定义的数据 用于保存Consumer Group信息的消息 用于删除Group过期位移甚至是删除Group的消息（消息体为null，当某个Consumer Group下所有Consumer都停止且位移数据都已被删除，Kafka会向位移主题的对应分区写入） 当Kafka集群中第一个Consumer启动时，会自动创建位移主题。__consumer_offsets主题的分区数通过Broker端参数offset.topic.num.partitions设置，默认50，副本数默认3。 如果Consumer采用的是自动提交位移的方式，就会无限期地向__consumer_offsets中写入主题消息。此时需要Kafka定期删除过期消息。Kafka使用的是Compact策略，即对于主题中同一个Key的多条消息，只保留最新的消息。Kafka提供了专门的后台线程（Log Cleaner）定期巡检待Compact的主题，看看是否存在满足条件的可删除数据。 多线程消费者实例 KafkaConsumer是单线程设计的（心跳线程是单独的线程），能简化Consumer端的设计，客户端可以在Consumer获取到消息后，多线程处理消息。 KafkaConsumer类不是线程安全的，如果在多线程中共享一个KafkaConsumer实例，程序会抛出异常。可以多线程中每个线程维护一个专属的KafkaConsumer实例，负责完整的消息获取、处理流程。 消费进度 Kafka监控的是分区的Lag，如果要计算主题的Lag，需要手动汇总素有分区的Lag。如果消费者速度赶不上生产者的速度，会导致数据不在操作系统的页缓存中，而是磁盘中，这样这些数据就不能通过零拷贝传输，进一步拉大消费者和生产者的差距，Lag越来越大。 有3种方法监控消费进度： Kafka自带的命令行工具kafka-consumer-groups脚本 Kafka Java Consumer API编程 Kafka自带的JMX监控指标 副本 副本机制，也称备份机制，通常指分布式系统在多台机器上有相同的数据拷贝。 提供数据冗余。即使部分组件失效，系统依然能继续运转，提高整体可用性和数据持久性。 提高高伸缩性。支持横向扩展，能通过增加机器的方式提高读写性能，进而提高吞吐量。 改善数据局部性。运行数据放入与用户地理位置相近的地方。 Kafka的副本机制与其他分布式系统不同，Kafka的追随者副本不对外提高服务，所有的请求必须由Leader副本处理，追随者副本的唯一任务就是从Leader副本异步拉取消息，写入到自己的提交日志种，从而实现与Leader副本的同步。当Leader副本挂了，Kafka依托于Zookeeper提供的监控功能可以实时感知，并立即开启新一轮的Leader选举。 这种机制有两个好处： 实现“Read-your-writes”，生产者写入消息后，消费者马上可以读取到，而不用等待异步同步完成。 实现单调读（Monotonic Reads），由于副本的同步进度不一样，避免从不同的副本上读取到不一致的数据。 ISR （In-Sync Replicas） ISR副本集合是一个动态调整的集合，集合中的副本都被认为是和Leader副本同步的，Leader副本是天然就在ISR中的（如果Leader副本挂了，集合就是空的）。Broker端参数replica.lag.time.max.ms含义是追随者副本能够落后Leader副本的最大时间间隔（默认10s），如果追随者副本落后超过这个时间，就会被踢出ISR集合，等重新追上了会被重新加回ISR。 所有不在ISR集合中的存活副本称为非同步副本，如果允许选举这种副本的过程称为Unclean 领导者选举，Broker端参数unclean.leader.election.enable可以控制是否允许非同步副本参与Leader选举。如果开启，可能回造成数据丢失，反之，则能保证数据的一致性。通过这个参数，Kafka赋予你选择CP还是AP的权利。 ","link":"https://xuhang.github.io/post/kafka-he-xin-ji-zhu-yu-shi-zhan-bi-ji/"},{"title":"Electron 应用开发","content":"一、了解Electron Electron 是由 Github 开发，用 HTML，CSS 和 JavaScript 来构建跨平台桌面应用程序的一个开源库。最初的时候是属于 Github 开源代码编辑器 Atom 的一部分，在 2014 春季这两个项目相继开源。 里程碑事件 时间 里程碑 2013 年 4 月 Atom Shell 项目启动 。 2014 年 5 月 Atom Shell 被开源 。 2015 年 4 月 Atom Shell 被重命名为 Electron 。 2016 年 5 月 Electron 发布了 v1.0.0 版本 。 2016 年 5 月 Electron 构建的应用程序可上架 Mac App Store 。 2016 年 8 月 Windows Store 支持 Electron 构建的应用程序 。 Electron优点： 跨平台，降低开发成本。 强大的 npm 生态，提升开发效率 简单易学的 JavaScript 语言，基于 Web 的桌面开发，不用再去学习 C# 或者 Swift，降低学习成本。 苹果商店和微软商店都支持提交 Electron 应用程序。 开发速度快，适合需要段时间内看到效果的应用开发 缺点： 打包体积大 Chromium比较吃内存 跨平台并不代表一次编写，处处运行，需要在各个系统上测试、打包 版本更新频繁，多个版本间有兼容性的问题 跨平台开发框架比较 框架 支持语言 优缺点 Qt C++/Python 优点：流行、稳定，内置自绘引擎，保证不同系统上界面一致，PyQt可以使用Python开发缺点：高分屏缩放显示问题，商业需授权，免费版本版权问题 GTK C/C++/JS/Rust等 优点：自绘引擎，提供大量系统相关API，商业授权友好缺点：只在Linux环境下流行，其他环境下开发困难 wxWidgets C++ 优点：稳定、成熟缺点：没有自绘引擎，界面风格不统一 FLTK C++ 优点：轻量，OpenGL引擎，有Rust绑定fltk-rs缺点：绘图API少 Duilib C++ 优点：开源、有不少大厂使用缺点：只支持Windows、不支持高分屏、几乎没有系统级API，更新不及时，开发环境搭建麻烦 Sciter C++/Rust/Go/Python等 优点：精简，个人使用免费，内部有一个浏览器核心，自研脚本类似JS缺点：闭源，商业收费，不支持Flex布局，非官方绑定质量差，社区不成熟 CEF C/C++/Go/Python/Java 优点：基于Chromium，装机量大，支持HTML/CSS/JS特性缺点：体积大、几乎不提供系统及API，版本发布频繁 MAUI C# 优点：微软开发，有自绘引擎，支持桌面端和移动端缺点：暂无稳定版，界面有XAML描述 Compose Multiplatform Kotlin 优点：新，自绘引擎Skia（Goole）缺点：稳定欠缺，打包需要封装JRE flutter-desktop Dart 优点：新，自绘引擎Skia，支持FFI缺点：不稳定，Dart大括号嵌套不友好，打包体积大 webview2 C#/C++ 优点：可以复用系统当中已存在的webview2二进制资源，打包体积小缺点：只支持Windows，闭源 TAURI Rust 优点：安装包小，新，开源，最近比较火缺点：新，不稳定，webview有的问题TARI都有 NW.js（node-webkit） JS 优点：开源，开发方便缺点：封装的系统级API不如Electron，生态不完善，API不稳定 Electron JS 优点：作者是NW.js原成员，Github，微软等大厂在用（github客户端，VSCode），生态完善，系统级API多缺点：打包体积大 ImGui C++ 优点：对游戏开发者友好，支持多种引擎比如OpenGL，Directx，Vulkan等，打包体积小缺点：循环绘制整个界面，消耗CPU，有一些小问题 如果是会C/C++，可选的框架就很多，可以根据需求选择最合适的框架，但是如果会一点JS，这里的Electron可能是比较合适的。首先是开源，开发语言JS相对大众，遇到问题也比较容易找到解决方案；其次是Electron的生态完善，有比较多的大厂在使用，比如微软的VS Code，Github官方的客户端GUI，Slack，Postman等，国内也有很多使用案例，比如迅雷X，utools等，更多的Electron 应用可以在 🔗这里 发现。 二、使用Electron开发应用 问题 在项目开发中，大部分时间是使用QQ作为沟通工具，那项目相关的需求文档、接口文档也基本上是通过QQ分享，时间久了在QQ的本地目录里就会有很多不同项目的相关文档，要找到就会很困难，而且很多文档名仅差几个字，很难区分每个文档属于哪个项目下的。 方案1 每次涉及新的需求，在用户目录下新建一个该项目的文件夹，每次有收到项目相关的文件就将文档存放到该路径下，再次使用时就在该目录下查找。 方案2 方案1虽然能将文件分项目存储，但是每个文件的内容很难通过文件获取；每次进入项目目录需要打开资源管理器，进入用户目录，选择相关项目，步骤繁琐；……这里我们开发一个专门解决这个痛点的软件，用来管理项目相关的文件，该软件有以下功能： 分项目管理所属文件 以易读的方式显示文件的加入时间 支持直接从聊天软件对话框中拖拽文件到项目中 如果其他人需要，可以将项目所有文件打包分享 可以对每个文件注释，标注该文件的作用、内容等 快速定位到文件和打开文件 针对每个项目内嵌一个Kanban来记录具体任务的完成情况 根据不同的命名规则自动关联到文件的最新版本 加入沟通功能，可以快速与小组成员分享文件 Windows UI MacOS UI 开发 由于Electron版本更新较快，接口和功能可能会有较大的变动，建议参考官方文档🔗官方文档（有中文）。 0. Electron中常用的模块 可以从命名规则中得知，如果引入的是类，用户是可以创建多个实例对象的，如果引入的是对象，则是Electron已经创建好的，且一个应用里只有一个实例。 const { app, // 对该应用程序的引用，一个应用可以包含0-多个窗口 BrowserWindow, // 窗口类 Menu, // 菜单类 Tray, // 任务栏图标 Notification, // 通知 shell, // Shell对象，执行系统命令 ipcMain, // IpcMain对象，用来触发和监听消息事件 globalShortcut, // GlobalShortcut对象，用来处理全局快捷键的绑定和解绑 screen, // Screen对象，用来获取显示器相关数据 nativeImage // NativeImage对象，用来生成和获取图片 } = require('electron'); 1. 单实例 单实例是为了保证只有一个在工作，防止误操作多次双击后启动多个实例。这里多个实例没有意义，所以限制多实例的生成，如果重复创建app对象则直接退出。 const gotTheLock = app.requestSingleInstanceLock() if (!gotTheLock) { app.quit() } else { app.on('second-instance', (event, commandLine, workingDirectory) =&gt; { // 当运行第二个实例时,将会聚焦到myWindow这个窗口 if (win) { if (win.isMinimized()) win.restore() win.focus() } }) // app 启动创建窗口 app.on('ready', createWindow); } 2. 自定义协议 在浏览器中常用的协议有http、https、file等，自定义协议可以让我们通过在浏览器访问自定义schema打开我们的应用，并执行指定操作。这里我们以软件名称fily作为协议名称，后面的uri路径为要访问的项目名称(目前支持英文字母和数字的名称，中文需要额外的编解码)。 fily://project-name 实现方式MacOS和Windows有区别，MacOS通过app.setAsDefaultProtocolClient设置默认协议，Windows通过第二实例打开。 // MacOS if (process.defaultApp) { if (process.argv.length &gt;= 2) { app.setAsDefaultProtocolClient('fily', process.execPath, [path.resolve(process.argv[1])]) } } else { app.setAsDefaultProtocolClient('fily') } app.on('open-url', (event, url) =&gt; { openUrl(url); }) //Windows const gotTheLock = app.requestSingleInstanceLock() if (!gotTheLock) { app.quit() } else { app.on('second-instance', (event, commandLine, workingDirectory) =&gt; { // 当运行第二个实例时,将会聚焦到myWindow这个窗口 if (win) { if (win.isMinimized()) win.restore() win.focus() let lastCommand = commandLine[commandLine.length - 1]; // ① if (lastCommand.indexOf('fily://') === 0) { // ② openUrl(lastCommand) // ③ } } }) // app 启动创建窗口 app.on('ready', createWindow); } 3. 进程间通信 Electron分主进程和渲染进程，主程序 (main process)，它运行在 Node.js环境里，负责控制应用的生命周期、显示原生界面、执行特殊操作并管理渲染器进程 (renderer processes)。应用中的每个页面都在一个单独的进程中运行，我们称这些进程为 渲染器(renderer) 。 渲染器也能访问前端开发常会用到的 API和工具，例如用于打包并压缩代码的webpack，还有用于构建用户界面的 React 。 通信方向 代码 主进程监听事件 ipcMain.on(EVENT-NAME, (event, data) =&gt; {}) 主进程向渲染进程发送通知 win.webContents.send(EVENT-NAME, {}) 渲染进程监听事件 ipcRenderer.on(EVENT-NAME, (event, data) =&gt; {}) 渲染进程向主进程发送通知 ipcRenderer.send(EVENT-NAME, {}) 主进程和渲染进程主要负责的功能范围不同，进程间的通信通过事件监听和消息通知来交互完成。 Electron 的主进程是一个拥有着完全操作系统访问权限的 Node.js 环境。 除了 Electron 模组之外，你也可以使用 Node.js 内置模块 和所有通过 npm 安装的软件包。另一方面，出于安全原因，渲染进程默认跑在网页页面上，而并非 Node.js里。为了将 Electron 的不同类型的进程桥接在一起，我们需要使用被称为 预加载 的特殊脚本。 4. 创建窗口 Windows UI MacOS UI 软件打开之后需要展示给用户使用的界面，ELectron封装了不同操作系统的UI界面，所以在不同系统上使用会有不同的风格。为了统一风格，我们去掉窗口的边框，使用HTML+CSS绘制一套统一的界面。可以从上面的对比图看出，Windows和MacOS上的顶部titleBar的样式是有区别的，Windows的titleBar会显示菜单，MacOS是空白的，因为MacOS的菜单是当程序窗口是活动窗口时，在显示器左上角会显示当前程序的菜单，所以两个平台的菜单绑定是有区别的。 // 创建窗口要在app启动完成之后开始 app.on('ready', createWindow); function createWindow() { // 读取package.json文件中的属性 var package = require('./package.json') console.log('starting ' + package.name + ' ' + package.version) // 隐藏菜单 Menu.setApplicationMenu(null) // 这里菜单仅对MacOS有效，因为Windows隐藏边框之后没有菜单栏 const appMenu = Menu.buildFromTemplate(appMenuTemplate); Menu.setApplicationMenu(appMenu) // 创建浏览器窗口并设置宽高 win = new BrowserWindow({ width: 1200, height: 800, titleBarStyle: 'customButtonsOnHover', icon: __dirname + './img/fily.png', maximizable: false, minimizable: false, fullscreenable: false, frame: false, // 是否有边框 // resizable: true, // 是否可调整大小 transparent: true, //是否透明 webPreferences: { // 新版Electron添加了一些安全限制，这里需要设置 nodeIntegration: true, enableRemoteModule: true, contextIsolation: false } }) remoteMain.initialize() remoteMain.enable(win.webContents); // MacOS底部的Dock栏图标 if (systemType === &quot;Darwin&quot;) { app.dock.setIcon(path.join(__dirname, './img/icon_512x512.png')); // 设置Dock菜单 const dockMenu = Menu.buildFromTemplate(dockMenuTempalte); app.dock.setMenu(dockMenu); } // 加载页面 -- 向用户展示的界面 win.loadFile('./page/projectRelatedFiles.html') // Windows 任务栏图标菜单（右下角） / MacOS任务栏图片（右上角） // tray = new Tray(path.join(__dirname, &quot;./img/fily.png&quot;)); const trayImage = nativeImage.createFromPath(path.join(__dirname, &quot;./img/icons.iconset/file20Template.png&quot;)); trayImage.resize({ width: 16, height: 16 }) trayImage.setTemplateImage(true); tray = new Tray(trayImage); const contextMenu = Menu.buildFromTemplate([ { label: '退出', click: function() { app.quit() } } ]) tray.setToolTip('Fily - File manager for you!') tray.setContextMenu(contextMenu) // 添加window关闭触发事件 win.on('close', () =&gt; { win = null; app.quit(); }); win.setProgressBar(0.7); } 5. 添加快捷键注册 可以通过注册全局快捷键快速执行某个操作或者在菜单项里对指定的菜单绑定快捷键。 // 注册全局快捷键-退出应用 globalShortcut.register('Ctrl+W', () =&gt; { win = null; app.quit(); }); // 全局快捷键-打开开发中工具 globalShortcut.register('Ctrl+Shift+I', () =&gt; { win.webContents.toggleDevTools() }) // 通过构建菜单绑定快捷键执行菜单操作 const appMenuTemplate = [{ label: 'Projects', submenu: [{ label: 'New Project', accelerator: 'Cmd+N', click: () =&gt; { win.webContents.send('new-project-dialog', {}) } }, { label: 'Rename Project' }, { label: 'Delete Project' }, { type: 'separator' }, { label: 'Archive' }] }, { label: 'Files', submenu: [{ label: 'Import File' }, { label: 'Import Folder' }] }] 6. 添加进度条 Windows和MacOS都可以在任务（Dock）栏的图标那里显示一个进度条，通常可以用来显示下载进度或者任务完成进度。 Windows UI MacOS UI // 这里的进度条在70%的进度 win.setProgressBar(0.7); 7. 任务栏预览快捷操作（仅Windows） 在任务栏的预览视图那里可以设置快捷操作键，这里我们通过添加两个按钮用于访问“前一个”和“后一个”项目。 // 主线程 win.setThumbarButtons([{ tooltip: '上一张', icon: path.join(__dirname, './resource/icos/pre.ico'), click() { switchThumbImage(-1) } }, { tooltip: '下一张', icon: path.join(__dirname, './resource/icos/next.ico'), click() { switchThumbImage(1) } }]) function switchThumbImage(delta) { if (delta &gt; 0) { win.webContents.send('show-next-project', {}) } else { win.webContents.send('show-prev-project', {}) } } // 渲染进程 ipcRenderer.on('show-next-project', (event, message) =&gt; { // document.getElementById('gallery_image').setAttribute('src', message['src']); let curProj = $('.projlist ul li.active') let nextProj; if (!!curProj[0]) { nextProj = curProj.next(); if (!nextProj[0]) { nextProj = $('.projlist ul li').first() } } else { nextProj = $('.projlist ul li').first(); } nextProj.click(); }) ipcRenderer.on('show-prev-project', (event, message) =&gt; { let curProj = $('.projlist ul li.active') let prevProj; if (!!curProj[0]) { prevProj = curProj.prev(); if (!prevProj[0]) { prevProj = $('.projlist ul li').last() } } else { prevProj = $('.projlist ul li').last(); } prevProj.click(); }) 8. 通知 这里监听右上角的关闭按钮，点击关闭之后会弹出通知，并延迟1s后退出应用。 Windows UI MacOS UI ipcMain.on('close-window', (event, data) =&gt; { if (!Notification.isSupported()) { console.log('当前系统不支持通知'); } app.setAppUserModelId(&quot;我的Electorn示例&quot;) const notification = { appId: &quot;com.xuhang.electron.demo&quot;, icon: path.join(__dirname, './img/icon_512x512.png'), title: '正在关闭……', body: 'Bye ヾ(•ω•`)o' } new Notification(notification).show() setTimeout(() =&gt; { console.log('fily is exiting ...') app.quit(); }, 1000); }); 9. 数据存储 项目列表和项目相关的文件关联关系需要持久化才能保证数据不丢失，通常可以选择将数据保存到远程数据库，但是这样必须要有一个实时在线的服务。也可以选择数据保存在本地，这样既能保证数据实时可用，也能保证数据的安全性，一来也是因为数据没有在线访问的必要。 离线存储有几种可选方案： 利用浏览器的localStorage存储，优点是浏览器原生支持，Electron本身就包含了Chromium，缺点是如果清空浏览器缓存数据也丢失了。 利用Electron的文件读写API，自定义存储格式和存储路径，将数据保存到本地文件，优点是没有第三方依赖，缺点是需要自己实现数据的序列化与反序列化。 利用第三方库，如lowdb将数据以JSON格式存储在本地文件，优点是提供了增删改查的API，缺点是有学习成本。 集成SQLite，优点是数据有各种工具可以读取，也可以通过网盘将数据进行同步，SQLite也比较稳定成熟，缺点是Electron对SQLite的集成比较复杂，打包后体积也会变大。 综合各种方案的优劣，我们前期需要快速出一个可用的版本，所以选择了lowdb。 10. 引入jQuery 前端开发的发展速度很快，可能两三年技术路线就会更新迭代，jQuery虽然过时了，但是对于非前端开发来说还是相对熟悉的框架，所以为了尽可能简化开发这里还是使用jQuery。 Electron默认启用了Node.js的require模块，而jQuery等新版本框架为了支持commondJS标准，当Window中存在require时，会启用模块引入的方式。这里主要针对jQuery1.11.1及以后的版本。 &lt;script src=&quot;../script/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; if (typeof module === 'object') { window.jQuery = window.$ = module.exports; } &lt;/script&gt; 由于Electron和ES6对于模块的导入导出采用了不同的方式，所以在引用JS文件时很容出现报错。关于各种Javascript运行时环境下的模块导入导出可以参见 🔗这篇文章 11. 开发者工具 为了方便对页面进行调试，需要为每个窗口添加打开/关闭开发者工具的快捷键监听事件。在页面的js文件中为window对象添加事件监听器并在按键为Ctrl+Shift+I是打开/关闭开发者工具，针对当前活动窗口的快捷键才会生效 。 window.addEventListener('keydown', (event) =&gt; { if (event.ctrlKey &amp;&amp; event.shiftKey &amp;&amp; event.key === 'I') { remote.getCurrentWindow().toggleDevTools(); } }) 12. 聊天功能 在界面左上角重载按钮的旁边有个聊天图标，点击会在屏幕右侧出现一个透明窗口，该窗口实现了鼠标点击穿透，但同时也保持窗口置顶，最下方的文字输入区域设置透明度0.6，但是为了能输入文字，该区域为点击不穿透。该聊天功能使用腾讯的IM云服务，每个打开该聊天界面的终端会自动申请加入一个开放群，目前仅实现了文字聊天功能。 Windows UI MacOS UI 三、打包发布 开发完成后将代码打包并生成系统安装包。之前主要使用electron-package和electron-builder来打包，目前Electron官方推荐使用electron-forge，后者针对发布和更新会更方便一点。 electron-builder在package.json中添加如下参数： &quot;build&quot;: { &quot;productName&quot;: &quot;fily&quot;, &quot;appId&quot;: &quot;com.xuhang.electron.demo&quot;, &quot;copyright&quot;: &quot;Copyright © 2021-2022 xu&quot;, &quot;directories&quot;: { &quot;output&quot;: &quot;build&quot; }, &quot;nsis&quot;: { &quot;oneClick&quot;: false, &quot;allowElevation&quot;: true, &quot;allowToChangeInstallationDirectory&quot;: true, &quot;installerIcon&quot;: &quot;./img/icon.ico&quot;, &quot;uninstallerIcon&quot;: &quot;./img/icon.ico&quot;, &quot;installerHeaderIcon&quot;: &quot;./img/icon.ico&quot;, &quot;createDesktopShortcut&quot;: true, &quot;createStartMenuShortcut&quot;: true, &quot;shortcutName&quot;: &quot;Fily&quot; }, &quot;win&quot;: { &quot;icon&quot;: &quot;./img/icon.ico&quot;, &quot;target&quot;: [ { &quot;target&quot;: &quot;nsis&quot;, &quot;arch&quot;: [ &quot;ia32&quot; ] } ] }, &quot;dmg&quot;: { &quot;background&quot;: &quot;media/images/dmg-bg.png&quot;, &quot;icon&quot;: &quot;media/images/icon.icns&quot;, &quot;iconSize&quot;: 100, &quot;sign&quot;: false, &quot;contents&quot;: [ { &quot;x&quot;: 112, &quot;y&quot;: 165 }, { &quot;type&quot;: &quot;link&quot;, &quot;path&quot;: &quot;/Applications&quot;, &quot;x&quot;: 396, &quot;y&quot;: 165 } ] }, &quot;mac&quot;: { &quot;target&quot;: [ &quot;dmg&quot; ], &quot;icon&quot;: &quot;./img/Icon.icns&quot;, &quot;hardenedRuntime&quot;: true, &quot;identity&quot;: null, &quot;entitlements&quot;: &quot;electron-builder/entitlements.plist&quot;, &quot;entitlementsInherit&quot;: &quot;electron-builder/entitlements.plist&quot;, &quot;provisioningProfile&quot;: &quot;electron-builder/comalibabaslobs.provisionprofile&quot; }, &quot;pkg&quot;: { &quot;isRelocatable&quot;: false, &quot;overwriteAction&quot;: &quot;upgrade&quot; }, &quot;mas&quot;: { &quot;icon&quot;: &quot;./img/Icon.icns&quot;, &quot;hardenedRuntime&quot;: true, &quot;entitlements&quot;: &quot;electron-builder/entitlements.mas.plist&quot;, &quot;entitlementsInherit&quot;: &quot;electron-builder/entitlements.mas.plist&quot; } } electron-forge添加如下参数： &quot;config&quot;: { &quot;forge&quot;: { &quot;packagerConfig&quot;: {}, &quot;makers&quot;: [ { &quot;name&quot;: &quot;@electron-forge/maker-squirrel&quot;, &quot;config&quot;: { &quot;name&quot;: &quot;fily&quot; } }, { &quot;name&quot;: &quot;@electron-forge/maker-zip&quot;, &quot;platforms&quot;: [ &quot;darwin&quot; ] }, { &quot;name&quot;: &quot;@electron-forge/maker-deb&quot;, &quot;config&quot;: {} }, { &quot;name&quot;: &quot;@electron-forge/maker-rpm&quot;, &quot;config&quot;: {} } ] } } 然后配置script脚步指定指定的任务： &quot;scripts&quot;: { &quot;test&quot;: &quot;echo \\&quot;Error: no test specified\\&quot; &amp;&amp; exit 1&quot;, &quot;start&quot;: &quot;electron-forge start&quot;, &quot;package&quot;: &quot;electron-forge package&quot;, &quot;dist&quot;: &quot;npm run package &amp;&amp; electron-builder build&quot;, &quot;make&quot;: &quot;electron-forge make&quot; } 四、源码 该实验项目开源，代码存放在 🔗Github ","link":"https://xuhang.github.io/post/electron-ying-yong-kai-fa/"},{"title":"MySQL45 讲笔记","content":"基础架构 连接器 连接器负责跟客户端建立连接、获取权限、维持和管理连接。 如果用户名或密码不对，你就会收到一个&quot;Access denied for user&quot;的错误，然后客户端程序结束执行。 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。 客户端如果太长时间没动静（show processlist结果的Command列显示为sleep），连接器就会自动将它断开。这个时间是由参数wait_timeout控制的，默认值是8小时。 MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，导致MySQL异常重启。 解决办法： 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果你用的是MySQL 5.7或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 查询缓存 执行过的语句及其结果可能会以key-value对的形式，被直接缓存在内存中。key是查询的语句，value是查询的结果。如果你的查询能够直接在这个缓存中找到key，那么这个value就会被直接返回给客户端。 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。 参数query_cache_type设置成DEMAND，这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用SQL_CACHE显式指定，像下面这个语句一样： mysql&gt; select SQL_CACHE * from T where ID=10； MySQL 8.0版本直接将查询缓存的整块功能删掉了，也就是说8.0开始彻底没有这个功能了。 分析器 分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条SQL语句，MySQL需要识别出里面的字符串分别是什么，代表什么。 做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个SQL语句是否满足MySQL语法。 优化器 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。 执行器 开始执行的时候，要先判断一下你对这个表T有没有执行查询的权限，如果没有，就会返回没有权限的错误。 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。 日志系统 日志模块：redo log 如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高。MySQL使用WAL技术来提升效率，WAL的全称是Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。 有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。 日志模块：binlog 上面聊到的redo log是InnoDB引擎特有的日志，而Server层也有自己的日志，称为binlog（归档日志）。 因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统——也就是redo log来实现crash-safe能力。 两种日志有以下三点不同。 redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。 redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。 redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 mysql&gt; update T set c=c+1 where ID=2; 执行上面这条update语句时执行器和InnoDB引擎内部的流程： 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的binlog，并把binlog写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。 innodb_flush_log_at_trx_commit这个参数设置成1的时候，表示每次事务的redo log都直接持久化到磁盘。sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。 binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED，区别如下： STATEMENT：每一条修改数据的 SQL 都会被记录到 binlog 中（相当于记录了逻辑操作，所以针对这种格式， binlog 可以称为逻辑日志），主从复制中 slave 端再根据 SQL 语句重现。但 STATEMENT 有动态函数的问题，比如你用了 uuid 或者 now 这些函数，你在主库上执行的结果并不是你在从库执行的结果，这种随时在变的函数会导致复制的数据不一致； ROW：记录行数据最终被修改成什么样了（这种格式的日志，就不能称为逻辑日志了），不会出现 STATEMENT 下动态函数的问题。但 ROW 的缺点是每行数据的变化结果都会被记录，比如执行批量 update 语句，更新多少行数据就会产生多少条记录，使 binlog 文件过大，而在 STATEMENT 格式下只会记录一个 update 语句而已； MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式； redo log 是物理日志，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新； 事务隔离 隔离性与隔离级别 事务拥有四个重要的特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability） 原子性 （Atomicity） 事务开始后所有操作，要么全部做完，要么全部不做，不可能停滞在中间环节。事务执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生一样。 一致性 （Consistency） 指事务将数据库从一种状态转变为另一种一致的的状态。事务开始前和结束后，数据库的完整性约束没有被破坏。 隔离性 （Isolation） 要求每个读写事务的对象对其他事务的操作对象能互相分离，即该事务提交前对其他事务不可见。 也可以理解为多个事务并发访问时，事务之间是隔离的，一个事务不应该影响其它事务运行效果。这指的是在并发环境中，当不同的事务同时操纵相同的数据时，每个事务都有各自的完整数据空间。由并发事务所做的修改必须与任何其他并发事务所做的修改隔离。 MySQL 通过锁机制来保证事务的隔离性。 持久性 （Durability） 事务一旦提交，则其结果就是永久性的。即使发生宕机的故障，数据库也能将数据恢复，也就是说事务完成后，事务对数据库的所有更新将被保存到数据库，不能回滚。 隔离级别 脏读 不可重复读 幻读 SERIALIZABLE 避免 避免 避免 REPEATABLE READ 避免 避免 允许 READ COMMITTED 避免 允许 允许 READ UNCOMMITTED 允许 允许 允许 幻读和不可重复读的区别 不可重复读的重点是修改：在同一事务中，相同的条件，第一次和第二次读到的数据不一致（中间有其它事务提交了修改）。 幻读的重点是新增或者删除：在同一事务中，相同的条件，第一次和第二次读到的记录数不一样（中间有其它事务提交了新增或者删除）。 在不同的隔离级别下，事务A的返回结果： 若隔离级别是“读未提交”， 则V1的值就是2。这时候事务B虽然还没有提交，但是结果已经被A看到了。因此，V2、V3也都是2。 若隔离级别是“读提交”，则V1是1，V2的值是2。事务B的更新在提交后才能被A看到。所以， V3的值也是2。 若隔离级别是“可重复读”，则V1、V2是1，V3是2。之所以V2还是1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。 若隔离级别是“串行化”，则在事务B执行“将1改成2”的时候，会被锁住。直到事务A提交后，事务B才可以继续执行。所以从A的角度看， V1、V2值是1，V3的值是2。 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。 事务隔离的实现 在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。 为什么尽量不要使用长事务？ 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。 多版本并发控制（Multiversion Concurrency Control），每一个写操作都会创建一个新版本的数据，读操作会从有限多个版本的数据中挑选一个最合适的结果直接返回；在这时，读写操作之间的冲突就不再需要被关注，而管理和快速挑选数据的版本就成了 MVCC 需要解决的主要问题。 MySQL与MVCC MySQL 中实现的多版本两阶段锁协议（Multiversion 2PL）将 MVCC 和 2PL 的优点结合了起来，每一个版本的数据行都具有一个唯一的时间戳，当有读事务请求时，数据库程序会直接从多个版本的数据项中具有最大时间戳的返回。 更新操作就稍微有些复杂了，事务会先读取最新版本的数据计算出数据更新后的结果，然后创建一个新版本的数据，新数据的时间戳是目前数据行的最大版本 ＋1： 数据版本的删除也是根据时间戳来选择的，MySQL 会将版本最低的数据定时从数据库中清除以保证不会出现大量的遗留内容。 事务的起点 begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用start transaction with consistent snapshot 这个命令。 在MySQL里，有两个“视图”的概念： 一个是view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是create view … ，而它的查询方法与表一样。 另一个是InnoDB在实现MVCC时用到的一致性读视图，即consistent read view，用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。 事务的启动方式 MySQL的事务启动方式有以下几种： 显式启动事务语句， begin 或 start transaction。配套的提交语句是commit，回滚语句是rollback。 set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个select语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行commit 或 rollback 语句，或者断开连接。 索引 InnoDB 的索引模型 在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。 每一个索引在InnoDB里面对应一棵B+树。 根据叶子节点的内容，索引类型分为主键索引和非主键索引。 主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引（clustered index）。 非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引（secondary index）。 基于主键索引和普通索引查询的区别 如果语句是select * from T where ID=500，即主键查询方式，则只需要搜索ID这棵B+树； 如果语句是select * from T where k=5，即普通索引查询方式，则需要先搜索k索引树，得到ID的值为500，再到ID索引树搜索一次。这个过程称为回表。 基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。 覆盖索引 如果一个索引包含所有需要查询的字段的值，称为覆盖索引，即只需扫描索引而无须回表。由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。 最左前缀原则 对于联合索引的索引项是按照索引定义里面出现的字段顺序排序的，比如对列A，B，C建立联合索引，实际上是建立了(A)，(A, B)，(A, B, C)三个索引。 MySQL会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如A = 1 and B = 2 and C &gt; 3 and D = 4 如果建立(A，B，C，D)顺序的索引，d是用不到索引的，如果建立(A，B，D，C)的索引则都可以用到，A，B，D的顺序可以任意调整。 =和in可以乱序，比如A = 1 and B = 2 and C = 3 建立(A，B，C)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。 为什么要用联合索引 减少开销。建一个（A，B，C）的联合索引，实际上是建立了(A)，(A, B)，(A, B, C)三个索引。 覆盖索引。联合索引（A，B，C）的叶节点包含ABC三列的数据，如果查询的列在（A，B，C）之中可以直接通过索引获取到数据，无需回表，减少树的随机IO。 效率高。通过联合索引筛选出的数据少，列越多数据越少 索引下推 对于联合索引中不符合最左前缀的部分，MySQL的处理方式： 在MySQL 5.6之前，只能从第一个满足最左前缀索引的数据开始一个个回表。到主键索引上找出数据行，再对比字段值。 而MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 也就是说在5.6之前的版本，不符合最左前缀的部分对InnoDB引擎是没用的，5.6及以后的版本进行了优化，直接在索引上进行筛选，减少回表次数。 普通索引 当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。 唯一索引的更新就不能使用change buffer 对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。 锁 全局锁 全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 **全局锁的典型使用场景是，做全库逻辑备份。**也就是把整库每个表都select出来存成文本。 对于支持事务的引擎，比如InnoDB，可以通过启动一个事务，确保拿到一致性视图，而且由于MVCC的支持，这个过程不需要锁库。对于不支持事务的引擎，比如MyISAM，就需要全局锁FTWRL来避免数据的更新。 set global readonly=true的方式也能使整个库只读，但是这种方式有两个弊端： 在有些系统中，readonly的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。 在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时 间处于不可写状态，风险较高。 表级锁 MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。 **表锁的语法是 lock tables … read/write。**与FTWRL类似，可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。 **另一类表级的锁是MDL（metadata lock)。**MDL不需要显式使用，在访问一个表的时候会被自动加上。MDL的作用是，保证读写的正确性。 MySQL 5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。 时间线 session A session B session C session D t1 begin -&gt; select t2 begin -&gt; select t3 begin -&gt; alter table【阻塞】 t4 begin -&gt; select【阻塞】 如何安全地给小表加字段？ 比较理想的机制是，在alter table语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。 ALTER TABLE tbl_name NOWAIT add column ... ALTER TABLE tbl_name WAIT N add column ... 行锁 MySQL的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如MyISAM引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB是支持行锁的，这也是MyISAM被InnoDB替代的重要原因之一。 两阶段锁 在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。 如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。这样事务之间锁等待的时间最少，提高并发度。 死锁和死锁检测 当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。 解决死锁的两种策略： 直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout来设置，但是会误伤等待非死锁的查询。 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。但是死锁检测在并发情况下会消耗大量CPU资源。 结合实际情况中的场景，一般是启用死锁检测，控制客户端的并发数，这样死锁检测不至于太占用CPU资源。 如何保证数据不丢失 binlog写入机制 事务执行过程中将日志写到 binlog cache，事务提交的时候，将 binlog cache 写入 binlog 文件中。一个事务的 binlog 是不能被拆分的，如果这个事务多的，都要确保一次性写入。系统给每个线程分配一个 binlog cache 内存区域（但是 binlog 文件只有一份），参数binlog_cache_size用于控制单个线程内 binlog cache 所占内存大小，如果超过这个大小，就要暂存到磁盘上。事务提交的时候，把 binlog cache 中的完整事务写入 binlog 中，并清空 binlog cache。 参数sync_binlog的含义： 0 ：每次事务提交只 write（将 binlog cache 中内容写入文件系统的 page cache），不 fsync（将 page cache 中数据持久化到硬盘） 1：每次事务提交都会执行 fsync N：每次提交都 write，但是累积 N 个事务后才 fsync redo log 写入机制 事务执行过程中将日志写到 redo log buffer，然后由innodb_flush_log_at_trx_commit参数控制 redo log 持久化的逻辑。 参数innodb_flush_log_at_trx_commit的含义： 0：每次事务提交只是把 redo log 留在 redo log buffer 中； 1：每次事务提交都将 redo log 持久化到硬盘； 2：每次事务提交只是把 redo log 写到 page cache。 同时，InnoDB 后台有个线程，每隔 1s，将 redo log buffer 中的日志调用 write 写到文件系统的 page cache 中，然后调用 fsync 持久化到硬盘。所以事务未提交时，redo log buffer 中的内容也有可能被持久化到硬盘上。 此外，还有 2 种情况会让没有提交的事务的 redo log 写入磁盘： redo log buffer 占用空间即将达到innodb_log_buffer_size的一半时，后台线程会主动写盘(write，没有 fsync)。 并行事务提交时，顺带将这个事务的 redo log buffer 持久化到磁盘。 组提交（Group commit）机制 日志逻辑序列号（log sequence number, LSN）:单调递增，对应 redo log 上的一个个写入点，每次写入长度为 length 的 redo log，LSN 的值就会加上 length。 LSN 也会写到 InnoDB 的数据页中，用来确保数据页不会被多次执行重复的 redo log。 多个事务同时执行，最先到达的会被选为这组的 leader；leader 写盘的时候会将整组的 redo log 一起持久化，组员越多，节约 IOPS 的效果越好。 binlog_group_commit_sync_delay：binlog 调用 write 多少微妙后才调用 fsync binlog_group_commit_sync_no_delay_count：累积多少次后调用 fsync 如何保证主备一致 通常将备库设置成只读(readonly)，可以方便地判断节点状态，但是 readonly 并不影响备库和主库保持同步。同步更新的线程，拥有超级权限，而 readonly 对超级权限用户是无效的。 事务日志同步的完整过程： 备库上通过change master命令设置主库 IP、端口、用户名、密码以及 binlog 文件名和日志偏移量（从哪个位置开始请求 binlog） 在备库上执行start slave命令，启动io_thread和sql_thread两个线程，分别用于负责与主库建立连接和解析日志执行 主库校验完用户名密码后，按照备库制定的文件和位置，读取 binlog 发送给备库 备库拿到 binlog 后写入本地文件（中转日志 relay log） sql_thread 从中转日志读取并解析，然后执行 binlog 的三种格式 STATEMENT：binlog 记录的时原始 SQL 语句（可能导致主备不一致） ROW：记录的是受影响的记录（占用空间），但是方便恢复数据 MIXED：由 MySQL 判断是否会影响主备不一致选择 ROW 或 STATEMENT 通过 SQL查看 binlog 中的内容： show binlog events in 'xxxx.0000001'; 借助 mysqlbinlog 工具查看 binlog 详细信息： mysqlbinlog -vv data/master.000001[binlog 文件] --start-position=9xxx[从指定行开始解析] --stop-position=9xxx[解析截止到制定行] ","link":"https://xuhang.github.io/post/mysql45-jiang-bi-ji/"},{"title":"Redis核心技术与实战笔记","content":"数据类型和数据结构 数据类型 数据结构 String SDS（简单动态字符串） List 双向链表、压缩列表(ZipList) Hash 压缩列表(ZipList)、哈希表(HashTable) Set 整数数组(IntSet)、哈希表(HashTable) Sorted Set 压缩列表(ZipList)、跳表(SkipList) 压缩列表 压缩列表类似数组，不同的是压缩列表表头有3个字段zlbytes、zltail、zllen，分别表示列表长度（字节数）、列表尾元素偏移量、列表中元素个数。此外列表尾还有一个zlend表示列表结束，固定值0xFF。 压缩列表只有查找第一个元素、最后一个元素获取列表长度的时间复杂度为O(1)，查找其他元素为O(N)。 area |&lt;------------------- entry --------------------&gt;| +------------------+----------+--------+---------+ component | pre_entry_length | encoding | length | content | +------------------+----------+--------+---------+ pre_entry_length: 1 字节：如果前一节点的长度小于 254 字节，便使用一个字节保存它的值。 5 字节：如果前一节点的长度大于等于 254 字节，那么将第 1 个字节的值设为 254 ，然后用接下来的 4 个字节保存实际长度。 Redis 基于压缩列表实现了 List、Hash 和 Sorted Set 集合类型，可以节省内存空间。 跳表 本质是一个有序链表，通过增加多级索引，实现数据的快速定位，时间复杂度为O(LogN)。 Hash Redis Hash 类型有两种底层实现，分别是压缩列表和哈希表。 hash-max-ziplist-entries：用压缩列表保存哈希时的最大元素个数，默认 512 hash-max-ziplist-value：用压缩列表保存哈希时单个元素的最大长度，默认 64 二者满足一个超过阈值，Redis 就会自动把 Hash 的实现由压缩列表转为哈希表，之后就会一直用哈希表进行保存，不会转回为压缩列表。 当使用 REDIS_ENCODING_ZIPLIST 编码哈希表时， 程序通过将键和值一同推入压缩列表， 从而形成保存哈希表所需的键-值对结构。 新添加的 key-value 对会被添加到压缩列表的表尾。 当进行查找/删除或更新操作时，程序先定位到键的位置，然后再通过对键的位置来定位值的位置。 Redis 6.0新特性 多线程处理网络请求 只是用多线程处理网络请求，读写命令仍然使用单线程。可以有效解决网络IO的瓶颈，提升整体性能。 io-threads-do-reads yes # 启用多线程处理IO io-threads 6 # 线程数 Tracking 业务应用中的Redis客户端可以把读取的数据缓存在业务应用本地。 普通模式：实例会在服务端记录客户端读取过的key，并监控key是否有修改，如果发送变化，服务端会给客户端发送invalidate消息，通知客户端缓存失效。但是该消息只会发送一次，只有客户端再次从服务端读取数据时，可能再次发送。通过命令CLIENT TRACKING ON|OFF启用或关闭。 广播模式：服务端会给客户端广播所有key的失效情况，如果key被频繁修改，可能会消耗大量网络带宽。 CLIENT TRACKING ON BCAST PREFIX user # 客户端指定监听的key的前缀，这个前缀的key被修改后，服务端会给客户端发送invalidate消息。需要key的命名规范。 上面的两种模式需要使用6.0新增的RESP 3通信协议，对于使用RESP 2 协议的客户端，需要使用重定向模式，即客户端需要使用SUBSCRIBE命令订阅_redis_:invalidate频道用于监听key失效的消息，同时还需要执行CLIENT TRACKING命令，用于接收RESP 2客户端重定向过来的消息。 权限控制 6.0之前只能通过设置密码来控制安全访问，6.0提供了更加细粒度的访问权限控制。 用户：6.0之前没有用户概念，6.0可以创建用户并单独设置密码 ACL SETUSER &lt;USER-NAME&gt; on &gt; &lt;PSSWORD&gt; 权限：可以将具体的操作权限，赋予给用户或撤销 ACL SETUSER &lt;USER-NAME&gt; +@hash -@string 命令 说明 +&lt;COMMAND&gt; 将一个命令添加到用户的可执行列表中 -&lt;COMMOND&gt; 将一个命令从用户可执行列表中移除 +@&lt;CATEGORY&gt; 将一类命令添加到用户的可执行列表中 -@&lt;CATEGORY&gt; 将一类命令从用户可执行列表中移除 +@ALL 运行执行所有命令 -@ALL 禁止执行所有命令 还可以以key的粒度设置访问权限。 ACL SETUSER &lt;USER-NAME&gt; ~KEY:* +@ALL 允许用户对KEY为前缀的key的所有操作。 rehash Redis使用2个全局哈希表，类似JVM堆内存中的Survivor区的S1和S2，始终只使用其中一个，rehash过程中使用另一个进行替换。 给哈希表2分配更大的空间 把哈希表1中的数据重新映射并拷贝到哈希表2中 释放哈希表1的空间 但是步骤 2 在拷贝数据时，如果一次性拷贝大量数据会造成Redis线程阻塞，无法服务其他请求。为避免这个问题，Redis采用了**渐进式rehash**。 具体步骤是在第2步拷贝数据时，仍然正常处理客户端请求，同时从原哈希表中的第一个索引位置开始，连同本次请求所在的索引位置上的所有entries拷贝到新的哈希表中，对于新增操作只会在新的哈希表中新增，保证原哈希表逐渐减小。但是即使没有新的请求，Redis也会以一定的频率定时地执行一次rehash，且每次时长不超过1ms。 rehash时机： load factor &gt;= 1 且 没有正在进行RDB生成和重新AOF load facotr &gt;= 5 不管有没有正在进行RDB生成或重新AOF bigkey 通过命令./redis-cli --bigkeys -i 0.1 -a 密码来查看整个数据库中bigkey情况，该命令会输出每种类型的键值对个数和平均大小，还会输出每种数据类型中最大的bigkey信息。因为这个工具会扫描整个数据库，可能会对Redis性能产生影响，所以最后是在从库上执行，同时加上-i选项指定扫描间隔，避免长时间的扫描。但是整个命令只返回了每个数据类型中最大的bigkey，对于集合类型是元素个数，无法查看Top-N的情况。所以最好是通过SCAN命令扫描数据库，然后用TYPE命令获取值的类型；对于集合类型，使用LLEN(List)、HLEN(Hash)、SCARD(Set)、ZCARD(Sorted Set)获取元素个数，对于字符串，使用STRLEN获取字符串长度；或者使用MEMROY USAGE &lt;KEY&gt;获取占用的内存空间。 高性能的IO模型 通常说的Redis是单线程的，主要是值Redis的网络IO和键值对的读写是由一个线程完成的。Redis在6.0中加入了多线程IO，提高网络请求处理的并行度，但是仅仅是对于网络请求的处理是使用多线程，命令的读写仍然是单线程。 AOF日志 AOF日志是写后日志，即先执行命令，数据写入内存后再记录日志。AOF记录的是Redis收到的每一条命令，以文本形式保存。 比如命令set testkey testvalue，其AOF日志形式为： AOF记录 说明 *3 当前命令有三部分组成，每部分由$数字开头，其中数字表示命令、键、值的字节长度 $3 这部分包含3字节命令，也就是set set 命令 $7 7字节，也就是testkey testkey $9 9字节，也就是testvalue testvalue 但是为了避免额外的检查开销，Redis不会检查这些命令的语法，所以只有先执行命令，命令执行成功后才会记录AOF日志，这样可以保证记录的是正确的命令。此外还有一个好处是不会阻塞当前的写操作（可能会阻塞下一个操作，因为AOF日志是在主线程中执行的）。 AOF的写回策略： 策略 过程 说明 Always 同步写回，每个写命令执行完，立即同步写回磁盘 基本不丢数据，但是会影响主线程性能 Everysec 每秒写回，每个写命令执行完，先把日志写到AOF文件的内存缓冲区，每隔一秒把缓冲区内容写入磁盘 宕机丢失数据 No 由操作系统控制写回，每个写命令执行完，只把日志写到AOF文件缓冲区，由操作系统决定何时将内容写回磁盘 在性能和数据丢失取折中 AOF文件过大导致的性能问题： 文件系统对文件大小有限制，不能无限大 过大的文件写入性能会受到影响 发生宕机利用AOF记录恢复数据，文件过大会导致整个过程很慢 AOF重写 解决AOF文件过大的问题，重写时，根据Redis数据库现状生成一个新的AOF文件。对一个键进行的多次写操作，最终会生成一条写命令，可以有效减小AOF文件的大小。AOF重写过程由后台线程bgrewriteaof完成，可以避免阻塞主线程。 一个拷贝，两处日志：主线程fork出后台的bgrewriteaof子进程，会把主线程的内存拷贝（父进程的页表，而非物理内存）一份给bgrewriteaof子进程，子进程把拷贝的数据计入重写日志。同时，新的写操作会写入原来的AOF日志（完整的AOF）和新的AOF重写日志（等拷贝生成的重写日志完成后，追加上这个重写期间的增量日志，凑成一份完整的日志，替换原来的AOF日志） 内存快照RDB 使用AOF进行故障恢复需要把操作日志重新执行一遍，耗时较长，使用内存快照将大大提升恢复速度。内存快照指的是内存中的数据在某一时刻的状态。 Redis提供了两个命令生成RDB文件：save （在主线程中执行，会阻塞）和 bgsave（fork子进程执行，默认） bgsave执行过程中，主线程仍能处理写请求，Redis会借助操作系统提供的写时复制(Copy-on-write, COW)正常处理写请求。 如果频繁地生成RDB文件，会给磁盘带来很大压力，而且fork创建bgsave子进程的创建过程会阻塞主线程，影响Redis性能。 Redis4.0提出了AOF和内存快照的混合使用：内存快照以一定的频率执行，两次快照期间使用AOF日志记录。 主从模式 通常说Redis具有高可靠性，是指数据尽量少丢失、服务尽量少中断。前者通过AOF和RDB保证，后者则是通过冗余副本，即一份数据保存在多个实例上。 主从读写分离中，写操作只在主库上执行，然后同步到从库，读操作则是主从库都可以。 通过命令设置从库： replicaof MASTER_IP MASTER_PORT # 5.0之前使用slaveof命令 从库向主库发送命令psync ? -1进行全量同步，命令的第一个参数是主库的runID，由于第一次复制从库不知道主库的runID，用?表示，第二个参数-1表示第一次复制； 主库响应FULLRESYNC {runID} {offset}给从库，runID为主库的实例ID，offset表示复制进度 主库执行bgsave生成RDB文件，并发送给从库，同时生成RDB文件过程中的写操作记录到replication buffer中 从库收到RDB文件后先清空当前数据库，然后加载RDB文件 主库将replication buffer发送给从库，从库执行操作 如果从库数量很多，且都要和主库进行全量复制，主库就会忙于fork子进程生成RDB文件，且占用主库的网络带宽，可以采用“主-从-从”的模式，以级联的模式将压力分散到从库上。 replicaof SLAVE_IP SLAVE_PORT 一旦完成主从全量复制之后，它们之间就会一直维护一个网络长连接，主库通过这个连接将后续收到的命令同步给从库，长连接可以避免频繁建立连接的开销。 如果发送网络断连，Redis2.8之后，主从库会采用增量复制的方式继续同步。主库会把断连期间收到的写命令，写入replication buffer和repl_backlog_buffer缓冲区，repl_backlog_buffer是一个环形缓冲区，主库记录自己写到的位置(master_repl_offset)，从库记录自己读到的位置(slave_repl_offset)。主从库的连接恢复之后，从库给主库发送psync命令，带着slave_repl_offset，主库则将slave_repl_offset~ master_repl_offset之间的命令操作同步给从库。 参数repl_backlog_size表示环形缓冲区的大小 主从同步的问题 主从数据不一致是指客户端从从库读到的值和主库中的最新值不一致。因为主从库间的命令是异步进行的，另外，主从库间的网络可能有延迟，导致从库接收到主库的同步命令延后；或者从库正在进行复杂度高的命令被阻塞，需要等命令结束了才能继续执行主库同步过来的命令。 INFO replication命令可以查看主库接收写命令的进度信息(master_repl_offset)和从库同步写命令的进度信息(slave_repl_offset)，我们可以通过这个命令监控主从的同步进度，对于同步进度差太多的从库可以选择从客户端的连接信息中移除，等恢复正常了再添加进来。 客户端可能在从库上读到过期的数据，Redis 采用惰性删除和定期删除的策略删除过期的数据，客户端在访问过期数据时，如果数据已过期，则会被删除，给客户端返回空；同时，Redis 会定期随机选出一定数量的数据，检查是否过期，并把过期的数据删除。但是客户端从从库读取到过期数据时，是不会主动删除的。（这个问题在 Redis3.2 之后的版本优化，从库读到过期数据，虽然不会删，但是返回的是空值）另外，如果设置过期时间使用的是EXPIRE key 60，命令同步到从库，再到执行，可能会延后一段时间，导致从库上的有效期在主库之后；如果使用的是EXPIREAT key &lt;TIMESTAMP&gt;，也可能由于主从节点上的时钟不一致，导致数据的过期时间不一致。不过，还是尽量使用EXPIREAT命令设置过期时间，同时主从节点和相同的 NTP 进行时钟同步。 将slave-serve-stale-data设置为 no，这样从库就只能服务 INFO、SLAVEOF 命令，避免从库执行写命令导致数据不一致。 slave-read-only为 yes 时，从库只能处理读请求，无法处理写请求，二者有区别 脑裂 在主从集群中，同时有两个主节点，都能接收写请求。结果是客户端可能往不同的主节点上写数据，导致数据丢失。 如果是主库的数据还没同步到从库，主库故障，发生主从切换导致的数据丢失， 可以通过原主库的master_repl_offset和原从库的slave_repl_offset的差值来判断。——如果offset一致，则说明丢数据不是由脑裂导致的。 脑裂发生可能是由于主库“假故障”导致的，主库CPU满载，导致无法响应心跳，哨兵会将主库标记为客观下线，然后开始执行主从切换。但是主库在切换过程中恢复正常，可以正常处理请求，主从切换完成后，客户端的写命令才会发送到新的主库上，主库也开始执行slave of和新主库进行全量同步，清空本地数据，导致数据丢失。 通过参数min-slaves-to-write和min-slaves-max-lag限制至少有N个从库和主库进行数据复制时的ACK消息延迟不超过T秒，主库才会正常接收客户端请求。如果主库“假故障”时，无法响应哨兵心跳，也就无法保证上面的限制，主库就不再接收客户端请求了。 哨兵机制 哨兵机制是实现主从库自动切换的关键机制，有效解决主从复制模式下故障转移的问题。 哨兵是运行在特殊模式下的Redis进程，通常以集群的模式运行，哨兵主要负责三个任务：监控、选主、通知。 #配置哨兵 sentinel monitor MASTER_NAME MASTER_IP MASTER_PORT QUORUM 主库上有一个名为__sentinel__:hello的频道，所有的哨兵都会订阅这个频道，实现相互通信。哨兵会将自己的IP和端口发布到__sentinel__:hello频道，其他哨兵就只到了新哨兵的地址和端口，然后和其建立连接。 哨兵和客户端进行信息同步通用是通过pub/sub机制 事件 频道 主库下线-主观下线 +sdown 主库下线-退出主观下线 -sdown 主库下线-客观下线 +odown 主库下线-退出客观下线 -odown 从库重新配置-哨兵发送SLAVEOF命令 +slave-reconf-sent 从库重新配置-从库配置新主库，尚未完成 +slave-reconf-inprog 从库重新配置-从库配置新主库，完成 +slave_reconf-down 主从切换 +switch-master 哨兵把新的主库选出来后，客户端根据订阅得到的事件(switch-master)和新主库进行通信 switch-master MASTER_NAME OLD_IP OLD_PORT NEW_IP NEW_PORT 哨兵会向主库发送INFO命令，该命令会返回从库列表，哨兵根据返回的信息和从库建立连接，通过此连接持续地对从库进行监控。 哨兵在运行时，周期性地给所有主从库发送PING命令，检查它们是否在线，如果没有在规定时间内响应哨兵的PING命令，哨兵就会将其标记为“主观下线”，如果是主库“主观下线”，该哨兵就会给其他哨兵实例发送is-master-down-by-addr命令，其他实例会根据自己和主库的连接情况，响应Y 或 N，如果得到的赞成票达到配置的QUORUM值，就可以将主库标记为“客观下线”。然后哨兵给其他哨兵发送命令，表明希望由自己来执行主从切换，其他哨兵投票，这个过程称为“Leader选举”。但是同一时间可能有多个哨兵判断主库为“客观下线”，多个哨兵都会发起“Leader选举”，其他哨兵会给第一个向其发送命令的哨兵投Y，给后续其他哨兵投N。最终拿到赞成票&gt;= N/2 + 1 &amp;&amp; &gt;= QUORUM的哨兵当选为“Leander”来执行主从切换。如果不满足“Leader”票数，哨兵集群会等待一段时间(哨兵故障转移时间*2)，再次重新选举。 保证所有哨兵实例的配置是一致的，否则可能由于主观下线的时间(down-after-milliseconds)不一致导致主库故障切换不及时。 切片集群 如果Redis数据库数据量特别大，占用很大内存，会导致RDB非常耗时，同时fork创建子进程阻塞主线程的时间也会相对较长。可以使用切片集群，或者分片集群，即启动多个Redis实例组成一个集群，按照一定的规则将数据划分成多份，每一份保存在一个实例上。这样每一个实例生成RDB的文件就小了很多。 纵向扩展(scale up)：升级单个Redis实例的配置，如内存、磁盘，CPU 横向扩展(scale out)：增加Redis实例个数 切片集群是一种保存大量数据的通用机制，Redis3.0之后，官方提供了名为Redis Cluster的实现方案。Redis Cluster方案采用哈希槽(Hash Slot)来处理数据和实例之间的映射关系。一个切片集群共有16384(2142^{14}214)个哈希槽，类似于数据分区，每个数据都会根据其key(CRC16算法)被映射到一个槽中。 Redis实例会把自己的哈希槽信息发送给和它相连的其他实例，来完成哈希槽分配信息的扩散，实例之间互联后，所有实例就都有哈希槽的映射关系了。客户端收到哈希槽信息后会缓存在本地，请求时先计算键对应的哈希槽，然后给响应的实例发送请求。但是如果集群实例有删减、或者实例负载不均衡都可能需要重新分配哈希槽。重新分配之后，各实例之间仍然可以通过连接相互传递分配信息实现同步，但是客户端无法感知这些变化。所以Redis Cluster提供了一种重定向机制，即客户端按原来的流程给实例发送命令，但是实例上没有该键映射的哈希槽，就会返回一个MOVED响应，包含新实例的地址和端口，客户端重新向新实例发送请求，同时更新缓存的映射关系。 使用cluster create命令创建集群，Redis默认会把这些槽平均分布在集群实例上。也可以通过命令cluster meet手动建立实例间的连接，再用cluster addslots指定每个实例上的slot。 #加入集群 redis-cli -p 6379 cluster meet 127.0.0.1 6380 # 查看集群节点信息 cluster nodes cluster info # 分配槽 redis-cli -h IP -p PORT cluster addslots {0..5461} 重新分片的流程 对目标节点发送cluster setslot &lt;slot&gt; importing &lt;source-node-id&gt;命令，让目标节点准备导入槽的数据。 对源节点发送cluster setslot &lt;slot&gt; migrating &lt;destination-node-id&gt;命令，让源节点准备迁出槽的数据。 源节点循环执行cluster getkeysinslot {slot} {count}命令，获取count个属于槽{slot}的键。 对于步骤3中获取的每个key，redis-trib.rb都向源节点发送一个MIGRATE &lt;target_ip&gt; &lt;target_port&gt; &lt;key_name&gt; 0 &lt;timeout&gt; 命令，将被选中的键原子性地从源节点迁移至目标节点。 重复执行步骤3和4，直到源节点保存的所有属于槽slot的键值对都被迁移到目标节点为止。 redis-trib.rb向集群中的任意一个节点发送CLUSTER SETSLOT &lt;slot&gt; NODE &lt;node-id&gt;命令，将槽slot指派给目标节点。这一消息会发送给整个集群。 客户端ASK重定向流程 Redis集群支持在线迁移slot和数据来完成水平伸缩，当slot对应的数据从源节点到目标节点迁移过程中，客户端需要做到智能识别，保证键命令可正常执行。例如，当一个slot数据从源节点迁移到目标节点时，可能会出现一部分数据在源节点，另一部分在目标节点。 如果出现这种情况，客户端键执行流程将发生变化，如下所示， 客户端根据slot缓存发送命令到源节点，如果存在key则直接执行并返回结果。 如果key不存在，则可能存在于目标节点，这时会回复ASK重定向异常，格式如下：(error) ASK {slot} {targetIP}:{targetPort}。 客户端从ASK重定向异常提出目标节点信息，发送asking命令到目标节点打开客户端连接标识，再执行键命令。如果存在则执行，不存在则返回不存在信息。 ASK与MOVED虽然都是对客户端进的重定向，但是有着本质区别，前者说明集群正在进行slot数据迁移，所以只是临时性的重定向，不会更新slot缓存，但是MOVED重定向说明键对应的槽已经明确指定到新的节点，会更新slot缓存。 字符串 简单动态字符串 SDS 的结构： struct __attribute__((__packed)) hisdshdr8 { uint8_t len; uint8_t alloc; unsigned char flags; char buf[]; } Redis 中字符串有 5 种结构体：hisdshdr5、hisdshdr8、hisdshdr16、hisdshdr32、hisdshdr64，分别表示 len 和 alloc 占用的位数，其中len表示 buf 已用长度，alloc表示分配的长度，flags表示字符串类型，buf数组存储实际的数据，为了表示数组的结束，Redis 会自动在数组最后添加'\\0'，会额外占用一个字节。对于 String 类型来说，除了 SDS 的开销，还有一个来自RedisObject结构体的开销。 typedef struct redisObject { unsigned type:4; // 数据类型，如字符串、列表、哈希等 unsigned encoding:4; // 编码方式，如 int、raw、hashtable 等 unsigned lru:LRU_BITS; // Least Recently Used，用于记录对象最近被访问的时间 int refcount; // 引用计数，用于自动内存管理 void *ptr; // 指向实际存储数据的指针 } robj; 当保存的是 Long 类型整数时，RedisObject 中的指针就直接赋值为整数数据，就不用额外指向整数了（int 编码）；当保存的是字符串，并且长度小于等于 44 字节时，RedisObject 中元数据、指针和 SDS是一块连续的内存区域（embstr 编码）；当保存的字符串长度大于44 字节时，RedisObject 和 SDS 就不在连续的内存区域了，而是给 SDS 分配独立的空间，并用指针指向它（raw 编码） 字符串长度小于等于 44 字节时，使用 hisdshdr8即可保存，根据hisdshdr8的结构体可知，SDS 占用内存：1(len) + 1(alloc) + 1(flags) + 44(字符串) + 1('\\0') = 48 Bytes，RedisObject 占用内存：8(元数据) + 8 (指针) = 16Bytes，一共 48+16 = 64 字节。 Redis 使用的内存分配库是jemalloc，分配内存时，会根据申请的字节数N，找一个&gt;=N，但是最接近 N 的 2n2^{n}2n数作为分配的空间。 消息队列 消息队列的3个基本需求：消息保序、重复消息处理、消息可靠性。 保序就是消费者需要按照生产者发送到消息队列的顺序处理消息； 重复消息处理是在网络堵塞时出现消息重传的情况，消费者可能会收到多条重复的消息； 消息可靠性是消费者出现消息时可能出现故障导致宕机，消息没有处理完成的情况； Redis的List和Stream两种数据类型满足作为消息队列的三种需求。 List作为队列使用时是按照先进先出的顺序进行存取的，比如LPUSH写入消息，RPOP消费消息，如果消息队列为空，则客户端需要循环地去请求。Redis提供了BRPOP命令，如果消息队列为空，客户端则会阻塞直到有消息写入队列。对于消息的重复处理，则是靠生产者和消费者约定的全局唯一ID实现，生产者给每个写入的消息一个全局唯一ID，消费消费时则记录处理过的ID，对每个ID的消息只处理一次。Redis对消息的可靠性是通过命令BRPOPLPUSH命令实现的，这个命令让消费者从队列中读取消息的同时，把这个消息插入到另一个List中备份。当宕机重启后，没处理完成的消息从备份列表中读取再次进行处理。BRPOPLPUSH MQ_LIST BACKUP_LIST TIMEOUT Redis在6.2.0版本只会引入了BLMOVE用来替代上面的命令 Streams是Redis5.0专为消息队列设计的数据类型。 XADD MQ_LIST * KEY VALUE # 向消息队列中加入消息，*表示由Redis生成全局唯一ID，也可以自行设定，但是要保证全局唯一 XREAD BLOCK 100 STREAMS MQ_LIST &lt;ID&gt;|$ # 从指定的ID开始，读取后面的所有消息，$表示读取最新消息，BLOCK表示阻塞读，阻塞超时时长100ms Streams支持以消费组的形式消费消息 XGROUP create MQ_LIST &lt;GROUP_NAME&gt; 0 # 创建消费组消费消息队列MQ_LIST XREADGROUP group &lt;GROUP_NAME&gt; &lt;CONSUMER_NAME&gt; streams MQ_LIST &gt; # 让组内的消费组从MQ_LIST中读取消息，&gt; 表示从第一条未被消费的消息开始读取 为了保证消费组在故障或宕机重启后，仍然能读取未处理完的消息，Streams会自动使用内部队列(PENDING list)留存组里每个消费者读取的消息，直到消费者用XACK命令通知Streams消息已处理完。消费者重启后，可以用XPENDING命令查看已读取但未ACK的消息。 XPENDING MQ_LIST &lt;GROUP_NAME&gt; - + 10 &lt;CUSOMER_NAME&gt; # 查询某个消费者已读取，但是未ACK的消息 XACK MQ_LIST &lt;GROUP_NAME&gt; &lt;ID&gt; 异步机制 阻塞点 阻塞结果 能否异步处理 网络IO 使用IO多路复用，不会阻塞主线程 增删改查 ✖️O(N)复杂度的操作，删除大集合（bigkey）会导致阻塞 ✖️读操作不能异步，但是删除只用返回给客户端OK，可以异步进行（惰性删除） 数据库操作 ✖️清空数据库（FLUSHDB、FLUSHALL）会阻塞 只用返回给客户端OK，可以异步进行 生成RDB文件 fork子进程进行，创建子进程过程会短暂阻塞，影响不大 记录AOF文件 ✖️always同步写回策略会阻塞主线程 写回策略改为everysec，Redis会启动子线程来执行AOF日志的写盘 重写AOF fork子进程进行，创建子进程过程会短暂阻塞，影响不大 主从复制-生成、传输RDB文件 子进程处理，不阻塞主线程 主从复制-从库接收RDB、清空数据库、加载RDB ✖️FLUSHDB清空数据库会阻塞，加载RDB会阻塞 ✖️加载RDB必须主线程执行，不能异步 切片集群-同步哈希槽信息 哈希槽信息量不大，影响不大 切片集群-数据迁移 Redis Cluster方案使用同步迁移，在迁移bigkey时会阻塞 异步的键值对删除和数据库清空是Redis4.0之后提供的. 删除大量元素的集合时，使用UNLINK，清空数据库使用FLUSHDB ASYNC和FLUSHALL ASYNC Redis变慢的原因 慢查询命令 在Redis中执行速度慢的命令，会导致Redis延迟增加。 针对慢查询的命令，可以选择其他高效的命令进行替代，比如SSCAN多次迭代替换SMEMBERS用于查询Set中所有成员。或者针对Set集合的排序、交集、并集可以在客户端完成。 KEYS *命令会遍历所有键值对，延时高，应该避免使用 slowlog-log-slower-than # 慢查询日志对执行时间大于多少微秒的命令进行记录 slowlog-max-len 1000 # 慢查询日志最多能记录1000条命令，默认128 使用SLOWLOG GET命令查看慢查询日志中记录的操作命令。Redis2.8.13开始提供latency monitor监控工具，用来监控Redis运行期间峰值延迟情况，首先设置要监控的阈值config set latency-monitor-threshold 1000，然后使用命令latency latest命令查看超过1000微秒的延迟情况。 过期Key操作 默认情况下，Redis每100ms会删除一些过期的key。步骤如下：采样ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOK个数（默认20）的key，将其中过期的key删除；如果过期的key占比找过25%，则重复此步骤，直到占比低于25%。 除了上面的定时任务，当访问一个键时，也会对键的过期时间进行检查，如果过期，就将键删掉。 两种方式删除过期键时，都会产生一个expired通知，产生 expired 通知的时间为过期键被删除的时候， 而不是键的生存时间变为 0 的时候。 如果使用EXPIREAT命令设置大量同一时间过期的键，会导致Redis一直忙于删除过期键来释放空间，这个过程会阻塞主线程，导致时延增加。所以在使用EXPIREAT或EXPIRE设置过期参数时加一个可以接收的随机数。 AOF写盘 AOF重写时，会进行大量的磁盘IO，可能会导致AOF日志(everysec或always策略)的fsync被阻塞，虽然fsync由子线程执行，但是主线程会监控fsync的进度，如果上一次的fsync未完成，主线程就会阻塞。 no-appendfsync-on-rewrite yes配置（默认no）表示在AOF重写时不进行fsync操作，但是此时宕机会导致数据丢失。 swap 内存swap是操作系统将内存数据在内存和磁盘间来回换入换出的机制，涉及磁盘的读写，影响性能。当Redis实例占用大量内存，或者同一机器上运行其他进程占用大量内存时，会导致分配给Redis实例的内存不足，进而触发swap。 内存碎片 INFO memory命令用于查看内存使用情况，used_memory是Redis为保存数据实际使用的空间，used_memory_rss是操作系统实际分配给Redis的空间，mem_fragmentation_ratio = used_memory_rss / used_memory 是表示当前内存碎片率的指标。 Redis从4.0-RC3版本开始，提供了自动清理内存碎片机制，通过命令config set activedefrag yes开启该机制。同时满足下面两个配置参数时自动触发内存清理： # 内存碎片字节数达到100MB active-defrag-ignore-bytes 100mb # 内存碎片空间占比达到 10% active-defrag-threshold-lower 10 但是内存碎片清理时，需要把多份数据拷贝到新的位置，会阻塞其他操作。 # 内存清理过程中所用CPU时间比例最低 25%， 保证清理能正常开展 active-defrag-cycle-min 25 # 内存清理过程中所用CPU时间占比最高 75%， 保证不影响其他操作 active-defrag-cycle-max 75 缓冲区 服务器给每个连接的客户端配置了一个输入输出缓冲区，输入缓冲区会把客户端发过来的命令暂存起来，Redis主线程从输入缓冲区读取命令再执行；执行完成后将结果写入输出缓冲区，返回给客户端。 输入缓冲区 client list # 查看和Redis服务器相连的每个客户端对输入缓冲区的使用情况 # ... qbuf=xxx qbuf-free=xxx cmd=xxx # qbuf表示缓冲区已使用大小，qbuf-free表示缓冲区未使用的大小，cmd表示最新执行的命令 当客户端写入bigkey，或者Redis主线程阻塞无法及时处理客户端命令，导致堆积在缓冲区溢出，Redis就会将连接关闭。输入缓冲区的大小无法调整，Redis设定的是1G。 输出缓冲区 Redis为每个客户端设置的输出缓冲区包含两部分：大小固定为16KB的固定缓冲区，暂存OK和出错信息；可以动态增加的缓冲区，存放大小可变的响应结果。 MONITOR命令是用来持续监测Redis执行的，该命令的输出会持续占用输出缓冲区，最终发生溢出，所以不要在生产中使用。 输出缓冲区可以通过参数client-output-buffer-limit来进行配置 # normal: 普通client,包括monitor 的 buffer限制 ，3个0表示不做限制 client-output-buffer-limit normal 0 0 0 # slave：主从的slave client buffer限制，超过256m，或者超过64m持续60s，则关闭客户端连接 client-output-buffer-limit slave 256mb 64mb 60 # pubsub：pubsub模式中的 client buffer限制，超过32m，或者超过8m持续60s，则关闭客户端连接 client-output-buffer-limit pubsub 32mb 8mb 60 一般情况下，对于普通客户端，client-output-buffer 是不设限制的，因为客户端每发送一个请求，会等到响应结果后，再进行下一个请求，除非bigkey，一般不会积压在缓冲区。对于用作 Pub/Sub 和 slave 的客户端，server 会主动把数据推送给他们，故需要设置 client-output-buffer 的限制。 复制缓冲区 主从全量复制过程中，主节点向从节点传输RDB文件的同时，将接收到的客户端写命令保存在复制缓冲区中，等RDB文件传输完成后，再发生给从节点，主节点会为每个从节点维护一个复制缓冲区。 所以复制缓冲区本质上还是一个用于从节点使用的输出缓冲区，发生溢出也会直接关闭。设置缓冲区大小见上面(slave)。 复制积压缓冲区 主节点在接收到命令同步给从节点时，会同时写到复制积压缓冲区，用于在网络断开恢复后，将断连期间的写命令，增量同步给从节点。复制积压缓冲区是一个有限的环形缓冲区，写满后会覆盖之前的旧数据，如果从节点还没有同步这些旧命令，会导致主从节点间重新开始全量复制。对比复制缓冲区，发送出去的数据会清除，复制积压缓冲区只会覆盖旧数据，仍然保留了最新的命令。通过repl_backlog_size配置复制积压缓冲区的大小。 淘汰机制 config set maxmemory 4gb 设置Redis使用的最大内存，缓存写满后会触发缓存淘汰策略。 策略 说明 noeviction 不进行数据淘汰(默认) volatile-random 针对设置了过期时间的键值对，随机删除 volatile-ttl 针对设置了过期时间的键值对，根据过期的先后进行删除 volatile-lru 针对设置了过期时间的键值对，删除最近最没使用的 volatile-lfu 针对设置了过期时间的键值对，删除最近最少使用的 allkeys-random 从所有键值对中随机选择删除 allkeys-lru 从所有键值对中删除最近最没有使用的 allkeys-lfu 从所有键值对中删除最近最少使用的 Redis默认记录每个数据的最近一次访问时间戳，保存在RedisObject的lru字段，在决定淘汰数据时，第一次会随机选出N个数据，比较这N个数据的lru字段，把lru最小的删除。通过配置参数CONFIG SET maxmemory-samples 100设置选出的样本数N。再次淘汰数据时，Redis需要挑选数据进入第一次淘汰时创建的候选集合，能进入候选集合的数据lru必须小于集合中最小的lrj值。当候选数据个数达到maxmemory-samples时，把lru最小的淘汰出去。 缓存异常 缓存雪崩 缓存雪崩是指大量的应用请求无法在Redis缓存中进行处理，大量的请求送到数据库层，导致数据库压力激增。一般是由于缓存中大量数据同时过期，导致大量请求无法得到处理，或者Redis缓存实例发生故障宕机了，无法处理请求。 针对大量缓存同时过期，可以在给缓存设置过期时间时，加一个较小的随机数，避免大量数据同时过期； 处理方式一般是服务降级：非核心业务，暂时停止从缓存中查询，而是返回预定义信息或者错误信息；核心业务，仍然运行查询缓存，缓存缺失则访问数据库。 如果是服务宕机，为了防止引发连锁的数据库雪崩，甚至整个系统的崩溃，在业务系统中实现服务熔断或请求限流。 缓存击穿 某个热点数据失效的场景。相比缓存雪崩，缓存击穿的数据量要小很多。 处理方式是针对热点数据，不设置过期时间。 缓存穿透 要访问的数据既不在Redis缓存中，也不在数据库中，导致每次请求都要访问数据库。处理方式是缓存空值或默认值，这样避免请求到底数据库；或者使用布隆过滤器判断数据是否存在，如果不存在就不用访问数据库了；或者针对恶意请求访问不存在的数据，在前端对请求进行过滤。 缓存污染 留存在缓存中，但是访问次数很少，或者以后不会再被访问的数据。使用LFU算法的淘汰策略能有效解决缓存污染的问题。Redis在实现LFU策略时，将lru字段拆分为两部分：前16bit的时间戳(分钟级别)和后8bit的访问次数。 8bit的访问次数理论最大只有255，但是Redis的实现是一个相对值。计算 $ 1 / (counter * lfu_log_factor + 1) $ 得到的值和一个(0,1)(0, 1)(0,1)区间的随机数r比较，比 r大counter++，所以计数器增加是一个概率事件，而且计数器越大，这个概率越低。同时，Redis还加入了计数器衰减的机制，首先计算当前时间和lru时间戳的分钟差值，用差值除以lfu_decay_time得到计数器要衰减的值。 用lfu_log_factor限制计数器增长的速度，用lfu_decay_time控制计数器衰减的速度。 这里猜测可能有个问题，前16bit表示的ldt上次衰减时间计算为 ((unixtime / 60) &amp; 65535)，最大时间间隔是65535分钟，约45天。若刚好65535分钟后，访问触发衰减时，衰减值 (now &gt;= ldt ? (now - ldt) : (65535 - ldt + now)) / lfu_decay_time ​ 很小，导致本该被淘汰的值继续留存下来。 原子操作 单命令操作 Redis是单线程处理客户端命令的，所以对于单条命令是原子操作。如果多个操作可以用一条命令实现相同的效果，那么就可以用这个单命令来实现原子操作。比如DECR &lt;KEY&gt;用来替代&quot;GET, -1, SET&quot;的操作。 LUA脚本 对于较为复杂的操作，无法用单条命令实现的，用LUA脚本来保证原子操作。 local current current = redis.call(&quot;incr&quot;, KEYS[1]) if tonumber(current) == 1 then redis.call(&quot;expire&quot;, KEYS[1], 60) end 然后调用脚本redis-cli --eval xxx.lua keys, args 分布式锁 基于单个Redis节点的分布式锁 加锁：SET lock_key &lt;RANDOM_STR&gt; NX EX 5，单命令保证原子性，NX只有键不存在时才能创建成功，EX设置过期时间5s，随机字符串避免被其他客户端删除。 解锁：redis-cli --eval unlock.lua lock_key, &lt;RANDOM_STR&gt;，lua脚本保证原子性，参数包含加锁时的随机字符串，避免锁的误删。 if redis.call(&quot;get&quot;, KEYS[1]) == ARGV[1] then return redis.call(&quot;del&quot;, KEYS[1]) else return 0 end 基于多个Redis节点的高可靠分布式锁 分布式锁算法Redlock： 客户端获取当前时间 客户端依次向N个Redis实例执行加锁操作，加锁操作的超时时间远小于锁的有效时间 客户端完成向所有Redis实例的加锁操作，计算整个过程的耗时 同时满足超过半数(N/2+1N/2 + 1N/2+1)的实例加锁超过和总耗时不超过锁的有效时间才算加锁超过。如果不成功，客户端就向所有Redis实例发起释放锁的操作。 事务 事务的ACID特性： Atomicity原子性 Consistency一致性 Isolation隔离性 Durability持久性 Redis通过MULTI和EXEC命令提供事务的支持。 通过这两个命令提交的事务命令要么都执行，要么都不执行。强调的是执行，而不管成不成功，如果：命令有语法错误，命令入队时会报错，则会放弃，都不执行，保证原子性；如果非语法错误，但是命令执行时报错，全部执行，保证原子性；如果使用DISCARD放弃事务，则全部不执行，保证原子性；如果Redis实例故障，在开启AOF日志的前提下，使用工具redis-check-aof可以检测未完成的事务，恢复时不执行，保证原子性。 Redis 提供了 WATCH命令来保证事务的隔离性，使用WATCH对一个或多个key 进行监控，在EXEC之前，如果有其他客户端修改了监控的 key，事务会被放弃。 Redis 在两次 RDB 之间，或者 AOF 刷盘前宕机，是保证不了持久性的。 ","link":"https://xuhang.github.io/post/redis-he-xin-ji-zhu-yu-shi-zhan-bi-ji/"}]}